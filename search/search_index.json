{
    "docs": [
        {
            "location": "/", 
            "text": "Notes\n\n\nNotes about various topics, courses, books, etc.\n\n\nCourses:\n\n\n\n\nBitcoin and cryptocurrency technologies\n\n\n\n\nBooks:\n\n\n\n\nBuilding microservices", 
            "title": "Home"
        }, 
        {
            "location": "/#notes", 
            "text": "Notes about various topics, courses, books, etc.  Courses:   Bitcoin and cryptocurrency technologies   Books:   Building microservices", 
            "title": "Notes"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/", 
            "text": "Bitcoin and cryptocurrency technologies\n\n\nNotes on the 2018 edition of the Coursera MOOC \nBitcoin and Cryptocurrency Technologies\n by Princeton University.\n\n\nAbout this course:\n\n\n\n\nTo really understand what is special about Bitcoin, we need to understand how it works at a technical level. We\u2019ll address the important questions about Bitcoin, such as:\n\n\nHow does Bitcoin work? What makes Bitcoin different? How secure are your Bitcoins? How anonymous are Bitcoin users? What determines the price of Bitcoins? Can cryptocurrencies be regulated? What might the future hold?\n\n\nAfter this course, you\u2019ll know everything you need to be able to separate fact from fiction when reading claims about Bitcoin and other cryptocurrencies. You\u2019ll have the conceptual foundations you need to engineer secure software that interacts with the Bitcoin network. And you\u2019ll be able to integrate ideas from Bitcoin in your own projects.\nCourse Lecturers:\nArvind Narayanan, Princeton University\n\n\n\n\nLessons:\n\n\n\n\nCryptographic hash functions\n\n\nDigital signature\n\n\nHash pointers\n\n\nCentralization and decentralization\n\n\nDistributed consensus", 
            "title": "Home"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/#bitcoin-and-cryptocurrency-technologies", 
            "text": "Notes on the 2018 edition of the Coursera MOOC  Bitcoin and Cryptocurrency Technologies  by Princeton University.  About this course:   To really understand what is special about Bitcoin, we need to understand how it works at a technical level. We\u2019ll address the important questions about Bitcoin, such as:  How does Bitcoin work? What makes Bitcoin different? How secure are your Bitcoins? How anonymous are Bitcoin users? What determines the price of Bitcoins? Can cryptocurrencies be regulated? What might the future hold?  After this course, you\u2019ll know everything you need to be able to separate fact from fiction when reading claims about Bitcoin and other cryptocurrencies. You\u2019ll have the conceptual foundations you need to engineer secure software that interacts with the Bitcoin network. And you\u2019ll be able to integrate ideas from Bitcoin in your own projects.\nCourse Lecturers:\nArvind Narayanan, Princeton University   Lessons:   Cryptographic hash functions  Digital signature  Hash pointers  Centralization and decentralization  Distributed consensus", 
            "title": "Bitcoin and cryptocurrency technologies"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/", 
            "text": "Hash function\n\n\nA function that maps a string to a fixed size output.\n\n\nH(x): x string -\n fixed size output\n\n\nProperties\n\n\nCollision-free\n\n\nIt's hard to find a collision.\nNote that no hash function has ever been formally proved to be collision-free.\n\n\nApplication\n\n\nMessage digest:\n if \nH(x) = H(y)\n then it's safe to assume that \nx = y\n. This means that hash functions can help verify the integrity of documents without scanning the whole document.\n\n\nHiding\n\n\nGiven \nH(x)\n, it's hard to find \nx\n.\n\n\nApplication\n\n\nCommitment problem:\n we want to commit to value and reveal it later to an audience. If we hash that value, thanks to this property, we know that it will be hard for attackers to correctly guess it before we chose to disclose it. While our audience also knows that we wouldn't be able to change the committed value because hash functions are collision-free: it would be infeasible to generate a collision with the value we picked.\n\n\nPuzzle-friendly\n\n\nGiven a puzzle \nid\n and a target set \nY\n, try to find a solution \nx\n such that \nH(id | x) is in Y\n.\nPuzzle-friendly means that, for the stated problem, no solving strategy is much better that trying random values of x.\n\n\nApplication\n\n\nCrypto puzzles:\n puzzles that can be used as proof of work. These puzzles are used in blockchain based coin technologies such as Bitcoin.\n\n\nHash function examples\n\n\nSHA-256\n\n\nA popular hash function.\nA high level description of SHA-256 is given by the following diagram:", 
            "title": "Cryptographic hash functions"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#hash-function", 
            "text": "A function that maps a string to a fixed size output.  H(x): x string -  fixed size output", 
            "title": "Hash function"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#properties", 
            "text": "", 
            "title": "Properties"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#collision-free", 
            "text": "It's hard to find a collision.\nNote that no hash function has ever been formally proved to be collision-free.", 
            "title": "Collision-free"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#application", 
            "text": "Message digest:  if  H(x) = H(y)  then it's safe to assume that  x = y . This means that hash functions can help verify the integrity of documents without scanning the whole document.", 
            "title": "Application"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#hiding", 
            "text": "Given  H(x) , it's hard to find  x .", 
            "title": "Hiding"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#application_1", 
            "text": "Commitment problem:  we want to commit to value and reveal it later to an audience. If we hash that value, thanks to this property, we know that it will be hard for attackers to correctly guess it before we chose to disclose it. While our audience also knows that we wouldn't be able to change the committed value because hash functions are collision-free: it would be infeasible to generate a collision with the value we picked.", 
            "title": "Application"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#puzzle-friendly", 
            "text": "Given a puzzle  id  and a target set  Y , try to find a solution  x  such that  H(id | x) is in Y .\nPuzzle-friendly means that, for the stated problem, no solving strategy is much better that trying random values of x.", 
            "title": "Puzzle-friendly"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#application_2", 
            "text": "Crypto puzzles:  puzzles that can be used as proof of work. These puzzles are used in blockchain based coin technologies such as Bitcoin.", 
            "title": "Application"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#hash-function-examples", 
            "text": "", 
            "title": "Hash function examples"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#sha-256", 
            "text": "A popular hash function.\nA high level description of SHA-256 is given by the following diagram:", 
            "title": "SHA-256"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/digital-signature/", 
            "text": "Digital signature\n\n\nDigital signatures must verify 2 properties:\n\n\n\n\n\n\nOnly you can sign some data, while anyone can verify the fact that you signed it.\n\n\n\n\n\n\nThe signature must be specific to the data that it signs: if it isn't, anyone can just copy the signature you shared and apply it to different documents.\n\n\n\n\n\n\nImplementation\n\n\nDigital signature schemes use a \npublic key\n and a \nprivate key\n:\n\n\n\n\n\n\nThe private key is used to sign data\n\n\n\n\n\n\nThe public key is used to verify signed data\n\n\n\n\n\n\nDigital signature schemes must guarantee that signed data is always correctly verified.\n\n\nUnforgeable signature schemes\n\n\nWhen is a signature scheme called unforgeable? Let's consider this game:\nThere is an attacker who knows the public key and a challenger who knows the private key too.\n\n\n\n\nThe attacker can pick a document and get the challenger to sign it.\n\n\nThe challenger will sign that document and send the signed data to the attacker.\n\n\nThe game can go on as for as much as the attacker wants (at least until a plausible amount of documents is signed)\n\n\n\n\nThen the attacker tries to sign a message that the challenger has not already signed: if the forged message verifies correctly then the attacker wins, else the challenger wins.\n\n\nSo a signature scheme is \nunforgeable\n if, not matter what algorithm the attacker is using, he has only a slim chance to succeed.\n\n\nUse cases\n\n\n\n\nPublic keys can be used as identities\n\n\nSignature schemes can be used to sign the last hash pointer in a blockchain, thus signing the whole blockchain.\n\n\n\n\nSignature scheme used in Bitcoin\n\n\nBitcoin uses ECDSA. Note that a good randomness source is essential to avoid leaking your private key using your public key.", 
            "title": "Digital signature"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/digital-signature/#digital-signature", 
            "text": "Digital signatures must verify 2 properties:    Only you can sign some data, while anyone can verify the fact that you signed it.    The signature must be specific to the data that it signs: if it isn't, anyone can just copy the signature you shared and apply it to different documents.", 
            "title": "Digital signature"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/digital-signature/#implementation", 
            "text": "Digital signature schemes use a  public key  and a  private key :    The private key is used to sign data    The public key is used to verify signed data    Digital signature schemes must guarantee that signed data is always correctly verified.", 
            "title": "Implementation"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/digital-signature/#unforgeable-signature-schemes", 
            "text": "When is a signature scheme called unforgeable? Let's consider this game:\nThere is an attacker who knows the public key and a challenger who knows the private key too.   The attacker can pick a document and get the challenger to sign it.  The challenger will sign that document and send the signed data to the attacker.  The game can go on as for as much as the attacker wants (at least until a plausible amount of documents is signed)   Then the attacker tries to sign a message that the challenger has not already signed: if the forged message verifies correctly then the attacker wins, else the challenger wins.  So a signature scheme is  unforgeable  if, not matter what algorithm the attacker is using, he has only a slim chance to succeed.", 
            "title": "Unforgeable signature schemes"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/digital-signature/#use-cases", 
            "text": "Public keys can be used as identities  Signature schemes can be used to sign the last hash pointer in a blockchain, thus signing the whole blockchain.", 
            "title": "Use cases"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/digital-signature/#signature-scheme-used-in-bitcoin", 
            "text": "Bitcoin uses ECDSA. Note that a good randomness source is essential to avoid leaking your private key using your public key.", 
            "title": "Signature scheme used in Bitcoin"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/", 
            "text": "Hash pointer\n\n\nA \nhash pointer\n consists of 2 informations:\n\n\n\n\nAddress where some info is stored\n\n\nHash of that info\n\n\n\n\nHash pointers can be used in every non-cyclical data structures that uses pointers.\n\n\nBlockchain\n\n\nA \nblockchain\n is a list of linked records, called blocks. Each block contains a cryptographic hash of the previous block.\n\n\n\n\nUse cases\n\n\nA blockchain can be used as a tamper evident log. For example, in Bitcoin, a blockchain logs all the transactions (organized in blocks) approved by the network.\n\n\nMerkle tree\n\n\nA \nMerkle tree\n is a tamper evident binary tree structure.\nThe following diagram explains how to build a Merkle tree starting from a known amount of data blocks:\n\n\n\n\nA Merkle tree needs to show \nlog(N)\n items to provide proof of membership for a given data block. The time complexity of this operation is \nlog(N)\n too.\n\n\nUse cases\n\n\nMerkle trees can be used to give informations about a sequence of transactions without needing the data of all the transactions in the sequence, while still preventing attackers to easily tamper that data.", 
            "title": "Hash pointers"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#hash-pointer", 
            "text": "A  hash pointer  consists of 2 informations:   Address where some info is stored  Hash of that info   Hash pointers can be used in every non-cyclical data structures that uses pointers.", 
            "title": "Hash pointer"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#blockchain", 
            "text": "A  blockchain  is a list of linked records, called blocks. Each block contains a cryptographic hash of the previous block.", 
            "title": "Blockchain"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#use-cases", 
            "text": "A blockchain can be used as a tamper evident log. For example, in Bitcoin, a blockchain logs all the transactions (organized in blocks) approved by the network.", 
            "title": "Use cases"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#merkle-tree", 
            "text": "A  Merkle tree  is a tamper evident binary tree structure.\nThe following diagram explains how to build a Merkle tree starting from a known amount of data blocks:   A Merkle tree needs to show  log(N)  items to provide proof of membership for a given data block. The time complexity of this operation is  log(N)  too.", 
            "title": "Merkle tree"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#use-cases_1", 
            "text": "Merkle trees can be used to give informations about a sequence of transactions without needing the data of all the transactions in the sequence, while still preventing attackers to easily tamper that data.", 
            "title": "Use cases"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/centralization-vs-decentralization/", 
            "text": "Centralization vs Decentralization\n\n\nUsually, in complex system, decentralization is not all-or-nothing (e.g. the email protocol is decentralized but there are private email providers).\n\n\nAspects of decentralization in cryptocurrencies\n\n\n\n\nWho maintains the ledger?\n\n\nWho has authority over transactions validity?\n\n\nWho creates new \ncoins\n?\n\n\nWho determines how the rules of the system change?\n\n\nHow does the currency acquire exchange value?\n\n\n\n\nNote that the cryptocurrencies protocols usually are decentralized but services built around them may be centralized (e.g. exchanges).\n\n\nAspects of decentralization in Bitcoin\n\n\n\n\nBitcoin is based on a p2p network:\n anybody is allowed to join the network. Also, the barrier to entry is really low.\n\n\nMining:\n anyone is allowed to mine Bitcoins, but in this case the power concentrates in few entities in the network.\n\n\nSoftware updates:\n Bitcoin core developers are trusted by the whole community and thus they have a centralized power over the network.", 
            "title": "Centralization and decentralization"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/centralization-vs-decentralization/#centralization-vs-decentralization", 
            "text": "Usually, in complex system, decentralization is not all-or-nothing (e.g. the email protocol is decentralized but there are private email providers).", 
            "title": "Centralization vs Decentralization"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/centralization-vs-decentralization/#aspects-of-decentralization-in-cryptocurrencies", 
            "text": "Who maintains the ledger?  Who has authority over transactions validity?  Who creates new  coins ?  Who determines how the rules of the system change?  How does the currency acquire exchange value?   Note that the cryptocurrencies protocols usually are decentralized but services built around them may be centralized (e.g. exchanges).", 
            "title": "Aspects of decentralization in cryptocurrencies"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/centralization-vs-decentralization/#aspects-of-decentralization-in-bitcoin", 
            "text": "Bitcoin is based on a p2p network:  anybody is allowed to join the network. Also, the barrier to entry is really low.  Mining:  anyone is allowed to mine Bitcoins, but in this case the power concentrates in few entities in the network.  Software updates:  Bitcoin core developers are trusted by the whole community and thus they have a centralized power over the network.", 
            "title": "Aspects of decentralization in Bitcoin"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/distributed-consensus/", 
            "text": "Distributed consensus\n\n\nA key challenge of distributed systems is achieving \ndistributed consensus\n, because it is required for reliability in the system.\nAs example, consider a distributed database: if sometimes consensus is not achieved then some databases will not be consistent with the others.\n\n\nDefinition\n\n\nLet's suppose there is a network with \nN\n nodes. Each node receives an input value. Consensus in the network happens if:\n\n\n\n\nThe consensus protocol terminates.\n\n\nAll \nN\n nodes decide on the same value.\n\n\nThe decided value must be one of the input values.\n\n\n\n\nConsensus in Bitcoin\n\n\nLet's examine what happens when Alice wants to pay Bob some bitcoins:\n\n\n\n\nAlice signs the transaction referencing Bob's public key. The transactions contain the hash pointing to previously received coins by Alice.\n\n\nAlice broadcasts the transaction to the whole network.\n\n\nIf Bob wants to be notified of the transaction, he might run a Bitcoin node. But his listening is not required for him to receive coins. The network will acknowledge (if valid) the transaction nonetheless.\n\n\n\n\nIt is really important that the network reaches consensus on the validity and ordering of transactions if we want the whole system to work.\n\n\nBut we cannot solve this problem with an algorithm that has the consensus properties described before, because:\n\n\n\n\nNodes may crash\n\n\nNodes may be malicious\n\n\nA p2p network is imperfect by nature (e.g. not all nodes are connected, there are faults, there is latency because the network has no notion of global time, etc.)\n\n\n\n\nAlso, the literature on distributed consensus is pessimistic, presenting several impossibility results (e.g. \nByzantine generals problem\n)\n\n\nStill, there exist algorithms for achieving distributed consensus that trade off some properties with others (e.g. \nPaxos\n).\n\n\nBut note that the hypotheses under which impossible results were proved are not applicable to the Bitcoin network. In fact, distributed consensus works better in practice than in theory for Bitcoin, because:\n\n\n\n\nThe idea of \nincentive\n is introduced\n\n\nConsensus happens over long periods of time (1h usually), not in fixed periods. As time goes on, the probability that an invalid transaction is considered valid decreases exponentially.\n\n\n\n\nSo Bitcoins solves the distributed consensus problem with a probabilistic approach.", 
            "title": "Distributed consensus"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/distributed-consensus/#distributed-consensus", 
            "text": "A key challenge of distributed systems is achieving  distributed consensus , because it is required for reliability in the system.\nAs example, consider a distributed database: if sometimes consensus is not achieved then some databases will not be consistent with the others.", 
            "title": "Distributed consensus"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/distributed-consensus/#definition", 
            "text": "Let's suppose there is a network with  N  nodes. Each node receives an input value. Consensus in the network happens if:   The consensus protocol terminates.  All  N  nodes decide on the same value.  The decided value must be one of the input values.", 
            "title": "Definition"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/distributed-consensus/#consensus-in-bitcoin", 
            "text": "Let's examine what happens when Alice wants to pay Bob some bitcoins:   Alice signs the transaction referencing Bob's public key. The transactions contain the hash pointing to previously received coins by Alice.  Alice broadcasts the transaction to the whole network.  If Bob wants to be notified of the transaction, he might run a Bitcoin node. But his listening is not required for him to receive coins. The network will acknowledge (if valid) the transaction nonetheless.   It is really important that the network reaches consensus on the validity and ordering of transactions if we want the whole system to work.  But we cannot solve this problem with an algorithm that has the consensus properties described before, because:   Nodes may crash  Nodes may be malicious  A p2p network is imperfect by nature (e.g. not all nodes are connected, there are faults, there is latency because the network has no notion of global time, etc.)   Also, the literature on distributed consensus is pessimistic, presenting several impossibility results (e.g.  Byzantine generals problem )  Still, there exist algorithms for achieving distributed consensus that trade off some properties with others (e.g.  Paxos ).  But note that the hypotheses under which impossible results were proved are not applicable to the Bitcoin network. In fact, distributed consensus works better in practice than in theory for Bitcoin, because:   The idea of  incentive  is introduced  Consensus happens over long periods of time (1h usually), not in fixed periods. As time goes on, the probability that an invalid transaction is considered valid decreases exponentially.   So Bitcoins solves the distributed consensus problem with a probabilistic approach.", 
            "title": "Consensus in Bitcoin"
        }, 
        {
            "location": "/building-microservices/", 
            "text": "Building microservices\n\n\nNotes on the \nBuilding Microservices: Designing Fine-Grained Systems\n book by Sam Newman.\n\n\nAbout this book:\n\n\n\n\nDistributed systems have become more fine-grained in the past 10 years, shifting from code-heavy monolithic applications to smaller, self-contained microservices. But developing these systems brings its own set of headaches. With lots of examples and practical advice, this book takes a holistic view of the topics that system architects and administrators must consider when building, managing, and evolving microservice architectures.\n\n\nMicroservice technologies are moving quickly. Author Sam Newman provides you with a firm grounding in the concepts while diving into current solutions for modeling, integrating, testing, deploying, and monitoring your own autonomous services. You\u2019ll follow a fictional company throughout the book to learn how building a microservice architecture affects a single domain.\n\n\n\n\nChapters:\n\n\n\n\nMicroservices\n\n\nEvolutionary architects\n\n\nHow to model services\n\n\nIntegration\n\n\nSplitting the monolith\n\n\nDeployment\n\n\nTesting\n\n\nMonitoring\n\n\nSecurity\n\n\nConway\u2019s Law and System Design", 
            "title": "Home"
        }, 
        {
            "location": "/building-microservices/#building-microservices", 
            "text": "Notes on the  Building Microservices: Designing Fine-Grained Systems  book by Sam Newman.  About this book:   Distributed systems have become more fine-grained in the past 10 years, shifting from code-heavy monolithic applications to smaller, self-contained microservices. But developing these systems brings its own set of headaches. With lots of examples and practical advice, this book takes a holistic view of the topics that system architects and administrators must consider when building, managing, and evolving microservice architectures.  Microservice technologies are moving quickly. Author Sam Newman provides you with a firm grounding in the concepts while diving into current solutions for modeling, integrating, testing, deploying, and monitoring your own autonomous services. You\u2019ll follow a fictional company throughout the book to learn how building a microservice architecture affects a single domain.   Chapters:   Microservices  Evolutionary architects  How to model services  Integration  Splitting the monolith  Deployment  Testing  Monitoring  Security  Conway\u2019s Law and System Design", 
            "title": "Building microservices"
        }, 
        {
            "location": "/building-microservices/microservices/", 
            "text": "Microservices\n\n\nMicroservices are small and autonomous services.\n\n\nBenefits\n\n\n\n\nAllow adoption of new technologies with reduced risk\n\n\nResiliency of services, the system can be kept up on partial failures\n\n\nScaling can be aimed at specific services, thus providing cost savings due to efficiency\n\n\nEase of deployment, can deploy small parts with small deltas to deploy more frequently\n\n\nOrganizational alignment, can assign team of ideal size (not too big, not too small) to a microservice development\n\n\nComposability and reusability of services\n\n\nSmall services can be easily decommissioned and replaced when the need arises\n\n\n\n\nRelationship with SOA\n\n\nSOA has some issues because it's not a well-defined specification, so there are lots of ways to do SOA. Microservices are a specific way to do SOA. Some say they are SOA done right.\n\n\nSimilar decomposition techniques\n\n\nDo we need microservices? Can similar decomposition techniques offer the same benefits provided by microservices?\n\n\nShared libraries\n\n\nDrawbacks:\n\n\n\n\nNeed to run on the same platform as the service (losing technology heterogeneity)\n\n\nCannot scale services independently\n\n\nUnless using DLLs, services cannot load a new version of the library without stopping their execution (losing ease of deploy in isolation)\n\n\nLack of system resiliency\n\n\n\n\nShared libraries are best suited for common code reuse. But be careful: business code reuse can cause coupling in microservices.\n\n\nModules\n\n\nUsually languages do not have proper support for isolated life cycle management of modules, it's hard (if possible at all) for developers to add this functionality.\nThe drawbacks in these cases are nearly the same as the ones provided by shared libraries.\n\n\nConsider even languages that have proper support for ILM (such as Erlang). The system should be based only on that language (losing technology heterogeneity) and this is usually not the case for projects that integrate with legacy software.\n\n\nAlso, in practice, using modules will likely make developers produce coupled code between modules, thus losing independence.\n\n\nNo silver bullet\n\n\nMicroservices are no silver bullet because they add to your system the challenges of distributed systems. Also, you have to be confident with deploys, testing, monitoring and scaling in order to effectively gain the benefits of microservices.", 
            "title": "Microservices"
        }, 
        {
            "location": "/building-microservices/microservices/#microservices", 
            "text": "Microservices are small and autonomous services.", 
            "title": "Microservices"
        }, 
        {
            "location": "/building-microservices/microservices/#benefits", 
            "text": "Allow adoption of new technologies with reduced risk  Resiliency of services, the system can be kept up on partial failures  Scaling can be aimed at specific services, thus providing cost savings due to efficiency  Ease of deployment, can deploy small parts with small deltas to deploy more frequently  Organizational alignment, can assign team of ideal size (not too big, not too small) to a microservice development  Composability and reusability of services  Small services can be easily decommissioned and replaced when the need arises", 
            "title": "Benefits"
        }, 
        {
            "location": "/building-microservices/microservices/#relationship-with-soa", 
            "text": "SOA has some issues because it's not a well-defined specification, so there are lots of ways to do SOA. Microservices are a specific way to do SOA. Some say they are SOA done right.", 
            "title": "Relationship with SOA"
        }, 
        {
            "location": "/building-microservices/microservices/#similar-decomposition-techniques", 
            "text": "Do we need microservices? Can similar decomposition techniques offer the same benefits provided by microservices?", 
            "title": "Similar decomposition techniques"
        }, 
        {
            "location": "/building-microservices/microservices/#shared-libraries", 
            "text": "Drawbacks:   Need to run on the same platform as the service (losing technology heterogeneity)  Cannot scale services independently  Unless using DLLs, services cannot load a new version of the library without stopping their execution (losing ease of deploy in isolation)  Lack of system resiliency   Shared libraries are best suited for common code reuse. But be careful: business code reuse can cause coupling in microservices.", 
            "title": "Shared libraries"
        }, 
        {
            "location": "/building-microservices/microservices/#modules", 
            "text": "Usually languages do not have proper support for isolated life cycle management of modules, it's hard (if possible at all) for developers to add this functionality.\nThe drawbacks in these cases are nearly the same as the ones provided by shared libraries.  Consider even languages that have proper support for ILM (such as Erlang). The system should be based only on that language (losing technology heterogeneity) and this is usually not the case for projects that integrate with legacy software.  Also, in practice, using modules will likely make developers produce coupled code between modules, thus losing independence.", 
            "title": "Modules"
        }, 
        {
            "location": "/building-microservices/microservices/#no-silver-bullet", 
            "text": "Microservices are no silver bullet because they add to your system the challenges of distributed systems. Also, you have to be confident with deploys, testing, monitoring and scaling in order to effectively gain the benefits of microservices.", 
            "title": "No silver bullet"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/", 
            "text": "Evolutionary architects\n\n\nArchitects of microservices based systems need to face difficult choices:\n\n\n\n\nDegree of technology unification between microservices\n\n\nTeam policies (e.g. allow different teams to use different patterns?)\n\n\nHow to merge/split microservices?\n\n\n\n\nBut to provide effective guidance we must first understand the role of software architects in IT.\n\n\nIT is a young industry that borrowed the \narchitect\n term from actual architects and engineers but there is a substantial difference in these roles: software is not subject to physical constraints. Software is flexible and can be easily adapted and evolved to new requirements.\n\n\nArchitects need to:\n\n\n\n\nSet direction in broad strokes (i.e. set software zones), they must be involved in specific implementation details in limited cases\n\n\nEnsure that the system is suitable for the current requirements\n\n\nEnsure that the system can accommodate future requirements\n\n\nMake the system work for both users and developers\n\n\nUnderstand implementation complexity\n\n\n\n\nTip: an architect should spend some time working on user stories with developers to better understand the state/challenges of the system.\n\n\nA principled approach\n\n\nThere are lots of tradeoffs in decisions about microservices based systems. Defining a set of principles and practices can guide us through these choices.\n\n\nPrinciples\n are rules made by an architect to align the development activity to larger system goals. An example is the \n12 factor app\n, defined by Heroku to guide the development of scalable cloud SaaS applications.\n\n\nPractices\n are ways to make sure a principle is followed.\nPractices can differ when following same principles (e.g. different practices for .NET and Java systems following the same principles).\n\n\nPrinciples and practices adopted should depend on goals that we want to achieve, also taking into account \nstrategic goals\n (i.e. the long term goals of your organization).\n\n\nThis is what happens in the real world:\n\n\n\n\nThe required standard\n\n\nOne of the core balances to find is how much variability to allow in your system.\nToo much variability can cause issues such as onboarding problems and other expressed before.\nOne way to identify a standard is to identify the attributes of an ideal microservice.\n\n\nZoning\n\n\nOur zones are service boundaries or groups of services.\nAs architects, more important to know how services communicate between each other than how a single isolated service works.\n\n\nMany organizations are using microservices to make teams more autonomous, architects then rely on those teams to make local optimal decisions.\nStill, care is needed for choosing the technologies of single services: sparse technology does not facilitate experience growth and makes it harder for developers to switch teams.\n\n\nAlso, care of protocol for communication between microservices, because each ms will need to know how to operate with a certain protocol and this adds complexity.\n\n\nMonitoring\n\n\nIt's important to monitor the health of the whole system and gather health and log data in a single place in order to analyze it. Remember to use an agnostic log/health/data reporting protocol/format so your monitoring system does not change as services change.\n\n\nInterfaces\n\n\nKeep interfaces of services as simple as possible, supporting the minimum standards required.\nThis makes it easier to handle versioning and system complexity, because it will be easier to evolve the system.\n\n\nArchitectural safety\n\n\nServices need to resist to partial failures in the system. A partial failure should not affect the system as a whole.\n\n\nGovernance through code\n\n\nMaking sure that developers are implementing the defined standards can be a burden. Exemplars and service templates help a lot with this problem.\n\n\nExemplars\n should ideally be real-world services following your standards. Developers can safely look at exemplars to further develop the application.\n\n\nService templates\n are a set of technologies or even frameworks to be used in your services. These can guide the developer teams and make their work easier. But be careful: frameworks should not be enforced by an external team and they should be user-friendly. Another danger is that service templates can cause coupling between services.\n\n\nTechnical debt\n\n\nOften the technical vision cannot be fully followed through because of business requirements. This is a source of technical debt because a short-term benefit will be paid with a long-term cost.\nTeams can manage their technical debt or it can be managed by a centralized source.\n\n\nHandling exceptions to the rules\n\n\nSometimes you will need to build a part of your system while not following some rules of the standard you defined. If you find this happening too often, it can make sense to change you rule set.\n\n\nSummary\n\n\nThe following core responsibilities of architects emerged:\n\n\n\n\nVision\n\n\nEmpathy\n\n\nCollaboration\n\n\nAdaptability\n\n\nTeam autonomy\n\n\nGovernance\n\n\n\n\nArchitects need to constantly balance aspects of their systems to successfully do their job.", 
            "title": "Evolutionary architects"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#evolutionary-architects", 
            "text": "Architects of microservices based systems need to face difficult choices:   Degree of technology unification between microservices  Team policies (e.g. allow different teams to use different patterns?)  How to merge/split microservices?   But to provide effective guidance we must first understand the role of software architects in IT.  IT is a young industry that borrowed the  architect  term from actual architects and engineers but there is a substantial difference in these roles: software is not subject to physical constraints. Software is flexible and can be easily adapted and evolved to new requirements.  Architects need to:   Set direction in broad strokes (i.e. set software zones), they must be involved in specific implementation details in limited cases  Ensure that the system is suitable for the current requirements  Ensure that the system can accommodate future requirements  Make the system work for both users and developers  Understand implementation complexity   Tip: an architect should spend some time working on user stories with developers to better understand the state/challenges of the system.", 
            "title": "Evolutionary architects"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#a-principled-approach", 
            "text": "There are lots of tradeoffs in decisions about microservices based systems. Defining a set of principles and practices can guide us through these choices.  Principles  are rules made by an architect to align the development activity to larger system goals. An example is the  12 factor app , defined by Heroku to guide the development of scalable cloud SaaS applications.  Practices  are ways to make sure a principle is followed.\nPractices can differ when following same principles (e.g. different practices for .NET and Java systems following the same principles).  Principles and practices adopted should depend on goals that we want to achieve, also taking into account  strategic goals  (i.e. the long term goals of your organization).  This is what happens in the real world:", 
            "title": "A principled approach"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#the-required-standard", 
            "text": "One of the core balances to find is how much variability to allow in your system.\nToo much variability can cause issues such as onboarding problems and other expressed before.\nOne way to identify a standard is to identify the attributes of an ideal microservice.", 
            "title": "The required standard"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#zoning", 
            "text": "Our zones are service boundaries or groups of services.\nAs architects, more important to know how services communicate between each other than how a single isolated service works.  Many organizations are using microservices to make teams more autonomous, architects then rely on those teams to make local optimal decisions.\nStill, care is needed for choosing the technologies of single services: sparse technology does not facilitate experience growth and makes it harder for developers to switch teams.  Also, care of protocol for communication between microservices, because each ms will need to know how to operate with a certain protocol and this adds complexity.", 
            "title": "Zoning"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#monitoring", 
            "text": "It's important to monitor the health of the whole system and gather health and log data in a single place in order to analyze it. Remember to use an agnostic log/health/data reporting protocol/format so your monitoring system does not change as services change.", 
            "title": "Monitoring"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#interfaces", 
            "text": "Keep interfaces of services as simple as possible, supporting the minimum standards required.\nThis makes it easier to handle versioning and system complexity, because it will be easier to evolve the system.", 
            "title": "Interfaces"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#architectural-safety", 
            "text": "Services need to resist to partial failures in the system. A partial failure should not affect the system as a whole.", 
            "title": "Architectural safety"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#governance-through-code", 
            "text": "Making sure that developers are implementing the defined standards can be a burden. Exemplars and service templates help a lot with this problem.  Exemplars  should ideally be real-world services following your standards. Developers can safely look at exemplars to further develop the application.  Service templates  are a set of technologies or even frameworks to be used in your services. These can guide the developer teams and make their work easier. But be careful: frameworks should not be enforced by an external team and they should be user-friendly. Another danger is that service templates can cause coupling between services.", 
            "title": "Governance through code"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#technical-debt", 
            "text": "Often the technical vision cannot be fully followed through because of business requirements. This is a source of technical debt because a short-term benefit will be paid with a long-term cost.\nTeams can manage their technical debt or it can be managed by a centralized source.", 
            "title": "Technical debt"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#handling-exceptions-to-the-rules", 
            "text": "Sometimes you will need to build a part of your system while not following some rules of the standard you defined. If you find this happening too often, it can make sense to change you rule set.", 
            "title": "Handling exceptions to the rules"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#summary", 
            "text": "The following core responsibilities of architects emerged:   Vision  Empathy  Collaboration  Adaptability  Team autonomy  Governance   Architects need to constantly balance aspects of their systems to successfully do their job.", 
            "title": "Summary"
        }, 
        {
            "location": "/building-microservices/how-to-model-services/", 
            "text": "How to model services\n\n\nWhat makes a good service? The whole point of microservices is the ability to deploy them independently, so \nloose coupling\n and \nhigh cohesion\n (same kind of logic not distributed across different microservices) are needed.\n\n\nLet's introduce the fictional domain of MusicCorp, an old company who wants to sell music tapes online.\n\n\nThe Bounded Context\n\n\nBounded context is a concept introduced in DDD (\nDomain Driven Design\n). A bounded context has private and public models relative to a domain context. Only public models are exposed to other contexts. Also, public models can be a different representation of private models (they can be \nmapped\n models).\n\n\nThis allows for both loose coupling, since there are no references to whole models, and high cohesion, since these contexts are modeled from actual domain contexts.\n\n\nFor example, in the MusicCorp online business, \nWarehouse\n and \nFinance\n are different bounded contexts.\n\n\n\n\nHere \nStock item\n is a public model shared by each context which has different representations in each context.\n\n\nThe benefits provided by bounded contexts makes them really good candidates to be microservices.\n\n\nBut be careful: restructuring bounded contexts has a high cost, so architects must not fall into the \npremature decomposition\n trap. If you have a solid understanding and vision of the whole system, your bounded contexts will be solid. Otherwise, you may need to reorganize them frequently.\n\n\nNesting\n\n\nIt's best to think first about coarse-grained bounded contexts. Then these usually can be further divided into subcontexts. Should you keep the nested contexts public or private to the parent context?\nUsually, if each subcontext has a respective team that handles that area in the organization, it's ideal to make each subcontext public (thus an effective context).", 
            "title": "How to model services"
        }, 
        {
            "location": "/building-microservices/how-to-model-services/#how-to-model-services", 
            "text": "What makes a good service? The whole point of microservices is the ability to deploy them independently, so  loose coupling  and  high cohesion  (same kind of logic not distributed across different microservices) are needed.  Let's introduce the fictional domain of MusicCorp, an old company who wants to sell music tapes online.", 
            "title": "How to model services"
        }, 
        {
            "location": "/building-microservices/how-to-model-services/#the-bounded-context", 
            "text": "Bounded context is a concept introduced in DDD ( Domain Driven Design ). A bounded context has private and public models relative to a domain context. Only public models are exposed to other contexts. Also, public models can be a different representation of private models (they can be  mapped  models).  This allows for both loose coupling, since there are no references to whole models, and high cohesion, since these contexts are modeled from actual domain contexts.  For example, in the MusicCorp online business,  Warehouse  and  Finance  are different bounded contexts.   Here  Stock item  is a public model shared by each context which has different representations in each context.  The benefits provided by bounded contexts makes them really good candidates to be microservices.  But be careful: restructuring bounded contexts has a high cost, so architects must not fall into the  premature decomposition  trap. If you have a solid understanding and vision of the whole system, your bounded contexts will be solid. Otherwise, you may need to reorganize them frequently.", 
            "title": "The Bounded Context"
        }, 
        {
            "location": "/building-microservices/how-to-model-services/#nesting", 
            "text": "It's best to think first about coarse-grained bounded contexts. Then these usually can be further divided into subcontexts. Should you keep the nested contexts public or private to the parent context?\nUsually, if each subcontext has a respective team that handles that area in the organization, it's ideal to make each subcontext public (thus an effective context).", 
            "title": "Nesting"
        }, 
        {
            "location": "/building-microservices/integration/", 
            "text": "Integration\n\n\nDesirable properties of communication between microservices:\n\n\n\n\nAvoid breaking changes as much as possible.\n\n\nTechnology agnostic APIs.\n\n\nMake it easy to consume APIs.\n\n\nHidden internal implementation details.\n\n\n\n\nShared database\n\n\nThe most common form of integration.\n\n\n\n\nHas the following issues:\n\n\n\n\nInternal representations are not private, causing high coupling.\n\n\nLogic to modify some kind of data is present in different services, causing loss of cohesion.\n\n\nEvery kind of data must be stored using the same DBMS technology.\n\n\n\n\nThese issues would eliminate the benefits of using microservices, so shared databases are to avoid.\n\n\nSynchronous vs Asynchronous\n\n\nSynchronous communication\n starts with a blocking call to the server that resolves once the operation completes. It's easy to debug but lacks capabilities to effectively handle long-running processes.\n\n\nAsynchronous communication\n does not wait for the server to respond. In theory, a client may even not need to know if the server completed the operation. It's not easy to debug but can effectively handle long-running processes.\n\n\nThese two different modes of communication can enable two different styles of collaboration:\n\n\n\n\nRequest/response:\n natural fit to synchronous communication, can handle asynchronous communication too using callbacks.\n\n\nEvent-based:\n natural fit to asynchronous communication. It's more flexible since a client just issues an event, allowing for more services to listen on that event later on, without modifying the client's code.\n\n\n\n\nOrchestration vs Choreography\n\n\nOrchestration\n means having an orchestrator service that instructs other services on what to do and organizes the whole flow. This provides a clear view of the whole flow but can cause coupling if the orchestrator becomes a \u201cgod\u201d microservice.\n\n\nChoreography\n means that services can issue or listen to events. This approach keeps services decoupled but can make it hard to understand the whole flow.\n\n\nRemote procedure calls\n\n\nRemote procedure call refers to the technique of making a local call and having it execute on a remote service somewhere.\n\n\nRPC fit well with the request/response collaboration style.\n\n\nThe selling point of RPC is ease of use: it's really practical to make a remote call look like a local call.\n\n\nHowever, RPC has issues too:\n\n\n\n\nUsually it causes technology coupling between client and server.\n\n\nLocal calls must not be confused with remote calls, because of latency and unreliability.\n\n\nBrittleness, because server signatures and interfaces need to match exactly the ones in the client.\n\n\n\n\nCompared to database integration, RPC is certainly an improvement when we think about options for request/response collaboration.\n\n\nREST\n\n\nREpresentational State Transfer (REST) is an architectural style inspired by the Web. The most important concept is the one of resource, which can be requested in different representations. This favours decoupling between internal and external representations.\n\n\nThere are many styles of REST, compared in the \nRichardson Maturity Model\n.\n\n\nUsually REST is implemented over HTTP because HTTP provides parts of the REST specification, such as verbs. Also, there are lots of tools supporting REST with HTTP.\n\n\nHATEOAS\n\n\nAnother principle introduced in REST that can help us avoid the coupling between client and server is the concept of \nhypermedia as the engine of application state\n (often abbreviated as HATEOAS). One of the downsides is that the navigation of controls can be quite chatty, as the client needs to follow links to find the operation it wants to perform. Ultimately, this is a trade-off.\n\n\nSerialization format\n\n\nREST provides flexibility over the serialization format of the data. The most popular choices are JSON and XML.\nXML has built-in support for hypermedia while there are standards to provide hypermedia data with JSON.\n\n\nDownsides to REST Over HTTP\n\n\n\n\nNot easy to generate stubs for REST over HTTP services as it would be with RPC.\n\n\nSome web servers do not \nfully\n support all the HTTP verbs.\n\n\nPerformance is penalized because of hypermedia data and HTTP overhead.\n\n\nHTTP is not suited for frequently exchanging small volumes of data, WebSockets or protocol buffers are more suitable for this kind of communication.\n\n\n\n\nDespite these disadvantages, REST over HTTP is a sensible default choice for service-to-service interactions.\n\n\nImplementing Asynchronous Event-Based Collaboration\n\n\nTo implement asynchronous event-based collaboration we need to consider:\n\n\n\n\nA way for our microservices to emit events.\n\n\nA way for our consumers to find out those events have happened.\n\n\n\n\nTraditionally, \nmessage brokers\n like RabbitMQ can handle both problems, while also being able to scale and have resiliency.\n\n\nBut note that this kind of collaboration comes with a system complexity increase (e.g. if you're not careful, you could have \ncatastrophic failovers\n as intended by Martin Fowler).\n\n\nReactive extensions\n can help you a lot when handling lots of calls to downstream services. They are a popular choice in distributed systems.\n\n\nDRY in Microservices\n\n\nFollowing the DRY principle can cause coupling between microservices. As a general rule, DRY is to be followed only inside service boundaries. Across different services, code duplication is a smaller problem than coupling. An exception to this rule can be model-agnostic code such as logging, which can be safely shared between microservices.\n\n\nClient libraries\n\n\nClient libraries can cause coupling between services and clients. To limit this danger, it's best if different developer teams develop the server API and the client library: this way there should be no \nlogic leaks\n from the server into the client.\nIt's also important to give clients control on when to upgrade their client libraries, to avoid coupling in deploys.\n\n\nAccess by Reference\n\n\nSometimes it may happen to pass around outdated information: we request a \nCustomer\n and then we use that customer in another request, but in the meanwhile it has changed.\nIn order to retrieve the current state, such requests must include an ID of the involved resources. But this approach has downsides too:\n\n\n\n\nIt may cause the \nCustomers\n service to be accessed too much.\n\n\nIt causes overhead in requests.\n\n\n\n\nThis is a tradeoff to consider. The point is: be aware of the freshness of data passed between microservices.\n\n\nService versioning\n\n\nThe following points can help you have a good service versioning in your system:\n\n\n\n\nDefer breaking changes as long as possible (e.g. by using the \nTolerant reader\n pattern).\n\n\nRobustness principle: \u201c\nBe conservative in what you do, be liberal in what you accept from others\n\u201d.\n\n\nCatch breaking changes early, tests help a lot here.\n\n\nUse semantic versioning.\n\n\nHave coexisting service versions to gradually adopt the new version in the system. Another option is to concurrently deploy microservices of different versions, but suppose you need to fix a bug in the service, then you would need to deploy 2 different services. Still, this is a good approach if you are doing blue/green deploys.\n\n\n\n\nUser interfaces\n\n\nEach type of user interface (e.g. browser, desktop, mobile) has its own constraints. So even though our core services are the same, we might need a way to adapt them for these constraints.\n\n\nLet\u2019s look at a few models of user interfaces to see how this might be achieved.\n\n\nAPI composition\n\n\nEach part of the UI communicates with a specific service via its API.\n\n\n\n\nDownsides:\n\n\n\n\nLittle ability to tailor the responses for different sorts of devices.\n\n\nIf another team is creating the UI, making even small changes requires change requests to multiple teams.\n\n\nThis communication could also be fairly chatty. Opening lots of calls directly to services can be quite intensive for mobile devices.\n\n\n\n\nUI Fragment composition\n\n\nRather than having our UI make API calls and map everything back to UI controls, we could have our services provide parts of the UI directly.\n\n\n\n\nThe same team that makes changes to the services can also be in charge of making changes to those parts of the UI, allowing us to get changes out faster.\n\n\nDownsides:\n\n\n\n\nWe need to ensure consistency of the user experience, CSS and HTML style guides can help.\n\n\nNot ideal for native interfaces, it would require falling back to the API composition model.\n\n\nThe more cross-cutting a form of interaction is, the less likely this model will fit, falling back to the API composition model.\n\n\n\n\nBackends for Frontends\n\n\nA common solution to the problem of chatty interfaces with backend services, or the need to vary content for different types of devices, is to have a server-side aggregation endpoint, or \nAPI gateway\n.\n\n\n\n\nThe problem that can occur is that normally we\u2019ll have one giant layer for all our services, losing ability to deploy clients independently.\n\n\nA model that solves this problem is \nBackends for frontends\n (\nBFFs\n), it restricts the use of backends for a specific client.\n\n\n\n\nThe danger with this approach is the same as with any aggregating layer: it can take on logic it shouldn\u2019t. These BFFs should only contain behavior specific to delivering a particular user experience.\n\n\nA Hybrid Approach\n\n\nSome systems use different models together (e.g. BFFs for mobile and UI fragment composition for web). The tricky part still remains avoiding putting too much logic into any intermediate layer. This causes coupling and low cohesion.\n\n\nIntegrating with Third-Party Software\n\n\nChallenges associated with integrating third-party software into your system:\n\n\n\n\nLack of control:\n probably many of the technical decisions have been made for you to simplify product usage. The tool selection process should take into account ease of use of third-party software.\n\n\nCustomization:\n many enterprise tools sell themselves on their ability to be heavily customized just for you. But the cost of customization can be more expensive than building something bespoke from scratch.\n\n\nIntegration spaghetti:\n ideally you want to standardize on a few types of integration. If one product forces you tu use proprietary protocols, it could mean troubles.\n\n\n\n\nBest practices:\n\n\n\n\nTreat third-party software as a service and place all the customization code in services you control, if possible.\n\n\nWhen moving away from integrated COTS or legacy software, adopt the \nStrangler Application Pattern\n: intercept calls to such software and route them either to the legacy services or to your new services. This allows for a gradual switch.\n\n\n\n\nSummary\n\n\nTo ensure our microservices remain as decoupled as possible from their other collaborators:\n\n\n\n\nAvoid database integration at all costs.\n\n\nUnderstand the trade-offs between REST and RPC, but strongly consider REST as a good starting point for request/response integration.\n\n\nPrefer choreography over orchestration.\n\n\nAvoid breaking changes and the need to version by understanding Postel\u2019s Law and using tolerant readers.\n\n\nThink of user interfaces as compositional layers.", 
            "title": "Integration"
        }, 
        {
            "location": "/building-microservices/integration/#integration", 
            "text": "Desirable properties of communication between microservices:   Avoid breaking changes as much as possible.  Technology agnostic APIs.  Make it easy to consume APIs.  Hidden internal implementation details.", 
            "title": "Integration"
        }, 
        {
            "location": "/building-microservices/integration/#shared-database", 
            "text": "The most common form of integration.   Has the following issues:   Internal representations are not private, causing high coupling.  Logic to modify some kind of data is present in different services, causing loss of cohesion.  Every kind of data must be stored using the same DBMS technology.   These issues would eliminate the benefits of using microservices, so shared databases are to avoid.", 
            "title": "Shared database"
        }, 
        {
            "location": "/building-microservices/integration/#synchronous-vs-asynchronous", 
            "text": "Synchronous communication  starts with a blocking call to the server that resolves once the operation completes. It's easy to debug but lacks capabilities to effectively handle long-running processes.  Asynchronous communication  does not wait for the server to respond. In theory, a client may even not need to know if the server completed the operation. It's not easy to debug but can effectively handle long-running processes.  These two different modes of communication can enable two different styles of collaboration:   Request/response:  natural fit to synchronous communication, can handle asynchronous communication too using callbacks.  Event-based:  natural fit to asynchronous communication. It's more flexible since a client just issues an event, allowing for more services to listen on that event later on, without modifying the client's code.", 
            "title": "Synchronous vs Asynchronous"
        }, 
        {
            "location": "/building-microservices/integration/#orchestration-vs-choreography", 
            "text": "Orchestration  means having an orchestrator service that instructs other services on what to do and organizes the whole flow. This provides a clear view of the whole flow but can cause coupling if the orchestrator becomes a \u201cgod\u201d microservice.  Choreography  means that services can issue or listen to events. This approach keeps services decoupled but can make it hard to understand the whole flow.", 
            "title": "Orchestration vs Choreography"
        }, 
        {
            "location": "/building-microservices/integration/#remote-procedure-calls", 
            "text": "Remote procedure call refers to the technique of making a local call and having it execute on a remote service somewhere.  RPC fit well with the request/response collaboration style.  The selling point of RPC is ease of use: it's really practical to make a remote call look like a local call.  However, RPC has issues too:   Usually it causes technology coupling between client and server.  Local calls must not be confused with remote calls, because of latency and unreliability.  Brittleness, because server signatures and interfaces need to match exactly the ones in the client.   Compared to database integration, RPC is certainly an improvement when we think about options for request/response collaboration.", 
            "title": "Remote procedure calls"
        }, 
        {
            "location": "/building-microservices/integration/#rest", 
            "text": "REpresentational State Transfer (REST) is an architectural style inspired by the Web. The most important concept is the one of resource, which can be requested in different representations. This favours decoupling between internal and external representations.  There are many styles of REST, compared in the  Richardson Maturity Model .  Usually REST is implemented over HTTP because HTTP provides parts of the REST specification, such as verbs. Also, there are lots of tools supporting REST with HTTP.", 
            "title": "REST"
        }, 
        {
            "location": "/building-microservices/integration/#hateoas", 
            "text": "Another principle introduced in REST that can help us avoid the coupling between client and server is the concept of  hypermedia as the engine of application state  (often abbreviated as HATEOAS). One of the downsides is that the navigation of controls can be quite chatty, as the client needs to follow links to find the operation it wants to perform. Ultimately, this is a trade-off.", 
            "title": "HATEOAS"
        }, 
        {
            "location": "/building-microservices/integration/#serialization-format", 
            "text": "REST provides flexibility over the serialization format of the data. The most popular choices are JSON and XML.\nXML has built-in support for hypermedia while there are standards to provide hypermedia data with JSON.", 
            "title": "Serialization format"
        }, 
        {
            "location": "/building-microservices/integration/#downsides-to-rest-over-http", 
            "text": "Not easy to generate stubs for REST over HTTP services as it would be with RPC.  Some web servers do not  fully  support all the HTTP verbs.  Performance is penalized because of hypermedia data and HTTP overhead.  HTTP is not suited for frequently exchanging small volumes of data, WebSockets or protocol buffers are more suitable for this kind of communication.   Despite these disadvantages, REST over HTTP is a sensible default choice for service-to-service interactions.", 
            "title": "Downsides to REST Over HTTP"
        }, 
        {
            "location": "/building-microservices/integration/#implementing-asynchronous-event-based-collaboration", 
            "text": "To implement asynchronous event-based collaboration we need to consider:   A way for our microservices to emit events.  A way for our consumers to find out those events have happened.   Traditionally,  message brokers  like RabbitMQ can handle both problems, while also being able to scale and have resiliency.  But note that this kind of collaboration comes with a system complexity increase (e.g. if you're not careful, you could have  catastrophic failovers  as intended by Martin Fowler).  Reactive extensions  can help you a lot when handling lots of calls to downstream services. They are a popular choice in distributed systems.", 
            "title": "Implementing Asynchronous Event-Based Collaboration"
        }, 
        {
            "location": "/building-microservices/integration/#dry-in-microservices", 
            "text": "Following the DRY principle can cause coupling between microservices. As a general rule, DRY is to be followed only inside service boundaries. Across different services, code duplication is a smaller problem than coupling. An exception to this rule can be model-agnostic code such as logging, which can be safely shared between microservices.", 
            "title": "DRY in Microservices"
        }, 
        {
            "location": "/building-microservices/integration/#client-libraries", 
            "text": "Client libraries can cause coupling between services and clients. To limit this danger, it's best if different developer teams develop the server API and the client library: this way there should be no  logic leaks  from the server into the client.\nIt's also important to give clients control on when to upgrade their client libraries, to avoid coupling in deploys.", 
            "title": "Client libraries"
        }, 
        {
            "location": "/building-microservices/integration/#access-by-reference", 
            "text": "Sometimes it may happen to pass around outdated information: we request a  Customer  and then we use that customer in another request, but in the meanwhile it has changed.\nIn order to retrieve the current state, such requests must include an ID of the involved resources. But this approach has downsides too:   It may cause the  Customers  service to be accessed too much.  It causes overhead in requests.   This is a tradeoff to consider. The point is: be aware of the freshness of data passed between microservices.", 
            "title": "Access by Reference"
        }, 
        {
            "location": "/building-microservices/integration/#service-versioning", 
            "text": "The following points can help you have a good service versioning in your system:   Defer breaking changes as long as possible (e.g. by using the  Tolerant reader  pattern).  Robustness principle: \u201c Be conservative in what you do, be liberal in what you accept from others \u201d.  Catch breaking changes early, tests help a lot here.  Use semantic versioning.  Have coexisting service versions to gradually adopt the new version in the system. Another option is to concurrently deploy microservices of different versions, but suppose you need to fix a bug in the service, then you would need to deploy 2 different services. Still, this is a good approach if you are doing blue/green deploys.", 
            "title": "Service versioning"
        }, 
        {
            "location": "/building-microservices/integration/#user-interfaces", 
            "text": "Each type of user interface (e.g. browser, desktop, mobile) has its own constraints. So even though our core services are the same, we might need a way to adapt them for these constraints.  Let\u2019s look at a few models of user interfaces to see how this might be achieved.", 
            "title": "User interfaces"
        }, 
        {
            "location": "/building-microservices/integration/#api-composition", 
            "text": "Each part of the UI communicates with a specific service via its API.   Downsides:   Little ability to tailor the responses for different sorts of devices.  If another team is creating the UI, making even small changes requires change requests to multiple teams.  This communication could also be fairly chatty. Opening lots of calls directly to services can be quite intensive for mobile devices.", 
            "title": "API composition"
        }, 
        {
            "location": "/building-microservices/integration/#ui-fragment-composition", 
            "text": "Rather than having our UI make API calls and map everything back to UI controls, we could have our services provide parts of the UI directly.   The same team that makes changes to the services can also be in charge of making changes to those parts of the UI, allowing us to get changes out faster.  Downsides:   We need to ensure consistency of the user experience, CSS and HTML style guides can help.  Not ideal for native interfaces, it would require falling back to the API composition model.  The more cross-cutting a form of interaction is, the less likely this model will fit, falling back to the API composition model.", 
            "title": "UI Fragment composition"
        }, 
        {
            "location": "/building-microservices/integration/#backends-for-frontends", 
            "text": "A common solution to the problem of chatty interfaces with backend services, or the need to vary content for different types of devices, is to have a server-side aggregation endpoint, or  API gateway .   The problem that can occur is that normally we\u2019ll have one giant layer for all our services, losing ability to deploy clients independently.  A model that solves this problem is  Backends for frontends  ( BFFs ), it restricts the use of backends for a specific client.   The danger with this approach is the same as with any aggregating layer: it can take on logic it shouldn\u2019t. These BFFs should only contain behavior specific to delivering a particular user experience.", 
            "title": "Backends for Frontends"
        }, 
        {
            "location": "/building-microservices/integration/#a-hybrid-approach", 
            "text": "Some systems use different models together (e.g. BFFs for mobile and UI fragment composition for web). The tricky part still remains avoiding putting too much logic into any intermediate layer. This causes coupling and low cohesion.", 
            "title": "A Hybrid Approach"
        }, 
        {
            "location": "/building-microservices/integration/#integrating-with-third-party-software", 
            "text": "Challenges associated with integrating third-party software into your system:   Lack of control:  probably many of the technical decisions have been made for you to simplify product usage. The tool selection process should take into account ease of use of third-party software.  Customization:  many enterprise tools sell themselves on their ability to be heavily customized just for you. But the cost of customization can be more expensive than building something bespoke from scratch.  Integration spaghetti:  ideally you want to standardize on a few types of integration. If one product forces you tu use proprietary protocols, it could mean troubles.   Best practices:   Treat third-party software as a service and place all the customization code in services you control, if possible.  When moving away from integrated COTS or legacy software, adopt the  Strangler Application Pattern : intercept calls to such software and route them either to the legacy services or to your new services. This allows for a gradual switch.", 
            "title": "Integrating with Third-Party Software"
        }, 
        {
            "location": "/building-microservices/integration/#summary", 
            "text": "To ensure our microservices remain as decoupled as possible from their other collaborators:   Avoid database integration at all costs.  Understand the trade-offs between REST and RPC, but strongly consider REST as a good starting point for request/response integration.  Prefer choreography over orchestration.  Avoid breaking changes and the need to version by understanding Postel\u2019s Law and using tolerant readers.  Think of user interfaces as compositional layers.", 
            "title": "Summary"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/", 
            "text": "Splitting the monolith\n\n\nWhy would you want to split a monolith?\n\n\n\n\nThere are lots of changes coming to a part of the monolith, splitting that part into a service will make you roll out those changes faster.\n\n\nSeparate teams work on separate parts of the monolith.\n\n\nA part of the monolith requires high security measures not needed by the rest of the system.\n\n\nA part of the monolith can be improved by switching technology.\n\n\n\n\nHow do we go about decomposing monolithic applications without having to embark on a big-bang rewrite?\n\n\nSeams\n\n\nWe want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both.\n\n\nA seam is a portion of the code that can be treated in isolation and worked on without impacting the rest of the codebase. Bounded contexts are good seams.\n\n\nSo when splitting, the first step is to identify seams in our system and then gradually move the code of these seams into different packages. Tests are really useful to make sure you're not introducing bugs with this packaging. This process will also help identify seams that you did not think of: they will come out when you are left with some code that you don't know in which package to place.\n\n\nThe splitting should start from the seam that is least depended on.\n\n\nDatabases\n\n\nWe have to find seams in databases too, but this is a difficult task.\n\n\nAfter having packaged your application code by seams, you should do the same for the code accessing the database (usually the code in the so called \nrepository layer\n).\n\n\nForeign keys\n\n\nSome tables may have foreign keys linking them to other tables. A common solution for this problem is to remove the table relationship and make the service accessing that table call the API of the service handling the other table.\n\n\n\n\nShared static data\n\n\nLet's suppose we have different services accessing a table filled with static data.\n\n\n\n\nThere are several solutions:\n\n\n\n\nDuplicate tables in each db, but this can cause consistency issues.\n\n\nTreat static data as code/configuration files in each service. This can cause consistency issues too, but they would be far easier to solve.\n\n\nCreate a microservice to handle the static data. This is overkill in most situations, but it can be justified if the static data has high complexity.\n\n\n\n\nShared mutable data\n\n\nLet's suppose we have different services accessing a table filled with mutable data.\n\n\n\n\nUsually this means we need a \nCustomer\n microservice to handle that data. This service can then be called by \nWarehouse\n and \nFinance\n.\n\n\nShared tables\n\n\nLet's suppose we have different services accessing a table which aggregates different information in the same record (catalog entry and stock level).\n\n\n\n\nThe answer here is to split the table in two, creating a stock levels table for the \nWarehouse\n and a catalog entry table for the \nCatalog\n.\n\n\nStaging the break\n\n\nThe best way to commit the database changes would be to keep the services together and split the schemas. The db split will increase the number of db calls and make you lose transactional integrity. Having the same application will enable you to deal more easily with these problems. Then, when you are satisfied with the new db, you can commit the changes.\n\n\nTransactional Boundaries\n\n\nTransactions allow us to say that operations either all happen together, or none of them happen.\n\n\nTransactions are typically used in databases, but they can be supported but other systems such as message brokers.\n\n\nSplitting schemas will cause the loss of transactional integrity in our system. There are several solutions to this problem:\n\n\n\n\nA \ntry again later\n mechanism, but this alone is not sufficient since it assumes that eventually a failed request will be successful. This is a form of \neventual consistency\n: rather than using a transactional boundary to ensure that the system is in a consistent state when the transaction completes, instead we accept that the system will get itself into a consistent state at some point in the future.\n\n\nCompensating transactions\n can be used to undo the committed transactions preceding a failed operation. But what if a compensating transaction fails? We would need other mechanism such as automated jobs or human administration. Also, this mechanism becomes more difficult to manage as the number of operations increases in transactions.\n\n\nDistributed transactions\n are transactions done across different process or network boundaries. They are orchestrated by a \ntransaction manager\n. The most common algorithm handling short-lived distributed transactions is \ntwo-phase commit\n. With a two-phase commit, first comes the voting phase: each participant in the distributed transaction tells the transaction manager whether it thinks its local transaction can be completed. If the transaction manager gets a yes vote from everyone, then it tells them all to go ahead and perform their commits. A single no vote is enough for the transaction manager to send out a rollback to all parties. Distributed transactions make scaling systems much more difficult, since the transaction manager is a single point of failure and waiting for response while locking resources can cause outages. Also, there is no guarantee that the transactions are actually committed when the clients approve them.\n\n\n\n\nEach of these solutions adds complexity. Before implementing business operations happening in a transaction, ask yourself: can they happen in different, local transactions, and rely on the concept of eventual consistency? These systems are much easier to build and scale.\n\n\nIf you do encounter state that really needs to be kept consistent, try to avoid splitting it. If you really need to split it, try moving from a purely technical view of the process (e.g., a database transaction) and actually create a concrete concept to represent the transaction. This gives you a hook on which to run other operations like compensating transactions, and a way to monitor and manage these more complex concepts in your system.\n\n\nReporting\n\n\nWhen splitting data, we'll come across the problem of splitting reporting data too.\n\n\nThe Reporting Database\n\n\nIn monolithic systems, aggregating data for reporting is easy. Usually reporting is implemented like this:\n\n\n\n\nBenefits:\n\n\n\n\nAll data is one place so it's easy to query it.\n\n\n\n\nDownsides:\n\n\n\n\nThe db schema is a shared API between the monolith and the reporting service.\n\n\nCannot optimize schema structure for both use cases. Either the db is optimized for the monolith or the reporting.\n\n\nCannot use different technology that could be more efficient for reporting.\n\n\n\n\nThere are several alternatives to this approach when our data is distributed across different services.\n\n\nData Retrieval via Service Calls\n\n\nA very simple approach: call service APIs and aggregate the results for reporting.\n\n\nBenefits:\n\n\n\n\nEasy to implement and works well for small volumes of data (e.g. #orders placed in the last 15 minutes).\n\n\n\n\nDownsides:\n\n\n\n\nBreaks down when trying to do reporting with large volumes of data (e.g. customer behavior of last 24 months).\n\n\nReporting systems usually need to integrate with third-party tools over SQL-like interfaces, this approach would require extra work.\n\n\nThe API may not have been designed for reporting, leading to an inefficient reporting system and general slowdown. Caching can help, but reporting data is usually historic so there would be a lot of expensive cache misses. Adding reporting-specific APIs can help.\n\n\n\n\nData Pumps\n\n\nRather than have the reporting system pull the data, the data can instead be pushed to the reporting system. This \ndata pump\n needs to have intimate knowledge of both the internal database for the service, and also the reporting schema. The pump\u2019s job is to map one from the other.\n\n\n\n\nBenefits:\n\n\n\n\nCan handle large amounts of data without maintaining a reporting-specific API.\n\n\n\n\nDownsides:\n\n\n\n\nCauses coupling with the reporting db schema. The reporting service must be treated as a published API that is hard to change. There is also a potential mitigation: exposing only specific schemas that are mapped to an underlying monolithic schema, but this can cause performance issues depending on the db technology choice.\n\n\n\n\nEvent Data Pump\n\n\nWe can write a subscriber listening to microservices events that pushes data in the reporting db.\n\n\nBenefits:\n\n\n\n\nAvoids coupling between db schemas.\n\n\nCan see reported data as it happens, opposed to wait for a scheduled data transfer.\n\n\nIt is easier to only process new events (i.e. \ndeltas\n), while with a data pump we would need to write the code ourselves.\n\n\nThe event mapper can be managed by a different team, and it can evolve independently of the services.\n\n\n\n\nDownsides:\n\n\n\n\nAll information must be broadcast as event. It may not scale well with large volumes of data, for which a data pump is more efficient.\n\n\n\n\nBackup data pump\n\n\nUsing backup data as a source for reporting. This approach was taken by Netflix: backed up Cassandra tables would be stored in Amazon's S3 object store and accessed by Hadoop for reporting. This ended up as a tool named \nAegisthus\n.\n\n\nBenefits:\n\n\n\n\nCan handle enormous amounts of data.\n\n\nEfficient if there is already a backup system in place.\n\n\n\n\nDownsides:\n\n\n\n\nHas coupling with the reporting db schema.\n\n\n\n\nSummary\n\n\nWe decompose our system by finding seams along which service boundaries can emerge, and this can be an incremental approach. This way, costs of errors are mitigated and we can continue to evolve the system as we proceed.", 
            "title": "Splitting the monolith"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#splitting-the-monolith", 
            "text": "Why would you want to split a monolith?   There are lots of changes coming to a part of the monolith, splitting that part into a service will make you roll out those changes faster.  Separate teams work on separate parts of the monolith.  A part of the monolith requires high security measures not needed by the rest of the system.  A part of the monolith can be improved by switching technology.   How do we go about decomposing monolithic applications without having to embark on a big-bang rewrite?", 
            "title": "Splitting the monolith"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#seams", 
            "text": "We want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both.  A seam is a portion of the code that can be treated in isolation and worked on without impacting the rest of the codebase. Bounded contexts are good seams.  So when splitting, the first step is to identify seams in our system and then gradually move the code of these seams into different packages. Tests are really useful to make sure you're not introducing bugs with this packaging. This process will also help identify seams that you did not think of: they will come out when you are left with some code that you don't know in which package to place.  The splitting should start from the seam that is least depended on.", 
            "title": "Seams"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#databases", 
            "text": "We have to find seams in databases too, but this is a difficult task.  After having packaged your application code by seams, you should do the same for the code accessing the database (usually the code in the so called  repository layer ).", 
            "title": "Databases"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#foreign-keys", 
            "text": "Some tables may have foreign keys linking them to other tables. A common solution for this problem is to remove the table relationship and make the service accessing that table call the API of the service handling the other table.", 
            "title": "Foreign keys"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#shared-static-data", 
            "text": "Let's suppose we have different services accessing a table filled with static data.   There are several solutions:   Duplicate tables in each db, but this can cause consistency issues.  Treat static data as code/configuration files in each service. This can cause consistency issues too, but they would be far easier to solve.  Create a microservice to handle the static data. This is overkill in most situations, but it can be justified if the static data has high complexity.", 
            "title": "Shared static data"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#shared-mutable-data", 
            "text": "Let's suppose we have different services accessing a table filled with mutable data.   Usually this means we need a  Customer  microservice to handle that data. This service can then be called by  Warehouse  and  Finance .", 
            "title": "Shared mutable data"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#shared-tables", 
            "text": "Let's suppose we have different services accessing a table which aggregates different information in the same record (catalog entry and stock level).   The answer here is to split the table in two, creating a stock levels table for the  Warehouse  and a catalog entry table for the  Catalog .", 
            "title": "Shared tables"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#staging-the-break", 
            "text": "The best way to commit the database changes would be to keep the services together and split the schemas. The db split will increase the number of db calls and make you lose transactional integrity. Having the same application will enable you to deal more easily with these problems. Then, when you are satisfied with the new db, you can commit the changes.", 
            "title": "Staging the break"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#transactional-boundaries", 
            "text": "Transactions allow us to say that operations either all happen together, or none of them happen.  Transactions are typically used in databases, but they can be supported but other systems such as message brokers.  Splitting schemas will cause the loss of transactional integrity in our system. There are several solutions to this problem:   A  try again later  mechanism, but this alone is not sufficient since it assumes that eventually a failed request will be successful. This is a form of  eventual consistency : rather than using a transactional boundary to ensure that the system is in a consistent state when the transaction completes, instead we accept that the system will get itself into a consistent state at some point in the future.  Compensating transactions  can be used to undo the committed transactions preceding a failed operation. But what if a compensating transaction fails? We would need other mechanism such as automated jobs or human administration. Also, this mechanism becomes more difficult to manage as the number of operations increases in transactions.  Distributed transactions  are transactions done across different process or network boundaries. They are orchestrated by a  transaction manager . The most common algorithm handling short-lived distributed transactions is  two-phase commit . With a two-phase commit, first comes the voting phase: each participant in the distributed transaction tells the transaction manager whether it thinks its local transaction can be completed. If the transaction manager gets a yes vote from everyone, then it tells them all to go ahead and perform their commits. A single no vote is enough for the transaction manager to send out a rollback to all parties. Distributed transactions make scaling systems much more difficult, since the transaction manager is a single point of failure and waiting for response while locking resources can cause outages. Also, there is no guarantee that the transactions are actually committed when the clients approve them.   Each of these solutions adds complexity. Before implementing business operations happening in a transaction, ask yourself: can they happen in different, local transactions, and rely on the concept of eventual consistency? These systems are much easier to build and scale.  If you do encounter state that really needs to be kept consistent, try to avoid splitting it. If you really need to split it, try moving from a purely technical view of the process (e.g., a database transaction) and actually create a concrete concept to represent the transaction. This gives you a hook on which to run other operations like compensating transactions, and a way to monitor and manage these more complex concepts in your system.", 
            "title": "Transactional Boundaries"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#reporting", 
            "text": "When splitting data, we'll come across the problem of splitting reporting data too.", 
            "title": "Reporting"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#the-reporting-database", 
            "text": "In monolithic systems, aggregating data for reporting is easy. Usually reporting is implemented like this:   Benefits:   All data is one place so it's easy to query it.   Downsides:   The db schema is a shared API between the monolith and the reporting service.  Cannot optimize schema structure for both use cases. Either the db is optimized for the monolith or the reporting.  Cannot use different technology that could be more efficient for reporting.   There are several alternatives to this approach when our data is distributed across different services.", 
            "title": "The Reporting Database"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#data-retrieval-via-service-calls", 
            "text": "A very simple approach: call service APIs and aggregate the results for reporting.  Benefits:   Easy to implement and works well for small volumes of data (e.g. #orders placed in the last 15 minutes).   Downsides:   Breaks down when trying to do reporting with large volumes of data (e.g. customer behavior of last 24 months).  Reporting systems usually need to integrate with third-party tools over SQL-like interfaces, this approach would require extra work.  The API may not have been designed for reporting, leading to an inefficient reporting system and general slowdown. Caching can help, but reporting data is usually historic so there would be a lot of expensive cache misses. Adding reporting-specific APIs can help.", 
            "title": "Data Retrieval via Service Calls"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#data-pumps", 
            "text": "Rather than have the reporting system pull the data, the data can instead be pushed to the reporting system. This  data pump  needs to have intimate knowledge of both the internal database for the service, and also the reporting schema. The pump\u2019s job is to map one from the other.   Benefits:   Can handle large amounts of data without maintaining a reporting-specific API.   Downsides:   Causes coupling with the reporting db schema. The reporting service must be treated as a published API that is hard to change. There is also a potential mitigation: exposing only specific schemas that are mapped to an underlying monolithic schema, but this can cause performance issues depending on the db technology choice.", 
            "title": "Data Pumps"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#event-data-pump", 
            "text": "We can write a subscriber listening to microservices events that pushes data in the reporting db.  Benefits:   Avoids coupling between db schemas.  Can see reported data as it happens, opposed to wait for a scheduled data transfer.  It is easier to only process new events (i.e.  deltas ), while with a data pump we would need to write the code ourselves.  The event mapper can be managed by a different team, and it can evolve independently of the services.   Downsides:   All information must be broadcast as event. It may not scale well with large volumes of data, for which a data pump is more efficient.", 
            "title": "Event Data Pump"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#backup-data-pump", 
            "text": "Using backup data as a source for reporting. This approach was taken by Netflix: backed up Cassandra tables would be stored in Amazon's S3 object store and accessed by Hadoop for reporting. This ended up as a tool named  Aegisthus .  Benefits:   Can handle enormous amounts of data.  Efficient if there is already a backup system in place.   Downsides:   Has coupling with the reporting db schema.", 
            "title": "Backup data pump"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#summary", 
            "text": "We decompose our system by finding seams along which service boundaries can emerge, and this can be an incremental approach. This way, costs of errors are mitigated and we can continue to evolve the system as we proceed.", 
            "title": "Summary"
        }, 
        {
            "location": "/building-microservices/deployment/", 
            "text": "Deployment\n\n\nDeployment in microservices differs from monolithic systems. It's important to have a working approach following the continuous integration and delivery practices.\n\n\nMapping Continuous Integration to Microservices\n\n\nThe goal is to be able to deploy microservices independently.\nSo, how to map microservices to builds and code repositories? We have different options:\n\n\nSingle repository and single build\n\n\nUse a single repository to store all our code, and have a single build, triggered on every code integration, that produces every build artifact we need.\n\n\nBenefits:\n\n\n\n\nEasy to implement.\n\n\nEasy to commit changes.\n\n\n\n\nDownsides:\n\n\n\n\nA small change to a single service will trigger builds we do not need.\n\n\nWhat services do we need to deploy? Hard to determine which services changed by only looking at the pushed commit.\n\n\nIf a commit breaks the build, the build needs to be fixed before any other team can push code in the repository, locking those teams.\n\n\n\n\nSingle repository and multiple builds\n\n\nA variation of the previous approach is to have a single repository but setup multiple CI builds mapping to parts of the source code.\n\n\nBenefits:\n\n\n\n\nEasy to commit changes.\n\n\n\n\nDownsides:\n\n\n\n\nDevelopers can get into the habit of making changes to different services in the same commit.\n\n\n\n\nMultiple repositories and multiple builds\n\n\nEach microservice has its own repository and CI build.\n\n\nBenefits:\n\n\n\n\nOnly needed builds and tests are run when the build is triggered.\n\n\nA team can own the repository that it's working on.\n\n\n\n\nDownsides:\n\n\n\n\nMaking changes across microservices is more difficult, but this is still preferable to the single repository approach.\n\n\n\n\nBuild Pipelines and Continuous Delivery\n\n\nIn build processes usually there are a lot of fast, small-scoped tests, and a few large-scoped, slow tests. We will not get fast feedback when our fast tests fail if we're waiting for the other tests to finish. Also, if the fast tests fail, there is no need to run other tests.\n\n\nA solution to this problem is to have different stages in our build, i.e. a \nbuild pipeline\n.\nBuild pipelines allow to track the software as it goes through each build stage, giving a clear idea of its stability.\n\n\nIn \ncontinuous delivery\n (\nCD\n) we get constant feedback on the production readiness of each and every check-in, and treat each and every check-in as a release candidate. So clearly \nCD\n benefits from build pipelines.\nIn microservices with CI, we want one pipeline per service, in which a single artifact will move through our path to production.\n\n\nExceptions to Continuous Delivery\n\n\nIn the starting stage of a project, a single repository and single build approach may be more convenient since developers are not confident with the domain yet because the service boundaries are likely to change a lot. In this case, having a multi repository model will increase a lot the cost of these changes.\nThen, when the development team acquires experience in the domain, it can start moving out services in their own repositories and build pipelines.\n\n\nPlatform-Specific Artifacts\n\n\nSome artifacts are platform-specific (e.g. JAR files). This means that they need a specific configuration and a specific platform to be run in an environment.\nTools like \nPuppet\n and \nChef\n can help to automate this process.\n\n\nOperating System Artifacts\n\n\nAnother option for platform-specific artifacts is to use os-specific artifacts. This way, the OS can manage dependencies, installation and removal of your software.\n\n\nThe downside is in actually creating these packages, because the difficulty depends on the target OS (e.g. teams using Windows, not known for package management capabilities, may be unhappy with this approach).\n\n\nAnother downside is that if you need to deploy on different OS, there is an increase in complexity in your build and test process.\n\n\nCustom images\n\n\nThe problem with tools like Puppet and Chef is that \nthey take time to provision a machine\n. They need to install platforms (e.g. JVM) or perform expensive checks on the system to detect if a valid platform version is already installed.\n\n\nAnd if we're using an on-demand compute platform we might be constantly shutting down and spinning up new instances frequently, making the time cost of these tools really high.\n\n\nIf you need to install the same tools multiple times per day (e.g. because of CI) this becomes a real problem in terms of providing fast feedback. It can also lead to increased downtime when deploying in production if your systems do not allow zero-downtime deployment (\nblue/green deployment\n can help mitigate this issue).\n\n\nOne approach to \nreducing the provisioning time\n is to create a virtual machine image that bakes in some common dependencies we use. When we want to deploy our software, we spin up an instance of this custom image, and all we have to do is install the latest version of our service.\n\n\nWhen you launch new copies of this image you don't need to spend time installing your dependencies, as they are already there. This can result in significant time savings.\n\n\nThere are drawbacks too:\n\n\n\n\nBuild times are increased.\n\n\nResulting images can be very large, making it hard to move them across the network.\n\n\nThe image build process differs from platform to platform (e.g. VMWare images, Vagrant images).\n  Tools like \nPacker\n can help.\n\n\n\n\nAs we'll see later, container technology mitigates these drawbacks.\n\n\nImages as Artifacts\n\n\nWhy stop at including only dependencies in these images? We can also include our software in it.\nThis will make our software platform agnostic and it is a good way to start implementing the \nimmutable server\n deployment concept.\n\n\nImmutable Servers\n\n\nTo keep our servers immutable we also must be sure that no one is able to access them after they've been deployed (e.g. by disabling \nSSH\n in the image artifact).\nOtherwise, the configuration could be edited, causing a \nconfiguration drift\n.\nIf we want to have environments that are easy to reason about, every configuration change must pass through a build pipeline.\n\n\nEnvironments\n\n\nOur microservice artifact will move in different environments during the CD pipeline.\nUsually these are:\n\n\n\n\nSlow tests environment.\n\n\nUAT environment.\n\n\nPerformance/load test environment.\n\n\nProduction environment.\n\n\n\n\nAs you go on in the pipeline, you want the environments to look more like the production environment, allowing us to catch production problems before they happen in production.\nBut consider that production environments are more expensive and slower to set up. So you should balance the ability to find production-like bugs with the ability to get fast feedback from builds.\n\n\nService configuration\n\n\nOur services need some configuration (e.g. db username and password). Ideally this should be a small amount of data. Also, it's best to minimize configuration that changes between environments, so that you minimize chances for environment-specific bugs.\nBut how to handle this kind of configuration?\n\n\n\n\nBundling the configuration in your build artifacts is to be avoided because it violates the principles of \nCD\n.\n  In this case it would be hard to avoid having sensitive data (e.g. passwords) in your source code.\n  Also, build times are increased since you now have more images.\n  Then you have to know at build time which environments exist, coupling the build process with the delivery process.\n\n\nCreate a single artifact and place configuration files in environments or use a dedicated system for providing configuration (a popular approach in microservices).\n\n\n\n\nService-to-Host Mapping\n\n\nIn this era of virtualization, the mapping between a single host running an operating system and the underlying physical infrastructure can vary a lot.\n\n\nLet's define \nhost\n to be the generic unit of isolation, i.e. an operating system onto which you can install and run your services.\n\n\nSo how many services per host should we have? There are different options.\n\n\nMultiple Services Per Host\n\n\nHaving multiple instances of your service per host.\n\n\n\n\nBenefits:\n\n\n\n\nSimpler work for the team that manages the infrastructure.\n\n\nUsing host virtualization can add overhead and thus increase costs.\n\n\nEasier for developers to deploy: a deploy with this setup works like a deploy to a dev machine.\n\n\n\n\nDownsides:\n\n\n\n\nMake monitoring more difficult (e.g. monitor the host CPU usage or each instance?).\n\n\nCauses side effects (e.g. when a service is under heavy load, it's likely some other service instances will slow down too).\n\n\nNeed to ensure that a service deployment does not affect other services on the same host.\n  Usually this is solved by deploying all service in one step, thus losing ability to deploy independently.\n\n\nAutonomy of teams is inhibited in case services of different teams are deployed to the same host.\n\n\nCannot deploy images and immutable servers.\n\n\nIt can be complicated to target scaling at a service in a host.\n\n\nIf a service handles sensitive data or has different needs (e.g. another network segment), you cannot deploy it with the others.\n\n\n\n\nApplication Containers\n\n\nUse an application container (e.g. IIS or Java servlet container) that provides utilities such as management, monitoring and scaling of services.\n\n\n\n\nBenefits:\n\n\n\n\nHas too for managing monitoring, scaling and other aspects.\n\n\nIf all services require the same runtime, this approach reduces overhead (e.g. for \nn\n Java services only a single JVM instance is needed).\n\n\n\n\nDownsides:\n\n\n\n\nTechnology choice and tools that automate services management are constrained. Losing automation here means having to do a lot of work in managing services.\n\n\nUsually slow spin-up times, slowing feedback for developers.\n\n\nAnalyzing resources use is hard, as you have multiple applications sharing a single process.\n\n\nApplication containers have their own resource consumption overhead.\n\n\n\n\nSingle Service per Host\n\n\nA host contains only a single service.\n\n\n\n\nBenefits:\n\n\n\n\nEasier to monitor resources usage.\n\n\nEasier to avoid the side effects of having multiple services in a single host.\n\n\nReduces complexity of your system.\n\n\n\n\nDownsides:\n\n\n\n\nMore hosts mean more servers to manage and costs might increase.\n\n\n\n\nYou can mitigate the complexity of managing more hosts by using a \nplatform as a service\n (PaaS). This way, the host management problem is simplified, but you lose control over your hosts.\n\n\nTip: some PaaS try to automate too much (e.g. automate scaling), making them less effective for your specific use case.\n\n\nAutomation\n\n\nAutomation is the solution to many of the problems we have raised so far.\n\n\nOne of the pushbacks for switching to single service per host is the perception that the amount of overhead for management will increase. If you do everything manually, it surely will, but automation will prevent this issue.\n\n\nAutomation also allow developers to be productive, especially if they have access to the same technologies used in production because it will help catch bugs early on.\n\n\nEmbracing a culture of automation is key if you want to keep the complexities of microservice architectures in check.\n\n\nFrom Physical to Virtual\n\n\nOne of the key tools available to us in managing many hosts is finding ways of chunking up existing physical machines into smaller parts.\n\n\nTraditional Virtualization\n\n\nHaving lots of hosts can be really expensive if you need a physical server per host. By virtualizing you can split a physical machine in separate parts but of course this comes with an overhead.\n\n\nFor example, in \nType 2\n virtualization, the \nhypervisor\n sets aside resources for each virtual machine it manages, but these resources could be used for something else instead of being idle and reserved.\n\n\n\n\nVagrant\n\n\nA deployment platform usually employed for development and testing. It allows us to define instructions about how to setup and configure VMs. This makes it easier for you to create production-like environments on your local machine.\n\n\nOne of the downsides is that if we have one service to one VM, you may not be able to bring up your entire system on your local machine.\n\n\nLinux containers\n\n\nLinux containers, instead of using an hypervisor, create a separate process space in which other processes live.\n\n\n\n\nEach container is effectively a subtree of the overall system process tree. These containers can have physical resources allocated to them, something the kernel handles for us.\n\n\nBenefits:\n\n\n\n\nNo need for an hypervisor.\n\n\nMuch faster to provision than traditional VMs.\n\n\nFiner-grained control over assignation of resources.\n\n\nSince they are lighter than VMs, we can have more containers running on the same host.\n\n\n\n\nDownsides:\n\n\n\n\nThe host OS has to share the same kernel with the base OS.\n\n\nNot as isolated from other processes as VMs, not suitable for running code you don't trust.\n\n\nHow to expose containers to the outer world? A specific network configuration is needed, something that is usually provided by hypervisors.\n\n\n\n\nDocker\n\n\nDocker is a platform built on top of lightweight containers. Docker manages the container provisioning, handles some networking problems and provides its own registry that allows you to store and version Docker applications.\n\n\nDocker can also alleviate some downsides of running lots of services locally for dev and test purposes, in a more efficient way than Vagrant.\n\n\nSeveral technologies are build around the Docker concepts, such as \nCoreOS\n, a stripped-down Linux OS that provides only the essential services to allow Docker to run.\n\n\nDocker itself doesn\u2019t solve all problems for us. Think of it as a simple PaaS that works on a single machine. If you want tools to help you manage services across multiple Docker instances across multiple machines, you\u2019ll need to look at software such as \nKubernetes\n or CoreOS.\n\n\nA Deployment Interface\n\n\nWhatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital to easily deploy microservices to development, test, production and other environments.\n\n\nA good way to trigger deployments is via CLI tools, because it can be triggered by other scripts, used in CI and called manually.\n\n\nWe need some information for a deploy:\n\n\n\n\nWhat microservice we want to deploy.\n\n\nWhat version of said microservice we want to deploy.\n\n\nWhat environment we want our microservice deployed into.\n\n\n\n\nFor this to work, we need to define in some way what our environments look like. YAML could be a good way of expressing our environments definitions.\n\n\nSummary\n\n\nMain points collected in this chapter:\n\n\n\n\nMaintain the ability to deploy microservices independently.\n\n\nSeparate source code and CI builds for each microservices.\n\n\nUse a single-service per host/container model. Evaluate the tooling aiming for high levels of automation.\n\n\nUnderstand how deployment choices affects developers. Creating tools that make it easy to deploy to different environments helps a lot.", 
            "title": "Deployment"
        }, 
        {
            "location": "/building-microservices/deployment/#deployment", 
            "text": "Deployment in microservices differs from monolithic systems. It's important to have a working approach following the continuous integration and delivery practices.", 
            "title": "Deployment"
        }, 
        {
            "location": "/building-microservices/deployment/#mapping-continuous-integration-to-microservices", 
            "text": "The goal is to be able to deploy microservices independently.\nSo, how to map microservices to builds and code repositories? We have different options:", 
            "title": "Mapping Continuous Integration to Microservices"
        }, 
        {
            "location": "/building-microservices/deployment/#single-repository-and-single-build", 
            "text": "Use a single repository to store all our code, and have a single build, triggered on every code integration, that produces every build artifact we need.  Benefits:   Easy to implement.  Easy to commit changes.   Downsides:   A small change to a single service will trigger builds we do not need.  What services do we need to deploy? Hard to determine which services changed by only looking at the pushed commit.  If a commit breaks the build, the build needs to be fixed before any other team can push code in the repository, locking those teams.", 
            "title": "Single repository and single build"
        }, 
        {
            "location": "/building-microservices/deployment/#single-repository-and-multiple-builds", 
            "text": "A variation of the previous approach is to have a single repository but setup multiple CI builds mapping to parts of the source code.  Benefits:   Easy to commit changes.   Downsides:   Developers can get into the habit of making changes to different services in the same commit.", 
            "title": "Single repository and multiple builds"
        }, 
        {
            "location": "/building-microservices/deployment/#multiple-repositories-and-multiple-builds", 
            "text": "Each microservice has its own repository and CI build.  Benefits:   Only needed builds and tests are run when the build is triggered.  A team can own the repository that it's working on.   Downsides:   Making changes across microservices is more difficult, but this is still preferable to the single repository approach.", 
            "title": "Multiple repositories and multiple builds"
        }, 
        {
            "location": "/building-microservices/deployment/#build-pipelines-and-continuous-delivery", 
            "text": "In build processes usually there are a lot of fast, small-scoped tests, and a few large-scoped, slow tests. We will not get fast feedback when our fast tests fail if we're waiting for the other tests to finish. Also, if the fast tests fail, there is no need to run other tests.  A solution to this problem is to have different stages in our build, i.e. a  build pipeline .\nBuild pipelines allow to track the software as it goes through each build stage, giving a clear idea of its stability.  In  continuous delivery  ( CD ) we get constant feedback on the production readiness of each and every check-in, and treat each and every check-in as a release candidate. So clearly  CD  benefits from build pipelines.\nIn microservices with CI, we want one pipeline per service, in which a single artifact will move through our path to production.", 
            "title": "Build Pipelines and Continuous Delivery"
        }, 
        {
            "location": "/building-microservices/deployment/#exceptions-to-continuous-delivery", 
            "text": "In the starting stage of a project, a single repository and single build approach may be more convenient since developers are not confident with the domain yet because the service boundaries are likely to change a lot. In this case, having a multi repository model will increase a lot the cost of these changes.\nThen, when the development team acquires experience in the domain, it can start moving out services in their own repositories and build pipelines.", 
            "title": "Exceptions to Continuous Delivery"
        }, 
        {
            "location": "/building-microservices/deployment/#platform-specific-artifacts", 
            "text": "Some artifacts are platform-specific (e.g. JAR files). This means that they need a specific configuration and a specific platform to be run in an environment.\nTools like  Puppet  and  Chef  can help to automate this process.", 
            "title": "Platform-Specific Artifacts"
        }, 
        {
            "location": "/building-microservices/deployment/#operating-system-artifacts", 
            "text": "Another option for platform-specific artifacts is to use os-specific artifacts. This way, the OS can manage dependencies, installation and removal of your software.  The downside is in actually creating these packages, because the difficulty depends on the target OS (e.g. teams using Windows, not known for package management capabilities, may be unhappy with this approach).  Another downside is that if you need to deploy on different OS, there is an increase in complexity in your build and test process.", 
            "title": "Operating System Artifacts"
        }, 
        {
            "location": "/building-microservices/deployment/#custom-images", 
            "text": "The problem with tools like Puppet and Chef is that  they take time to provision a machine . They need to install platforms (e.g. JVM) or perform expensive checks on the system to detect if a valid platform version is already installed.  And if we're using an on-demand compute platform we might be constantly shutting down and spinning up new instances frequently, making the time cost of these tools really high.  If you need to install the same tools multiple times per day (e.g. because of CI) this becomes a real problem in terms of providing fast feedback. It can also lead to increased downtime when deploying in production if your systems do not allow zero-downtime deployment ( blue/green deployment  can help mitigate this issue).  One approach to  reducing the provisioning time  is to create a virtual machine image that bakes in some common dependencies we use. When we want to deploy our software, we spin up an instance of this custom image, and all we have to do is install the latest version of our service.  When you launch new copies of this image you don't need to spend time installing your dependencies, as they are already there. This can result in significant time savings.  There are drawbacks too:   Build times are increased.  Resulting images can be very large, making it hard to move them across the network.  The image build process differs from platform to platform (e.g. VMWare images, Vagrant images).\n  Tools like  Packer  can help.   As we'll see later, container technology mitigates these drawbacks.", 
            "title": "Custom images"
        }, 
        {
            "location": "/building-microservices/deployment/#images-as-artifacts", 
            "text": "Why stop at including only dependencies in these images? We can also include our software in it.\nThis will make our software platform agnostic and it is a good way to start implementing the  immutable server  deployment concept.", 
            "title": "Images as Artifacts"
        }, 
        {
            "location": "/building-microservices/deployment/#immutable-servers", 
            "text": "To keep our servers immutable we also must be sure that no one is able to access them after they've been deployed (e.g. by disabling  SSH  in the image artifact).\nOtherwise, the configuration could be edited, causing a  configuration drift .\nIf we want to have environments that are easy to reason about, every configuration change must pass through a build pipeline.", 
            "title": "Immutable Servers"
        }, 
        {
            "location": "/building-microservices/deployment/#environments", 
            "text": "Our microservice artifact will move in different environments during the CD pipeline.\nUsually these are:   Slow tests environment.  UAT environment.  Performance/load test environment.  Production environment.   As you go on in the pipeline, you want the environments to look more like the production environment, allowing us to catch production problems before they happen in production.\nBut consider that production environments are more expensive and slower to set up. So you should balance the ability to find production-like bugs with the ability to get fast feedback from builds.", 
            "title": "Environments"
        }, 
        {
            "location": "/building-microservices/deployment/#service-configuration", 
            "text": "Our services need some configuration (e.g. db username and password). Ideally this should be a small amount of data. Also, it's best to minimize configuration that changes between environments, so that you minimize chances for environment-specific bugs.\nBut how to handle this kind of configuration?   Bundling the configuration in your build artifacts is to be avoided because it violates the principles of  CD .\n  In this case it would be hard to avoid having sensitive data (e.g. passwords) in your source code.\n  Also, build times are increased since you now have more images.\n  Then you have to know at build time which environments exist, coupling the build process with the delivery process.  Create a single artifact and place configuration files in environments or use a dedicated system for providing configuration (a popular approach in microservices).", 
            "title": "Service configuration"
        }, 
        {
            "location": "/building-microservices/deployment/#service-to-host-mapping", 
            "text": "In this era of virtualization, the mapping between a single host running an operating system and the underlying physical infrastructure can vary a lot.  Let's define  host  to be the generic unit of isolation, i.e. an operating system onto which you can install and run your services.  So how many services per host should we have? There are different options.", 
            "title": "Service-to-Host Mapping"
        }, 
        {
            "location": "/building-microservices/deployment/#multiple-services-per-host", 
            "text": "Having multiple instances of your service per host.   Benefits:   Simpler work for the team that manages the infrastructure.  Using host virtualization can add overhead and thus increase costs.  Easier for developers to deploy: a deploy with this setup works like a deploy to a dev machine.   Downsides:   Make monitoring more difficult (e.g. monitor the host CPU usage or each instance?).  Causes side effects (e.g. when a service is under heavy load, it's likely some other service instances will slow down too).  Need to ensure that a service deployment does not affect other services on the same host.\n  Usually this is solved by deploying all service in one step, thus losing ability to deploy independently.  Autonomy of teams is inhibited in case services of different teams are deployed to the same host.  Cannot deploy images and immutable servers.  It can be complicated to target scaling at a service in a host.  If a service handles sensitive data or has different needs (e.g. another network segment), you cannot deploy it with the others.", 
            "title": "Multiple Services Per Host"
        }, 
        {
            "location": "/building-microservices/deployment/#application-containers", 
            "text": "Use an application container (e.g. IIS or Java servlet container) that provides utilities such as management, monitoring and scaling of services.   Benefits:   Has too for managing monitoring, scaling and other aspects.  If all services require the same runtime, this approach reduces overhead (e.g. for  n  Java services only a single JVM instance is needed).   Downsides:   Technology choice and tools that automate services management are constrained. Losing automation here means having to do a lot of work in managing services.  Usually slow spin-up times, slowing feedback for developers.  Analyzing resources use is hard, as you have multiple applications sharing a single process.  Application containers have their own resource consumption overhead.", 
            "title": "Application Containers"
        }, 
        {
            "location": "/building-microservices/deployment/#single-service-per-host", 
            "text": "A host contains only a single service.   Benefits:   Easier to monitor resources usage.  Easier to avoid the side effects of having multiple services in a single host.  Reduces complexity of your system.   Downsides:   More hosts mean more servers to manage and costs might increase.   You can mitigate the complexity of managing more hosts by using a  platform as a service  (PaaS). This way, the host management problem is simplified, but you lose control over your hosts.  Tip: some PaaS try to automate too much (e.g. automate scaling), making them less effective for your specific use case.", 
            "title": "Single Service per Host"
        }, 
        {
            "location": "/building-microservices/deployment/#automation", 
            "text": "Automation is the solution to many of the problems we have raised so far.  One of the pushbacks for switching to single service per host is the perception that the amount of overhead for management will increase. If you do everything manually, it surely will, but automation will prevent this issue.  Automation also allow developers to be productive, especially if they have access to the same technologies used in production because it will help catch bugs early on.  Embracing a culture of automation is key if you want to keep the complexities of microservice architectures in check.", 
            "title": "Automation"
        }, 
        {
            "location": "/building-microservices/deployment/#from-physical-to-virtual", 
            "text": "One of the key tools available to us in managing many hosts is finding ways of chunking up existing physical machines into smaller parts.", 
            "title": "From Physical to Virtual"
        }, 
        {
            "location": "/building-microservices/deployment/#traditional-virtualization", 
            "text": "Having lots of hosts can be really expensive if you need a physical server per host. By virtualizing you can split a physical machine in separate parts but of course this comes with an overhead.  For example, in  Type 2  virtualization, the  hypervisor  sets aside resources for each virtual machine it manages, but these resources could be used for something else instead of being idle and reserved.", 
            "title": "Traditional Virtualization"
        }, 
        {
            "location": "/building-microservices/deployment/#vagrant", 
            "text": "A deployment platform usually employed for development and testing. It allows us to define instructions about how to setup and configure VMs. This makes it easier for you to create production-like environments on your local machine.  One of the downsides is that if we have one service to one VM, you may not be able to bring up your entire system on your local machine.", 
            "title": "Vagrant"
        }, 
        {
            "location": "/building-microservices/deployment/#linux-containers", 
            "text": "Linux containers, instead of using an hypervisor, create a separate process space in which other processes live.   Each container is effectively a subtree of the overall system process tree. These containers can have physical resources allocated to them, something the kernel handles for us.  Benefits:   No need for an hypervisor.  Much faster to provision than traditional VMs.  Finer-grained control over assignation of resources.  Since they are lighter than VMs, we can have more containers running on the same host.   Downsides:   The host OS has to share the same kernel with the base OS.  Not as isolated from other processes as VMs, not suitable for running code you don't trust.  How to expose containers to the outer world? A specific network configuration is needed, something that is usually provided by hypervisors.", 
            "title": "Linux containers"
        }, 
        {
            "location": "/building-microservices/deployment/#docker", 
            "text": "Docker is a platform built on top of lightweight containers. Docker manages the container provisioning, handles some networking problems and provides its own registry that allows you to store and version Docker applications.  Docker can also alleviate some downsides of running lots of services locally for dev and test purposes, in a more efficient way than Vagrant.  Several technologies are build around the Docker concepts, such as  CoreOS , a stripped-down Linux OS that provides only the essential services to allow Docker to run.  Docker itself doesn\u2019t solve all problems for us. Think of it as a simple PaaS that works on a single machine. If you want tools to help you manage services across multiple Docker instances across multiple machines, you\u2019ll need to look at software such as  Kubernetes  or CoreOS.", 
            "title": "Docker"
        }, 
        {
            "location": "/building-microservices/deployment/#a-deployment-interface", 
            "text": "Whatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital to easily deploy microservices to development, test, production and other environments.  A good way to trigger deployments is via CLI tools, because it can be triggered by other scripts, used in CI and called manually.  We need some information for a deploy:   What microservice we want to deploy.  What version of said microservice we want to deploy.  What environment we want our microservice deployed into.   For this to work, we need to define in some way what our environments look like. YAML could be a good way of expressing our environments definitions.", 
            "title": "A Deployment Interface"
        }, 
        {
            "location": "/building-microservices/deployment/#summary", 
            "text": "Main points collected in this chapter:   Maintain the ability to deploy microservices independently.  Separate source code and CI builds for each microservices.  Use a single-service per host/container model. Evaluate the tooling aiming for high levels of automation.  Understand how deployment choices affects developers. Creating tools that make it easy to deploy to different environments helps a lot.", 
            "title": "Summary"
        }, 
        {
            "location": "/building-microservices/testing/", 
            "text": "Testing\n\n\nDistributed systems add complexity in automated tests too.\n\n\nTypes of Tests\n\n\nTests can be categorized by the following diagram:\n\n\n\n\nIn microservices, the amount of manual tests should be kept at a minimum in order to reduce test times. Also, since there are no significant difference in manual testing, we will examine how automated testing changes from monolithic systems to microservices systems.\n\n\nTest scope\n\n\nThe Test Pyramid is a model proposed by Mike Cohn to associate the ideal amount of tests to each test scope.\n\n\n\n\nNote that terms like \nservice\n and \nunit\n in this context are ambiguous and we will refer to the \nUI\n layer as \nend-to-end\n tests.\n\n\nAs we go up the pyramid our confidence increases but we reduce the ability to pinpoint bug causes and have a slower feedback.\n\n\nIdeally, you want test of \ndifferent scopes for different purposes\n (e.g. you can catch an integration bug with an e2e test and then you can keep it covered with a unit test).\n\n\nHow many tests? It's better to increase the number of tests as you go down the pyramid. Doing the opposite has the potential to keep your build \nred\n for long times.\n\n\nUnit Tests\n\n\nThey typically test a single function or method call in isolation (i.e. without starting services or using external resources such as network connectivity).\n\n\nDone right, they can be very fast. You could run a lot of them in less than a minute.\n\n\nThese are \ntechnology-facing\n tests that will help us catch the most bugs and guide us through code restructuring thanks to their fast feedback and reliability.\n\n\nUnit tests are also easier to be implemented than other tests.\n\n\nService Tests\n\n\nThey are designed to bypass the user interface and test services directly.\n\n\nIn monolithic systems, a group of classes that provide a certain service to users can be tested together.\n\n\nIn microservices, we need to isolate the service we want to test so that we are able to quickly find the root cause of a bug. To achieve this isolation, we need to stub out other services interacting with the one under test.\nBut note that while a \nstubbed service\n does not care if it's called 1 or 100 times, a \nmocked service\n can provide you with that information so you could write more solid tests. The downside is that mocked services can make your tests brittle because of the magnitude of details tested.\n\n\nSo, after our service tests pass, we are confident that the new microservice is ready to contact other microservices with no errors. But what about other microservices calling the one we want to deploy?\n\n\nEnd-to-End Tests\n\n\nThey are run against the whole system, so they cover a lot of code and give you a lot of confidence that the system will work in production.\nOn the other hand, it's harder to diagnose an issue that comes up in e2e tests.\n\n\nThese tests are tricky to deal with, suppose we add them at the end of our deploy pipeline:\n\n\n\n\nThen we have 2 issues:\n\n\n\n\nWhich services version are we going to use in our tests?\n\n\nExecuting such a pipeline for each microservice is going to be really inefficient.\n\n\n\n\nBoth of them are solved with a \nfan in\n model:\n\n\n\n\nBut there are other disadvantages when using e2e tests:\n\n\n\n\nAs the scope increases, we might face more errors due to \ncauses unrelated to the behavior we want to test\n (e.g. network failures).\n\n\nWhen tests \nsometimes\n fail because of unrelated issues (these are called \nflaky tests\n), people will tend to re-run them without any effort to understand the errors. This will cause lots of scheduled builds and lead to a broken system because some issues (e.g. concurrency issues) may slip through this process as unrelated issues. Flaky tests will also cause a \nnormalization of deviance\n in the test system, so it's mandatory to remove them (or temporarily disable them to apply a fix) as soon as they're spotted.\n\n\n\n\nOwnership\n\n\nUsually the tests of a service are owned by the team developing the service. But in e2e tests there can be multiple services involved in a single test.\n\n\nAvoid:\n\n\n\n\nA \nfree for all\n approach, because no team would have a real ownership of any test, causing failing tests to be ignored or easily dismissed as responsibility of another team.\n\n\nA \ntest team\n whose job is solely to write e2e tests. A team like this would not know how to fix issues caught in tests and would cause the development team to become distant from the tests of their code.\n\n\n\n\nInstead, aim to share responsibilities between the teams involved in each single test: everyone should be able to add a new test and it must be clear who is responsible for the success of that test.\n\n\nSpeed\n\n\nWhen e2e tests are too slow:\n\n\n\n\nThey are more prone to unrelated issues happening during their execution, this makes them brittle tests.\n\n\nThe feedback cycle is slowed, so it takes more time to fix a failing build. This will cause builds to pile up: we will no longer able to quickly deploy small features.\n\n\n\n\nThings can be sped up by running tests in parallel, but it can only help to a certain extent.\n\n\nTo speed up your e2e tests, you should aim to remove useless tests and weigh risk/rewards of implemented tests to determine if they're worth having around. But this is a really difficult task since humans are not that good at estimating risks.\n\n\nThe Metaversion\n\n\nAfter running e2e tests one can start thinking, \nSo, I know all these services at these versions work together, so why not deploy them all together?\n.\nThis reasoning is to avoid because it usually leads to a proposal of a unified version number for the system, which in turn leads to coupling deploys. In the end, it will lead to a microservices system without the benefits of microservices.\n\n\nTest Journeys, Not Stories\n\n\nDespite the disadvantages, e2e test can be doable with one or two services. So what if we have 20 services?\n\n\nAvoid:\n\n\n\n\nAdding an e2e for each story to implement because it leads to slow feedback times and huge overlaps in test coverage.\n\n\n\n\nInstead, you should test on a few core journeys. Any functionality not covered in these journeys needs to be covered in tests that analyze services in isolation from each other.\n\n\nConsumer-Driven Tests\n\n\nWe use e2e to be sure that when a new service is deployed the system will keep working. Another way to approach this problem is by defining \nconsumer-driven contracts (CDC)\n, which express the expectations of the consumers of a service.\n\n\nThese tests can be run in isolation and are at the \nservice\n test scope, although they serve a different purpose from classic service tests: we are able to identify breaking changes and decide whether to introduce breaking changes in the involved consumer service too or think through it before deploying the new service.\n\n\n\n\nThe major benefit is that these tests are not as expensive as e2e tests, while they give us confidence that the new service does not break the expectations of its consumers.\n\n\nConsumer-driven tests can be implemented by the teams developing consumer services but this requires a good communication channel between the teams. If for some reason it's hard to communicate, they can be written by the team developing the new service.\n\n\nTesting after production\n\n\nMost testing is done before the system is in production. Still, when the system is in production:\n\n\n\n\nSome bugs may have slipped through our tests.\n\n\nNew failure modes are discovered.\n\n\nOur users use the system in ways we could never expect.\n\n\n\n\nOne reaction to this is often to define more and more tests. However, at a certain point we have we will hit diminishing returns with this approach.\n\n\nSeparating Deployment from Release\n\n\nIf we deploy the system in an environment prior to directing production loads against it, we are able to detect issues specific to the environment.\n\n\nIn this environment we can also run \nsmoke tests\n to make sure that the deploy was successful and there are no environmental issues.\n\n\nThere are different practices that you can adopt to follow this approach.\n\n\nBlue/Green deployment\n\n\nWith blue/green deployment we have two copies of our software deployed at a time, but only one version of it is receiving real requests.\n\n\nThis diagram describes how blue/green deployment works:\n\n\n\n\nBenefits:\n\n\n\n\nSmoke tests can be run after the deploy.\n\n\nIf we keep the old version running, we can quickly fallback to it if we detect errors in the new version. The \ndetect and revert\n process can be automated too.\n\n\nZero downtime between deploys can be achieved.\n\n\n\n\nDownsides:\n\n\n\n\nRequires some networking engineering but usually cloud providers support the needed functionalities.\n\n\n\n\nCanary Releasing\n\n\nWith canary releasing, we verify our newly deployed service by directing amounts of production traffic against it to see if it performs as expected.\n\n\nPerformance can be measured the way you prefer, some examples are:\n\n\n\n\nMeasuring response times.\n\n\nMeasuring error rates.\n\n\nMeasuring sales conversions of a new recommendation algorithm.\n\n\n\n\nWhen considering canary releasing, you need to decide if you are going to divert a portion of production requests to the canary or just copy production load.\n\n\nBenefits:\n\n\n\n\nHas all the benefits provided by blue/green deployment.\n\n\nLets you evaluate the performance of the new service according to custom/complex metrics.\n\n\n\n\nDownsides:\n\n\n\n\nMore difficult to setup than blue/green deployment.\n\n\nNeeds advanced network routing capabilities.\n\n\nMay need more computing power because of multiple services that have to run together for long times.\n\n\n\n\nMTBF vs MTTR\n\n\nBy using techniques such as blue/green and canary deployment we acknowledge that some issues can only be discovered in production: sometimes expending the same effort into getting better at remediation of a release can be significantly more beneficial than adding more automated tests.\nThis is referred to as the trade-off between optimizing for \nmean time between failures\n (\nMTBF\n) and \nmean time to repair\n (\nMTTR\n).\n\n\nFor different organizations, this trade-off between MTBF and MTTR will vary, and much of this lies with understanding the true impact of failure in a production environment.\n\n\nCross-Functional Testing\n\n\nNonfunctional requirements\n describe those characteristics your system exhibits that cannot simply be implemented like a normal feature. We will use the term \ncross-functional requirements\n \n(CFR)\n instead to refer to these tests.\n\n\nAs example, these are popular CFRs:\n\n\n\n\nLatency of a web server.\n\n\nMaximum concurrent users.\n\n\nHow secure the stored data should be.\n\n\n\n\nTests around CFRs should follow the pyramid too: some tests will have to be end-to-end (e.g. load tests) but others won\u2019t (e.g. tests to catch performance bottlenecks).\n\n\nIt's really important to consider CFRs from the start of the development process, because they shape too the design of your system.\n\n\nPerformance Tests\n\n\nPerformance tests are a way of ensuring that some of our CFRs can be met.\n\n\nIn microservices systems performance is critical since what would be a single database call in a monolithic system could now become 3-4 calls to different services.\n\n\nAs with functional tests, you may want different performance tests for each test scope.\n\n\nDue to the time it takes to run performance tests, it isn\u2019t always feasible to run them on every check-in. It is a common practice to run a subset every day, and a larger set every week.\n\n\nAlso, it's important to have targets so that the results of these tests can be correctly interpreted and evaluated (they may mark a build as failed is the performance level is below a certain threshold).\n\n\nSummary\n\n\nFundamental notions:\n\n\n\n\nOptimize for fast feedback, and separate types of tests accordingly.\n\n\nAvoid the need for end-to-end tests wherever possible by using consumer-driven contracts.\n\n\nUse consumer-driven contracts to provide focus points for conversations between teams.\n\n\nTry to understand the trade-off between putting more efforts into testing and detecting issues faster in production (optimizing for MTBF versus MTTR).", 
            "title": "Testing"
        }, 
        {
            "location": "/building-microservices/testing/#testing", 
            "text": "Distributed systems add complexity in automated tests too.", 
            "title": "Testing"
        }, 
        {
            "location": "/building-microservices/testing/#types-of-tests", 
            "text": "Tests can be categorized by the following diagram:   In microservices, the amount of manual tests should be kept at a minimum in order to reduce test times. Also, since there are no significant difference in manual testing, we will examine how automated testing changes from monolithic systems to microservices systems.", 
            "title": "Types of Tests"
        }, 
        {
            "location": "/building-microservices/testing/#test-scope", 
            "text": "The Test Pyramid is a model proposed by Mike Cohn to associate the ideal amount of tests to each test scope.   Note that terms like  service  and  unit  in this context are ambiguous and we will refer to the  UI  layer as  end-to-end  tests.  As we go up the pyramid our confidence increases but we reduce the ability to pinpoint bug causes and have a slower feedback.  Ideally, you want test of  different scopes for different purposes  (e.g. you can catch an integration bug with an e2e test and then you can keep it covered with a unit test).  How many tests? It's better to increase the number of tests as you go down the pyramid. Doing the opposite has the potential to keep your build  red  for long times.", 
            "title": "Test scope"
        }, 
        {
            "location": "/building-microservices/testing/#unit-tests", 
            "text": "They typically test a single function or method call in isolation (i.e. without starting services or using external resources such as network connectivity).  Done right, they can be very fast. You could run a lot of them in less than a minute.  These are  technology-facing  tests that will help us catch the most bugs and guide us through code restructuring thanks to their fast feedback and reliability.  Unit tests are also easier to be implemented than other tests.", 
            "title": "Unit Tests"
        }, 
        {
            "location": "/building-microservices/testing/#service-tests", 
            "text": "They are designed to bypass the user interface and test services directly.  In monolithic systems, a group of classes that provide a certain service to users can be tested together.  In microservices, we need to isolate the service we want to test so that we are able to quickly find the root cause of a bug. To achieve this isolation, we need to stub out other services interacting with the one under test.\nBut note that while a  stubbed service  does not care if it's called 1 or 100 times, a  mocked service  can provide you with that information so you could write more solid tests. The downside is that mocked services can make your tests brittle because of the magnitude of details tested.  So, after our service tests pass, we are confident that the new microservice is ready to contact other microservices with no errors. But what about other microservices calling the one we want to deploy?", 
            "title": "Service Tests"
        }, 
        {
            "location": "/building-microservices/testing/#end-to-end-tests", 
            "text": "They are run against the whole system, so they cover a lot of code and give you a lot of confidence that the system will work in production.\nOn the other hand, it's harder to diagnose an issue that comes up in e2e tests.  These tests are tricky to deal with, suppose we add them at the end of our deploy pipeline:   Then we have 2 issues:   Which services version are we going to use in our tests?  Executing such a pipeline for each microservice is going to be really inefficient.   Both of them are solved with a  fan in  model:   But there are other disadvantages when using e2e tests:   As the scope increases, we might face more errors due to  causes unrelated to the behavior we want to test  (e.g. network failures).  When tests  sometimes  fail because of unrelated issues (these are called  flaky tests ), people will tend to re-run them without any effort to understand the errors. This will cause lots of scheduled builds and lead to a broken system because some issues (e.g. concurrency issues) may slip through this process as unrelated issues. Flaky tests will also cause a  normalization of deviance  in the test system, so it's mandatory to remove them (or temporarily disable them to apply a fix) as soon as they're spotted.", 
            "title": "End-to-End Tests"
        }, 
        {
            "location": "/building-microservices/testing/#ownership", 
            "text": "Usually the tests of a service are owned by the team developing the service. But in e2e tests there can be multiple services involved in a single test.  Avoid:   A  free for all  approach, because no team would have a real ownership of any test, causing failing tests to be ignored or easily dismissed as responsibility of another team.  A  test team  whose job is solely to write e2e tests. A team like this would not know how to fix issues caught in tests and would cause the development team to become distant from the tests of their code.   Instead, aim to share responsibilities between the teams involved in each single test: everyone should be able to add a new test and it must be clear who is responsible for the success of that test.", 
            "title": "Ownership"
        }, 
        {
            "location": "/building-microservices/testing/#speed", 
            "text": "When e2e tests are too slow:   They are more prone to unrelated issues happening during their execution, this makes them brittle tests.  The feedback cycle is slowed, so it takes more time to fix a failing build. This will cause builds to pile up: we will no longer able to quickly deploy small features.   Things can be sped up by running tests in parallel, but it can only help to a certain extent.  To speed up your e2e tests, you should aim to remove useless tests and weigh risk/rewards of implemented tests to determine if they're worth having around. But this is a really difficult task since humans are not that good at estimating risks.", 
            "title": "Speed"
        }, 
        {
            "location": "/building-microservices/testing/#the-metaversion", 
            "text": "After running e2e tests one can start thinking,  So, I know all these services at these versions work together, so why not deploy them all together? .\nThis reasoning is to avoid because it usually leads to a proposal of a unified version number for the system, which in turn leads to coupling deploys. In the end, it will lead to a microservices system without the benefits of microservices.", 
            "title": "The Metaversion"
        }, 
        {
            "location": "/building-microservices/testing/#test-journeys-not-stories", 
            "text": "Despite the disadvantages, e2e test can be doable with one or two services. So what if we have 20 services?  Avoid:   Adding an e2e for each story to implement because it leads to slow feedback times and huge overlaps in test coverage.   Instead, you should test on a few core journeys. Any functionality not covered in these journeys needs to be covered in tests that analyze services in isolation from each other.", 
            "title": "Test Journeys, Not Stories"
        }, 
        {
            "location": "/building-microservices/testing/#consumer-driven-tests", 
            "text": "We use e2e to be sure that when a new service is deployed the system will keep working. Another way to approach this problem is by defining  consumer-driven contracts (CDC) , which express the expectations of the consumers of a service.  These tests can be run in isolation and are at the  service  test scope, although they serve a different purpose from classic service tests: we are able to identify breaking changes and decide whether to introduce breaking changes in the involved consumer service too or think through it before deploying the new service.   The major benefit is that these tests are not as expensive as e2e tests, while they give us confidence that the new service does not break the expectations of its consumers.  Consumer-driven tests can be implemented by the teams developing consumer services but this requires a good communication channel between the teams. If for some reason it's hard to communicate, they can be written by the team developing the new service.", 
            "title": "Consumer-Driven Tests"
        }, 
        {
            "location": "/building-microservices/testing/#testing-after-production", 
            "text": "Most testing is done before the system is in production. Still, when the system is in production:   Some bugs may have slipped through our tests.  New failure modes are discovered.  Our users use the system in ways we could never expect.   One reaction to this is often to define more and more tests. However, at a certain point we have we will hit diminishing returns with this approach.", 
            "title": "Testing after production"
        }, 
        {
            "location": "/building-microservices/testing/#separating-deployment-from-release", 
            "text": "If we deploy the system in an environment prior to directing production loads against it, we are able to detect issues specific to the environment.  In this environment we can also run  smoke tests  to make sure that the deploy was successful and there are no environmental issues.  There are different practices that you can adopt to follow this approach.", 
            "title": "Separating Deployment from Release"
        }, 
        {
            "location": "/building-microservices/testing/#bluegreen-deployment", 
            "text": "With blue/green deployment we have two copies of our software deployed at a time, but only one version of it is receiving real requests.  This diagram describes how blue/green deployment works:   Benefits:   Smoke tests can be run after the deploy.  If we keep the old version running, we can quickly fallback to it if we detect errors in the new version. The  detect and revert  process can be automated too.  Zero downtime between deploys can be achieved.   Downsides:   Requires some networking engineering but usually cloud providers support the needed functionalities.", 
            "title": "Blue/Green deployment"
        }, 
        {
            "location": "/building-microservices/testing/#canary-releasing", 
            "text": "With canary releasing, we verify our newly deployed service by directing amounts of production traffic against it to see if it performs as expected.  Performance can be measured the way you prefer, some examples are:   Measuring response times.  Measuring error rates.  Measuring sales conversions of a new recommendation algorithm.   When considering canary releasing, you need to decide if you are going to divert a portion of production requests to the canary or just copy production load.  Benefits:   Has all the benefits provided by blue/green deployment.  Lets you evaluate the performance of the new service according to custom/complex metrics.   Downsides:   More difficult to setup than blue/green deployment.  Needs advanced network routing capabilities.  May need more computing power because of multiple services that have to run together for long times.", 
            "title": "Canary Releasing"
        }, 
        {
            "location": "/building-microservices/testing/#mtbf-vs-mttr", 
            "text": "By using techniques such as blue/green and canary deployment we acknowledge that some issues can only be discovered in production: sometimes expending the same effort into getting better at remediation of a release can be significantly more beneficial than adding more automated tests.\nThis is referred to as the trade-off between optimizing for  mean time between failures  ( MTBF ) and  mean time to repair  ( MTTR ).  For different organizations, this trade-off between MTBF and MTTR will vary, and much of this lies with understanding the true impact of failure in a production environment.", 
            "title": "MTBF vs MTTR"
        }, 
        {
            "location": "/building-microservices/testing/#cross-functional-testing", 
            "text": "Nonfunctional requirements  describe those characteristics your system exhibits that cannot simply be implemented like a normal feature. We will use the term  cross-functional requirements   (CFR)  instead to refer to these tests.  As example, these are popular CFRs:   Latency of a web server.  Maximum concurrent users.  How secure the stored data should be.   Tests around CFRs should follow the pyramid too: some tests will have to be end-to-end (e.g. load tests) but others won\u2019t (e.g. tests to catch performance bottlenecks).  It's really important to consider CFRs from the start of the development process, because they shape too the design of your system.", 
            "title": "Cross-Functional Testing"
        }, 
        {
            "location": "/building-microservices/testing/#performance-tests", 
            "text": "Performance tests are a way of ensuring that some of our CFRs can be met.  In microservices systems performance is critical since what would be a single database call in a monolithic system could now become 3-4 calls to different services.  As with functional tests, you may want different performance tests for each test scope.  Due to the time it takes to run performance tests, it isn\u2019t always feasible to run them on every check-in. It is a common practice to run a subset every day, and a larger set every week.  Also, it's important to have targets so that the results of these tests can be correctly interpreted and evaluated (they may mark a build as failed is the performance level is below a certain threshold).", 
            "title": "Performance Tests"
        }, 
        {
            "location": "/building-microservices/testing/#summary", 
            "text": "Fundamental notions:   Optimize for fast feedback, and separate types of tests accordingly.  Avoid the need for end-to-end tests wherever possible by using consumer-driven contracts.  Use consumer-driven contracts to provide focus points for conversations between teams.  Try to understand the trade-off between putting more efforts into testing and detecting issues faster in production (optimizing for MTBF versus MTTR).", 
            "title": "Summary"
        }, 
        {
            "location": "/building-microservices/monitoring/", 
            "text": "Monitoring\n\n\nBreaking our system up in microservices adds complexity in the monitoring process. The answer is to add monitoring at a single service level and then aggregate the data, because eventually there are gonna be too many services for manual monitoring.\n\n\nIn microservices, monitoring can help you efficiently scale your system too.\n\n\nSingle Service, Single Server\n\n\nLet's first consider the simplest setup: one host, running one service. What should we monitor?\n\n\n\n\nThe \nhost\n (e.g. CPU and memory usage).\n\n\nThe \nlogs of the server\n, so when a user reports an error we can pinpoint it to a log record.\n\n\nThe \napplication\n (e.g. response times).\n\n\n\n\nSingle Service, Multiple Servers\n\n\nNow there multiple copies of the service running on separate hosts, behind a load balancer.\n\n\n\n\nWhat should we monitor?\n\n\n\n\nThe \nhosts\n, both individually and by aggregating data: it would be useful to determine if memory usage is due to a software bug or to a rogue OS process.\n\n\nThe \nlogs\n can still be saved on each host. We would be able to easily navigate them via tools like ssh-multiplexers.\n\n\nThe load balancer can help with aggregating data for tasks like \nresponse time tracking\n. Ideally the load balancer should be able to tell if a microservice is healthy and remove it if that's not the case.\n\n\n\n\nMultiple Services, Multiple Servers\n\n\nMultiple services are providing capabilities to our users, and those services are running on multiple hosts \u2014 be they physical or virtual.\n\n\n\n\nIn this case we would need specific subsystems to aggregate (e.g. \nLogstash\n, \nGraphite\n) and visualize (e.g. \nKibana\n) data.\n\n\nService Metrics\n\n\nIdeally your service should expose basic metrics too (e.g. invoices emitted per day).\n\n\nThe benefits are:\n\n\n\n\nYou can detect which features of a service are actually used in production.\n\n\nYou can react to how your users are using your system in order to improve it.\n\n\nIt's hard to know what data will be useful when you first deploy your system.\n\n\n\n\nSynthetic Monitoring\n\n\nIt's about monitoring systems acting like users and reporting back issues if they occur.\n\n\nFor example, out monitoring system could publish a message in a queue from time to time and track how the system behaves when handling that message. If any issues are found, they are reported and we would be able to debug them because we have low-level monitoring in place too.\nSending this fake message is an example of a \nsynthetic transaction\n used to ensure the system was behaving semantically, which is why this technique is often called \nsemantic monitoring\n.\n\n\nBut how to implement semantic monitoring?\nWe can reuse the code in our tests and run a subset of it against the production system. You must carefully check the data requirements and ensure that no side effects are going to take place. A common solution is to enable a set of fake users that will perform synthetic transaction in production.\n\n\nCorrelation IDs\n\n\nWith many services, a single initiating call can end up generating multiple more downstream service calls.\n\n\nHow to track these calls?\n\n\nA possible solution is to use correlation IDs: when the first call is made, you generate a GUID for the call. This is then passed along to all subsequent calls.\n\n\nIt would be better to adopt correlation ids from the start, because it's usually very hard to retro fit call logs with correlation ids.\n\n\nAlso, note that using correlation IDs justifies the development of a client library shared between microservices, because it would simplify communication a lot. Just remember to keep these shared libraries as thin as possible to avoid any kind of coupling.\n\n\nCascading Failures\n\n\nCascading failures can be especially perilous. Imagine a situation where a service cannot communicate with a downstream service while they both look healthy.\n\n\nUsing synthetic monitoring would pick up the problem. But we\u2019d also need to report on the fact that one service cannot see another in order to determine the cause of the problem. Therefore, monitoring the integration points between systems is fundamental. Each service instance should track and expose the health of its downstream dependencies, from the database to other collaborating services.\n\n\nStandardization\n\n\nOne of the ongoing balancing acts you\u2019ll need to pull off is where to allow for decisions to be made narrowly for a single service versus where you need to standardize across your system. Monitoring is one area where standardization is incredibly important, because it allows for easy data aggregation and monitoring.\n\n\nConsider the audience\n\n\nIt's fundamental to display data according to who is going to consume it (e.g. sysops can consume bulks of system-level logs while a manager is going to need a dashboard to visualize business-related metrics).\n\n\nSummary\n\n\nFor each service:\n\n\n\n\nTrack inbound response time at a bare minimum. Once you\u2019ve done that, follow with error rates and then start working on application-level metrics.\n\n\nTrack the health of all downstream responses, at a bare minimum including the response time of downstream calls, and at best tracking error rates.\n\n\nStandardize on how and where metrics are collected.\n\n\nLog into a standard location, in a standard format if possible. Aggregation is a pain if every service uses a different layout!\n\n\nMonitor the underlying operating system so you can track down rogue processes and do capacity planning.\n\n\n\n\nFor the system:\n\n\n\n\nAggregate host-level metrics like CPU together with application-level metrics.\n\n\nEnsure your metric storage tool allows for aggregation at a system or service level, and drill down to individual hosts.\n\n\nEnsure your metric storage tool allows you to maintain data long enough to understand trends in your system.\n\n\nHave a single, queryable tool for aggregating and storing logs.\n\n\nStrongly consider standardizing on the use of correlation IDs.\n\n\nUnderstand what requires a call to action, and structure alerting and dashboards accordingly.\n\n\nInvestigate the possibility of unifying how you aggregate all of your various metrics.", 
            "title": "Monitoring"
        }, 
        {
            "location": "/building-microservices/monitoring/#monitoring", 
            "text": "Breaking our system up in microservices adds complexity in the monitoring process. The answer is to add monitoring at a single service level and then aggregate the data, because eventually there are gonna be too many services for manual monitoring.  In microservices, monitoring can help you efficiently scale your system too.", 
            "title": "Monitoring"
        }, 
        {
            "location": "/building-microservices/monitoring/#single-service-single-server", 
            "text": "Let's first consider the simplest setup: one host, running one service. What should we monitor?   The  host  (e.g. CPU and memory usage).  The  logs of the server , so when a user reports an error we can pinpoint it to a log record.  The  application  (e.g. response times).", 
            "title": "Single Service, Single Server"
        }, 
        {
            "location": "/building-microservices/monitoring/#single-service-multiple-servers", 
            "text": "Now there multiple copies of the service running on separate hosts, behind a load balancer.   What should we monitor?   The  hosts , both individually and by aggregating data: it would be useful to determine if memory usage is due to a software bug or to a rogue OS process.  The  logs  can still be saved on each host. We would be able to easily navigate them via tools like ssh-multiplexers.  The load balancer can help with aggregating data for tasks like  response time tracking . Ideally the load balancer should be able to tell if a microservice is healthy and remove it if that's not the case.", 
            "title": "Single Service, Multiple Servers"
        }, 
        {
            "location": "/building-microservices/monitoring/#multiple-services-multiple-servers", 
            "text": "Multiple services are providing capabilities to our users, and those services are running on multiple hosts \u2014 be they physical or virtual.   In this case we would need specific subsystems to aggregate (e.g.  Logstash ,  Graphite ) and visualize (e.g.  Kibana ) data.", 
            "title": "Multiple Services, Multiple Servers"
        }, 
        {
            "location": "/building-microservices/monitoring/#service-metrics", 
            "text": "Ideally your service should expose basic metrics too (e.g. invoices emitted per day).  The benefits are:   You can detect which features of a service are actually used in production.  You can react to how your users are using your system in order to improve it.  It's hard to know what data will be useful when you first deploy your system.", 
            "title": "Service Metrics"
        }, 
        {
            "location": "/building-microservices/monitoring/#synthetic-monitoring", 
            "text": "It's about monitoring systems acting like users and reporting back issues if they occur.  For example, out monitoring system could publish a message in a queue from time to time and track how the system behaves when handling that message. If any issues are found, they are reported and we would be able to debug them because we have low-level monitoring in place too.\nSending this fake message is an example of a  synthetic transaction  used to ensure the system was behaving semantically, which is why this technique is often called  semantic monitoring .  But how to implement semantic monitoring?\nWe can reuse the code in our tests and run a subset of it against the production system. You must carefully check the data requirements and ensure that no side effects are going to take place. A common solution is to enable a set of fake users that will perform synthetic transaction in production.", 
            "title": "Synthetic Monitoring"
        }, 
        {
            "location": "/building-microservices/monitoring/#correlation-ids", 
            "text": "With many services, a single initiating call can end up generating multiple more downstream service calls.  How to track these calls?  A possible solution is to use correlation IDs: when the first call is made, you generate a GUID for the call. This is then passed along to all subsequent calls.  It would be better to adopt correlation ids from the start, because it's usually very hard to retro fit call logs with correlation ids.  Also, note that using correlation IDs justifies the development of a client library shared between microservices, because it would simplify communication a lot. Just remember to keep these shared libraries as thin as possible to avoid any kind of coupling.", 
            "title": "Correlation IDs"
        }, 
        {
            "location": "/building-microservices/monitoring/#cascading-failures", 
            "text": "Cascading failures can be especially perilous. Imagine a situation where a service cannot communicate with a downstream service while they both look healthy.  Using synthetic monitoring would pick up the problem. But we\u2019d also need to report on the fact that one service cannot see another in order to determine the cause of the problem. Therefore, monitoring the integration points between systems is fundamental. Each service instance should track and expose the health of its downstream dependencies, from the database to other collaborating services.", 
            "title": "Cascading Failures"
        }, 
        {
            "location": "/building-microservices/monitoring/#standardization", 
            "text": "One of the ongoing balancing acts you\u2019ll need to pull off is where to allow for decisions to be made narrowly for a single service versus where you need to standardize across your system. Monitoring is one area where standardization is incredibly important, because it allows for easy data aggregation and monitoring.", 
            "title": "Standardization"
        }, 
        {
            "location": "/building-microservices/monitoring/#consider-the-audience", 
            "text": "It's fundamental to display data according to who is going to consume it (e.g. sysops can consume bulks of system-level logs while a manager is going to need a dashboard to visualize business-related metrics).", 
            "title": "Consider the audience"
        }, 
        {
            "location": "/building-microservices/monitoring/#summary", 
            "text": "For each service:   Track inbound response time at a bare minimum. Once you\u2019ve done that, follow with error rates and then start working on application-level metrics.  Track the health of all downstream responses, at a bare minimum including the response time of downstream calls, and at best tracking error rates.  Standardize on how and where metrics are collected.  Log into a standard location, in a standard format if possible. Aggregation is a pain if every service uses a different layout!  Monitor the underlying operating system so you can track down rogue processes and do capacity planning.   For the system:   Aggregate host-level metrics like CPU together with application-level metrics.  Ensure your metric storage tool allows for aggregation at a system or service level, and drill down to individual hosts.  Ensure your metric storage tool allows you to maintain data long enough to understand trends in your system.  Have a single, queryable tool for aggregating and storing logs.  Strongly consider standardizing on the use of correlation IDs.  Understand what requires a call to action, and structure alerting and dashboards accordingly.  Investigate the possibility of unifying how you aggregate all of your various metrics.", 
            "title": "Summary"
        }, 
        {
            "location": "/building-microservices/security/", 
            "text": "Security\n\n\nWe need to be aware of the importance of customers' data in our systems. How can we work out what is \nenough\n security to protect that data?\n\n\nAuthentication and Authorization\n\n\nAuthentication\n is the mechanism by which an actor proves that he is who he says he is. The actor who is being authenticated is called \nprincipal\n.\n\n\nAuthorization\n maps a principal to the action he's allowed to do.\n\n\nGenerally, in monolithic applications, web framework provide all the authentication and authorization functionalities for you. While in distributed systems we aim to authenticate a principal a single time for all microservices.\n\n\nSingle Sign-On\n\n\nSSO is a common approach to authentication and authorization.\n\n\nHow it works:\n\n\nWhen a \nprincipal\n tries to access a \nresource\n, she is directed to authenticate with an \nidentity provider\n that may ask her to provide a \nsecret\n. If the identity provider is satisfied of the secret, it gives information to the service provider, allowing it to decide whether to grant her access to the resource.\n\n\nThe identity provider can be an external (e.g. Google's OpenID Connect service) or internal (common for enterprise platforms).\n\n\nCommon SSO implementations:\n\n\n\n\nSAML\n (SOAP-based standard)\n\n\nOpenID Connect\n (authentication layer on top of OAuth 2.0)\n\n\n\n\nSingle Sign-On Gateway\n\n\nRather than having each microservice communicate with the identity provider, you could add a SSO Gateway to your system:\n\n\n\n\nInformation about principals can be passed to downstream microservices with HTTP headers.\n\n\nBenefits:\n\n\n\n\nDrastically reduces network usage in your system.\n\n\nCan be used to terminate HTTPS at this level, run intrusion detection and so on.\n\n\n\n\nDownsides:\n\n\n\n\nYou need to make sure that developers are able to launch microservices behind a SSO gateway to test the system without too much effort.\n\n\nCould give a false sense of security.\n\n\n\n\nA gateway may be able to provide fairly effective coarse-grained authorization using \nroles\n. Always prefer coarse over fine-grained authorization. The latter make your system hard to manage and reason about because business rules end up in the identity provider. Instead, these rules should be owned by your microservices.\n\n\nService-to-Service Authentication and Authorization\n\n\nWhen talking about programs authenticating with each other, we have several possibilities.\n\n\nAllow Everything Inside the Perimeter\n\n\nAuthentication is placed only at the perimeter of the system. Depending on the sensitivity of the data, this approach could be fine.\n\n\nBenefits:\n\n\n\n\nVery easy to implement.\n\n\n\n\nDownsides:\n\n\n\n\nShould an attacker penetrate the \nperimeter\n, you'll end up with major troubles. This can be mitigated by using HTTPS but still the stakes are high.\n\n\n\n\nHTTP(S) Basic Authentication\n\n\nHTTP Basic Authentication allows for a client to send username and password in a standard HTTP header. This should normally be used with HTTPS to securely send credentials.\n\n\nBenefits:\n\n\n\n\nWell understood and supported standard.\n\n\n\n\nDownsides:\n\n\n\n\nYou need to manage SSL certificates for your microservices.\n\n\nThe overhead of HTTPS traffic can place additional strain on servers.\n\n\nSSL encrypted traffic can usually be cached only at service level.\n\n\nYou need to manage user and passwords, either by syncing with an existing identity provider or independently (could cause functionality duplication).\n\n\n\n\nReuse a SSO implementation\n\n\nIf you already have a SSO gateway implemented, you could reuse it for service-to-service authentication. Microservices should have their own \nservice accounts\n to authenticate with each other.\n\n\nBenefits:\n\n\n\n\nReuses existing infrastructure.\n\n\nWith each microservice having its own credentials, it's easy to revoke/restore them if they get compromised.\n\n\n\n\nDownsides:\n\n\n\n\nYou need to write code that supports the SSO implementation of your choice.\n\n\nYou need to securely store the service accounts' credentials.\n\n\n\n\nClient certificates\n\n\nClients can install their own TLS certificates to authenticate to servers. This could be a mandatory choice if your service-to-service traffic goes through networks you don't control (e.g. Internet).\n\n\nBenefits:\n\n\n\n\nServers have strong guarantee that they're communicating with the right client.\n\n\n\n\nDownsides:\n\n\n\n\nEven more difficult than managing only server certificates.\n\n\n\n\nHMAC over HTTP\n\n\nAn alternative approach is to use a \nhash-based messaging code (HMAC)\n to sign the request with a hash computed from the request's body and a private key.\n\n\nBenefits:\n\n\n\n\nPrevents MITM attacks.\n\n\nTraffic can be cached more easily than HTTPS traffic.\n\n\nUsually the overhead of generating hashes is lower than the one caused by HTTPS.\n\n\n\n\nDownsides:\n\n\n\n\nYou need a way to share the secret key.\n\n\nHard to revoke the private key in case it's compromised.\n\n\nIt's a pattern, not a standard, so there are divergent ways to implement it.\n\n\nTraffic is not encrypted.\n\n\n\n\nAPI Keys\n\n\nAPI keys allow a service to identify who is making a call, and place limits on what they can do.\n\n\nBenefits:\n\n\n\n\nEasy to use for service-to-service authentication.\n\n\n\n\nDownsides:\n\n\n\n\nYou need an API keys manager. Still, there are a lot of available tools.\n\n\n\n\nThe deputy problem\n\n\nHaving a principal authenticate with a given microservice is simple. But what happens if that service then needs to call more services to complete an operation?\n\n\n\n\nWithout countermeasures, users can potentially trick the system into making calls to microservices that retrieve information they're not allowed to access (e.g. other users' data).\n\n\nThere is no simple answer to this problem. Depending on the sensitivity of the operation, you have different choices:\n\n\n\n\nUse an implicit trust model inside the microservices perimeter.\n\n\nVerify the identity of the caller before accepting a request (this could add duplicate logic in your microservices).\n\n\nAsk the caller to provide the credentials of the original principal.\n\n\n\n\nSecuring Data at Rest\n\n\nIt's important to carefully store data at rest to limit the damage if an attacker obtains access to it.c Some principles to observe:\n\n\n\n\nUse existing implementations of cryptographic algorithms.\n\n\nBe careful how you manage your keys.\n\n\nIdentify sensitive data that should be encrypted and data that can be shown in logs.\n\n\nEncrypt data when you first see it. Only decrypt it on demand.\n\n\nEncrypt backups.\n\n\n\n\nOther protections\n\n\nThere are other protective measures that you can apply to your system:\n\n\n\n\nFirewall.\n\n\nLogging (but be careful of what you log!).\n\n\nIntrusion detection systems.\n\n\nNetwork segregation (e.g. put separate microservices in separate networks).\n\n\n\n\nBest practices:\n\n\n\n\nKeep your software updated and look out for new vulnerabilities.\n\n\nGive the service user on the server OS as few permissions as possible.\n\n\nBe frugal: collect only the data you need.\n\n\n\n\nThe Human Element\n\n\nYou may also need policies to deal with the human element in your organization:\n\n\n\n\nHow do you revoke access to credentials when someone leaves the organization?\n\n\nHow can you protect yourself against social engineering?\n\n\n\n\nBaking Security In\n\n\nHelping educate developers about security concerns is key. Also, you can integrate automated vulnerabilities probing tools in your CI build.\n\n\nBut when in doubt, reach out to an expert for a thorough check of your system.\n\n\nSummary\n\n\nHaving a system decomposed into finer-grained services gives us many more options as to how to solve a problem. This concept applies to security too. So, while we have additional problems to solve (e.g. the deputy problem), we can apply a mix of known solutions to solve these problems.", 
            "title": "Security"
        }, 
        {
            "location": "/building-microservices/security/#security", 
            "text": "We need to be aware of the importance of customers' data in our systems. How can we work out what is  enough  security to protect that data?", 
            "title": "Security"
        }, 
        {
            "location": "/building-microservices/security/#authentication-and-authorization", 
            "text": "Authentication  is the mechanism by which an actor proves that he is who he says he is. The actor who is being authenticated is called  principal .  Authorization  maps a principal to the action he's allowed to do.  Generally, in monolithic applications, web framework provide all the authentication and authorization functionalities for you. While in distributed systems we aim to authenticate a principal a single time for all microservices.", 
            "title": "Authentication and Authorization"
        }, 
        {
            "location": "/building-microservices/security/#single-sign-on", 
            "text": "SSO is a common approach to authentication and authorization.  How it works:  When a  principal  tries to access a  resource , she is directed to authenticate with an  identity provider  that may ask her to provide a  secret . If the identity provider is satisfied of the secret, it gives information to the service provider, allowing it to decide whether to grant her access to the resource.  The identity provider can be an external (e.g. Google's OpenID Connect service) or internal (common for enterprise platforms).  Common SSO implementations:   SAML  (SOAP-based standard)  OpenID Connect  (authentication layer on top of OAuth 2.0)", 
            "title": "Single Sign-On"
        }, 
        {
            "location": "/building-microservices/security/#single-sign-on-gateway", 
            "text": "Rather than having each microservice communicate with the identity provider, you could add a SSO Gateway to your system:   Information about principals can be passed to downstream microservices with HTTP headers.  Benefits:   Drastically reduces network usage in your system.  Can be used to terminate HTTPS at this level, run intrusion detection and so on.   Downsides:   You need to make sure that developers are able to launch microservices behind a SSO gateway to test the system without too much effort.  Could give a false sense of security.   A gateway may be able to provide fairly effective coarse-grained authorization using  roles . Always prefer coarse over fine-grained authorization. The latter make your system hard to manage and reason about because business rules end up in the identity provider. Instead, these rules should be owned by your microservices.", 
            "title": "Single Sign-On Gateway"
        }, 
        {
            "location": "/building-microservices/security/#service-to-service-authentication-and-authorization", 
            "text": "When talking about programs authenticating with each other, we have several possibilities.", 
            "title": "Service-to-Service Authentication and Authorization"
        }, 
        {
            "location": "/building-microservices/security/#allow-everything-inside-the-perimeter", 
            "text": "Authentication is placed only at the perimeter of the system. Depending on the sensitivity of the data, this approach could be fine.  Benefits:   Very easy to implement.   Downsides:   Should an attacker penetrate the  perimeter , you'll end up with major troubles. This can be mitigated by using HTTPS but still the stakes are high.", 
            "title": "Allow Everything Inside the Perimeter"
        }, 
        {
            "location": "/building-microservices/security/#https-basic-authentication", 
            "text": "HTTP Basic Authentication allows for a client to send username and password in a standard HTTP header. This should normally be used with HTTPS to securely send credentials.  Benefits:   Well understood and supported standard.   Downsides:   You need to manage SSL certificates for your microservices.  The overhead of HTTPS traffic can place additional strain on servers.  SSL encrypted traffic can usually be cached only at service level.  You need to manage user and passwords, either by syncing with an existing identity provider or independently (could cause functionality duplication).", 
            "title": "HTTP(S) Basic Authentication"
        }, 
        {
            "location": "/building-microservices/security/#reuse-a-sso-implementation", 
            "text": "If you already have a SSO gateway implemented, you could reuse it for service-to-service authentication. Microservices should have their own  service accounts  to authenticate with each other.  Benefits:   Reuses existing infrastructure.  With each microservice having its own credentials, it's easy to revoke/restore them if they get compromised.   Downsides:   You need to write code that supports the SSO implementation of your choice.  You need to securely store the service accounts' credentials.", 
            "title": "Reuse a SSO implementation"
        }, 
        {
            "location": "/building-microservices/security/#client-certificates", 
            "text": "Clients can install their own TLS certificates to authenticate to servers. This could be a mandatory choice if your service-to-service traffic goes through networks you don't control (e.g. Internet).  Benefits:   Servers have strong guarantee that they're communicating with the right client.   Downsides:   Even more difficult than managing only server certificates.", 
            "title": "Client certificates"
        }, 
        {
            "location": "/building-microservices/security/#hmac-over-http", 
            "text": "An alternative approach is to use a  hash-based messaging code (HMAC)  to sign the request with a hash computed from the request's body and a private key.  Benefits:   Prevents MITM attacks.  Traffic can be cached more easily than HTTPS traffic.  Usually the overhead of generating hashes is lower than the one caused by HTTPS.   Downsides:   You need a way to share the secret key.  Hard to revoke the private key in case it's compromised.  It's a pattern, not a standard, so there are divergent ways to implement it.  Traffic is not encrypted.", 
            "title": "HMAC over HTTP"
        }, 
        {
            "location": "/building-microservices/security/#api-keys", 
            "text": "API keys allow a service to identify who is making a call, and place limits on what they can do.  Benefits:   Easy to use for service-to-service authentication.   Downsides:   You need an API keys manager. Still, there are a lot of available tools.", 
            "title": "API Keys"
        }, 
        {
            "location": "/building-microservices/security/#the-deputy-problem", 
            "text": "Having a principal authenticate with a given microservice is simple. But what happens if that service then needs to call more services to complete an operation?   Without countermeasures, users can potentially trick the system into making calls to microservices that retrieve information they're not allowed to access (e.g. other users' data).  There is no simple answer to this problem. Depending on the sensitivity of the operation, you have different choices:   Use an implicit trust model inside the microservices perimeter.  Verify the identity of the caller before accepting a request (this could add duplicate logic in your microservices).  Ask the caller to provide the credentials of the original principal.", 
            "title": "The deputy problem"
        }, 
        {
            "location": "/building-microservices/security/#securing-data-at-rest", 
            "text": "It's important to carefully store data at rest to limit the damage if an attacker obtains access to it.c Some principles to observe:   Use existing implementations of cryptographic algorithms.  Be careful how you manage your keys.  Identify sensitive data that should be encrypted and data that can be shown in logs.  Encrypt data when you first see it. Only decrypt it on demand.  Encrypt backups.", 
            "title": "Securing Data at Rest"
        }, 
        {
            "location": "/building-microservices/security/#other-protections", 
            "text": "There are other protective measures that you can apply to your system:   Firewall.  Logging (but be careful of what you log!).  Intrusion detection systems.  Network segregation (e.g. put separate microservices in separate networks).   Best practices:   Keep your software updated and look out for new vulnerabilities.  Give the service user on the server OS as few permissions as possible.  Be frugal: collect only the data you need.", 
            "title": "Other protections"
        }, 
        {
            "location": "/building-microservices/security/#the-human-element", 
            "text": "You may also need policies to deal with the human element in your organization:   How do you revoke access to credentials when someone leaves the organization?  How can you protect yourself against social engineering?", 
            "title": "The Human Element"
        }, 
        {
            "location": "/building-microservices/security/#baking-security-in", 
            "text": "Helping educate developers about security concerns is key. Also, you can integrate automated vulnerabilities probing tools in your CI build.  But when in doubt, reach out to an expert for a thorough check of your system.", 
            "title": "Baking Security In"
        }, 
        {
            "location": "/building-microservices/security/#summary", 
            "text": "Having a system decomposed into finer-grained services gives us many more options as to how to solve a problem. This concept applies to security too. So, while we have additional problems to solve (e.g. the deputy problem), we can apply a mix of known solutions to solve these problems.", 
            "title": "Summary"
        }, 
        {
            "location": "/building-microservices/conway/", 
            "text": "Conway\u2019s Law and System Design\n\n\nMelvin Conway\u2019s paper \nHow Do Committees Invent\n (April 1968) observed that:\n\n\n\n\nAny organization that designs a system (defined more broadly here than just information systems) will inevitably produce a design whose structure is a copy of the organization\u2019s communication structure.\n\n\n\n\nThis idea can be summarized as \n\u201cIf you have four groups working on a compiler, you\u2019ll get a 4-pass compiler.\u201d\n\n\nEvidence\n\n\nVarious studies have found supporting evidence for this claim. You can read more on \nWikipedia\n.\n\n\nSome examples from the IT industry:\n\n\n\n\nAmazon conceived its \ntwo-pizza teams\n from this idea. This organizational structure mostly is what drove the creation of AWS.\n\n\nNetflix designed the organizational structure for the system architecture it wanted. Small teams allowed for independent services.\n\n\n\n\nApplications of Conway's Law\n\n\nLet's examine 3 cases:\n\n\n\n\nSingle team owns a single service\n (i.e. multiple teams own different services). Here takes place fine-grained communication, which suits well the nature of software communication inside the service's boundaries. Team communication is fast-paced just like function calls. Following Conway's Law, the outcome will be an efficient system which is isolated from external services because communications is harder between different teams.\n\n\nSingle team owns multiple services.\n Here takes place fine-grained communication, so the services might end up being coupled.\n\n\nMultiple teams own the same service.\n Here takes place coarse-grained communication, so the development process will be inefficient and the service's code unnecessarily abstract and/or complex.\n\n\n\n\nService Ownership\n\n\nHaving one team responsible for deploying and maintaining the application means it has an incentive to create services that are easy to deploy. There will be no one else to catch the code if the team wants to \nthrow it over the wall\n.\n\n\nSome factors that drive away from the ideal service ownership model:\n\n\n\n\nHigh cost of splitting a service\n may make multiple teams work on the same service. Try to gradually split the service.\n\n\nFeature teams\n own the same service but work on separate feature. This approach bases the organization on the technical model (i.e. UI, database, etc.). It's an approach to avoid because microservices are by nature to be aligned with the domain model, not the technical one. Another reason to avoid it is that, in the end, no team will end up having clear ownership of anything; this gives space to a lot of blaming.\n\n\nDelivery bottlenecks\n may make multiple teams work on the same service. This can be solved by temporarily adding a new member to the overloaded team or by splitting the service if the feature load is really high and it's expected to be kept up or increased in the future.\n\n\n\n\nIf, for some reason, it's unavoidable having shared services, we can adopt the \ninternal open source\n model. In this model, a service is owned by a core team of trusted committers that review changes requested by untrusted committers.\n\n\nSummary\n\n\nConway\u2019s law highlights the perils of trying to enforce a system design that doesn\u2019t match the organization.", 
            "title": "Conway's Law and System Desing"
        }, 
        {
            "location": "/building-microservices/conway/#conways-law-and-system-design", 
            "text": "Melvin Conway\u2019s paper  How Do Committees Invent  (April 1968) observed that:   Any organization that designs a system (defined more broadly here than just information systems) will inevitably produce a design whose structure is a copy of the organization\u2019s communication structure.   This idea can be summarized as  \u201cIf you have four groups working on a compiler, you\u2019ll get a 4-pass compiler.\u201d", 
            "title": "Conway\u2019s Law and System Design"
        }, 
        {
            "location": "/building-microservices/conway/#evidence", 
            "text": "Various studies have found supporting evidence for this claim. You can read more on  Wikipedia .  Some examples from the IT industry:   Amazon conceived its  two-pizza teams  from this idea. This organizational structure mostly is what drove the creation of AWS.  Netflix designed the organizational structure for the system architecture it wanted. Small teams allowed for independent services.", 
            "title": "Evidence"
        }, 
        {
            "location": "/building-microservices/conway/#applications-of-conways-law", 
            "text": "Let's examine 3 cases:   Single team owns a single service  (i.e. multiple teams own different services). Here takes place fine-grained communication, which suits well the nature of software communication inside the service's boundaries. Team communication is fast-paced just like function calls. Following Conway's Law, the outcome will be an efficient system which is isolated from external services because communications is harder between different teams.  Single team owns multiple services.  Here takes place fine-grained communication, so the services might end up being coupled.  Multiple teams own the same service.  Here takes place coarse-grained communication, so the development process will be inefficient and the service's code unnecessarily abstract and/or complex.", 
            "title": "Applications of Conway's Law"
        }, 
        {
            "location": "/building-microservices/conway/#service-ownership", 
            "text": "Having one team responsible for deploying and maintaining the application means it has an incentive to create services that are easy to deploy. There will be no one else to catch the code if the team wants to  throw it over the wall .  Some factors that drive away from the ideal service ownership model:   High cost of splitting a service  may make multiple teams work on the same service. Try to gradually split the service.  Feature teams  own the same service but work on separate feature. This approach bases the organization on the technical model (i.e. UI, database, etc.). It's an approach to avoid because microservices are by nature to be aligned with the domain model, not the technical one. Another reason to avoid it is that, in the end, no team will end up having clear ownership of anything; this gives space to a lot of blaming.  Delivery bottlenecks  may make multiple teams work on the same service. This can be solved by temporarily adding a new member to the overloaded team or by splitting the service if the feature load is really high and it's expected to be kept up or increased in the future.   If, for some reason, it's unavoidable having shared services, we can adopt the  internal open source  model. In this model, a service is owned by a core team of trusted committers that review changes requested by untrusted committers.", 
            "title": "Service Ownership"
        }, 
        {
            "location": "/building-microservices/conway/#summary", 
            "text": "Conway\u2019s law highlights the perils of trying to enforce a system design that doesn\u2019t match the organization.", 
            "title": "Summary"
        }
    ]
}