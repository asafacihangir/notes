{
    "docs": [
        {
            "location": "/", 
            "text": "Notes\n\n\nNotes about various topics, courses, books, etc.\n\n\nCourses:\n\n\n\n\nBitcoin and cryptocurrency technologies\n\n\n\n\nBooks:\n\n\n\n\nBuilding microservices", 
            "title": "Home"
        }, 
        {
            "location": "/#notes", 
            "text": "Notes about various topics, courses, books, etc.  Courses:   Bitcoin and cryptocurrency technologies   Books:   Building microservices", 
            "title": "Notes"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/", 
            "text": "Bitcoin and cryptocurrency technologies\n\n\nNotes on the 2018 edition of the Coursera MOOC \nBitcoin and Cryptocurrency Technologies\n by Princeton University.\n\n\nAbout this course:\n\n\n\n\nTo really understand what is special about Bitcoin, we need to understand how it works at a technical level. We\u2019ll address the important questions about Bitcoin, such as:\n\n\nHow does Bitcoin work? What makes Bitcoin different? How secure are your Bitcoins? How anonymous are Bitcoin users? What determines the price of Bitcoins? Can cryptocurrencies be regulated? What might the future hold?\n\n\nAfter this course, you\u2019ll know everything you need to be able to separate fact from fiction when reading claims about Bitcoin and other cryptocurrencies. You\u2019ll have the conceptual foundations you need to engineer secure software that interacts with the Bitcoin network. And you\u2019ll be able to integrate ideas from Bitcoin in your own projects.\nCourse Lecturers:\nArvind Narayanan, Princeton University", 
            "title": "Home"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/#bitcoin-and-cryptocurrency-technologies", 
            "text": "Notes on the 2018 edition of the Coursera MOOC  Bitcoin and Cryptocurrency Technologies  by Princeton University.  About this course:   To really understand what is special about Bitcoin, we need to understand how it works at a technical level. We\u2019ll address the important questions about Bitcoin, such as:  How does Bitcoin work? What makes Bitcoin different? How secure are your Bitcoins? How anonymous are Bitcoin users? What determines the price of Bitcoins? Can cryptocurrencies be regulated? What might the future hold?  After this course, you\u2019ll know everything you need to be able to separate fact from fiction when reading claims about Bitcoin and other cryptocurrencies. You\u2019ll have the conceptual foundations you need to engineer secure software that interacts with the Bitcoin network. And you\u2019ll be able to integrate ideas from Bitcoin in your own projects.\nCourse Lecturers:\nArvind Narayanan, Princeton University", 
            "title": "Bitcoin and cryptocurrency technologies"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/", 
            "text": "Hash function\n\n\nA function that maps a string to a fixed size output.\n\n\nH(x): x string -\n fixed size output\n\n\nProperties\n\n\nCollision-free\n\n\nIt's hard to find a collision.\nNote that no hash function has ever been formally proved to be collision-free.\n\n\nApplication\n\n\nMessage digest:\n if \nH(x) = H(y)\n then it's safe to assume that \nx = y\n. This means that hash functions can help verify the integrity of documents without scanning the whole document.\n\n\nHiding\n\n\nGiven \nH(x)\n, it's hard to find \nx\n.\n\n\nApplication\n\n\nCommitment problem:\n we want to commit to value and reveal it later to an audience. If we hash that value, thanks to this property, we know that it will be hard for attackers to correctly guess it before we chose to disclose it. While our audience also knows that we wouldn't be able to change the committed value because hash functions are collision-free: it would be infeasible to generate a collision with the value we picked.\n\n\nPuzzle-friendly\n\n\nGiven a puzzle \nid\n and a target set \nY\n, try to find a solution \nx\n such that \nH(id | x) is in Y\n.\nPuzzle-friendly means that, for the stated problem, no solving strategy is much better that trying random values of x.\n\n\nApplication\n\n\nCrypto puzzles:\n puzzles that can be used as proof of work. These puzzles are used in blockchain based coin technologies such as Bitcoin.\n\n\nHash function examples\n\n\nSHA-256\n\n\nA popular hash function.\nA high level description of SHA-256 is given by the following diagram:", 
            "title": "Cryptographic hash functions"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#hash-function", 
            "text": "A function that maps a string to a fixed size output.  H(x): x string -  fixed size output", 
            "title": "Hash function"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#properties", 
            "text": "", 
            "title": "Properties"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#collision-free", 
            "text": "It's hard to find a collision.\nNote that no hash function has ever been formally proved to be collision-free.", 
            "title": "Collision-free"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#application", 
            "text": "Message digest:  if  H(x) = H(y)  then it's safe to assume that  x = y . This means that hash functions can help verify the integrity of documents without scanning the whole document.", 
            "title": "Application"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#hiding", 
            "text": "Given  H(x) , it's hard to find  x .", 
            "title": "Hiding"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#application_1", 
            "text": "Commitment problem:  we want to commit to value and reveal it later to an audience. If we hash that value, thanks to this property, we know that it will be hard for attackers to correctly guess it before we chose to disclose it. While our audience also knows that we wouldn't be able to change the committed value because hash functions are collision-free: it would be infeasible to generate a collision with the value we picked.", 
            "title": "Application"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#puzzle-friendly", 
            "text": "Given a puzzle  id  and a target set  Y , try to find a solution  x  such that  H(id | x) is in Y .\nPuzzle-friendly means that, for the stated problem, no solving strategy is much better that trying random values of x.", 
            "title": "Puzzle-friendly"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#application_2", 
            "text": "Crypto puzzles:  puzzles that can be used as proof of work. These puzzles are used in blockchain based coin technologies such as Bitcoin.", 
            "title": "Application"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#hash-function-examples", 
            "text": "", 
            "title": "Hash function examples"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/crypto-hash-functions/#sha-256", 
            "text": "A popular hash function.\nA high level description of SHA-256 is given by the following diagram:", 
            "title": "SHA-256"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/digital-signature/", 
            "text": "Digital signature\n\n\nDigital signatures must verify 2 properties:\n\n\n\n\n\n\nOnly you can sign some data, while anyone can verify the fact that you signed it.\n\n\n\n\n\n\nThe signature must be specific to the data that it signs: if it isn't, anyone can just copy the signature you shared and apply it to different documents.\n\n\n\n\n\n\nImplementation\n\n\nDigital signature schemes use a \npublic key\n and a \nprivate key\n:\n\n\n\n\n\n\nThe private key is used to sign data\n\n\n\n\n\n\nThe public key is used to verify signed data\n\n\n\n\n\n\nDigital signature schemes must guarantee that signed data is always correctly verified.\n\n\nUnforgeable signature schemes\n\n\nWhen is a signature scheme called unforgeable? Let's consider this game:\nThere is an attacker who knows the public key and a challenger who knows the private key too.\n\n\n\n\nThe attacker can pick a document and get the challenger to sign it.\n\n\nThe challenger will sign that document and send the signed data to the attacker.\n\n\nThe game can go on as for as much as the attacker wants (at least until a plausible amount of documents is signed)\n\n\n\n\nThen the attacker tries to sign a message that the challenger has not already signed: if the forged message verifies correctly then the attacker wins, else the challenger wins.\n\n\nSo a signature scheme is \nunforgeable\n if, not matter what algorithm the attacker is using, he has only a slim chance to succeed.\n\n\nUse cases\n\n\n\n\nPublic keys can be used as identities\n\n\nSignature schemes can be used to sign the last hash pointer in a blockchain, thus signing the whole blockchain.\n\n\n\n\nSignature scheme used in Bitcoin\n\n\nBitcoin uses ECDSA. Note that a good randomness source is essential to avoid leaking your private key using your public key.", 
            "title": "Digital signature"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/digital-signature/#digital-signature", 
            "text": "Digital signatures must verify 2 properties:    Only you can sign some data, while anyone can verify the fact that you signed it.    The signature must be specific to the data that it signs: if it isn't, anyone can just copy the signature you shared and apply it to different documents.", 
            "title": "Digital signature"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/digital-signature/#implementation", 
            "text": "Digital signature schemes use a  public key  and a  private key :    The private key is used to sign data    The public key is used to verify signed data    Digital signature schemes must guarantee that signed data is always correctly verified.", 
            "title": "Implementation"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/digital-signature/#unforgeable-signature-schemes", 
            "text": "When is a signature scheme called unforgeable? Let's consider this game:\nThere is an attacker who knows the public key and a challenger who knows the private key too.   The attacker can pick a document and get the challenger to sign it.  The challenger will sign that document and send the signed data to the attacker.  The game can go on as for as much as the attacker wants (at least until a plausible amount of documents is signed)   Then the attacker tries to sign a message that the challenger has not already signed: if the forged message verifies correctly then the attacker wins, else the challenger wins.  So a signature scheme is  unforgeable  if, not matter what algorithm the attacker is using, he has only a slim chance to succeed.", 
            "title": "Unforgeable signature schemes"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/digital-signature/#use-cases", 
            "text": "Public keys can be used as identities  Signature schemes can be used to sign the last hash pointer in a blockchain, thus signing the whole blockchain.", 
            "title": "Use cases"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/digital-signature/#signature-scheme-used-in-bitcoin", 
            "text": "Bitcoin uses ECDSA. Note that a good randomness source is essential to avoid leaking your private key using your public key.", 
            "title": "Signature scheme used in Bitcoin"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/", 
            "text": "Hash pointer\n\n\nA \nhash pointer\n consists of 2 informations:\n\n\n\n\nAddress where some info is stored\n\n\nHash of that info\n\n\n\n\nHash pointers can be used in every non-cyclical data structures that uses pointers.\n\n\nBlockchain\n\n\nA \nblockchain\n is a list of linked records, called blocks. Each block contains a cryptographic hash of the previous block.\n\n\n\n\nUse cases\n\n\nA blockchain can be used as a tamper evident log. For example, in Bitcoin, a blockchain logs all the transactions (organized in blocks) approved by the network.\n\n\nMerkle tree\n\n\nA \nMerkle tree\n is a tamper evident binary tree structure.\nThe following diagram explains how to build a Merkle tree starting from a known amount of data blocks:\n\n\n\n\nA Merkle tree needs to show \nlog(N)\n items to provide proof of membership for a given data block. The time complexity of this operation is \nlog(N)\n too.\n\n\nUse cases\n\n\nMerkle trees can be used to give informations about a sequence of transactions without needing the data of all the transactions in the sequence, while still preventing attackers to easily tamper that data.", 
            "title": "Hash pointers"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#hash-pointer", 
            "text": "A  hash pointer  consists of 2 informations:   Address where some info is stored  Hash of that info   Hash pointers can be used in every non-cyclical data structures that uses pointers.", 
            "title": "Hash pointer"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#blockchain", 
            "text": "A  blockchain  is a list of linked records, called blocks. Each block contains a cryptographic hash of the previous block.", 
            "title": "Blockchain"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#use-cases", 
            "text": "A blockchain can be used as a tamper evident log. For example, in Bitcoin, a blockchain logs all the transactions (organized in blocks) approved by the network.", 
            "title": "Use cases"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#merkle-tree", 
            "text": "A  Merkle tree  is a tamper evident binary tree structure.\nThe following diagram explains how to build a Merkle tree starting from a known amount of data blocks:   A Merkle tree needs to show  log(N)  items to provide proof of membership for a given data block. The time complexity of this operation is  log(N)  too.", 
            "title": "Merkle tree"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/hash-pointers-and-data-structures/#use-cases_1", 
            "text": "Merkle trees can be used to give informations about a sequence of transactions without needing the data of all the transactions in the sequence, while still preventing attackers to easily tamper that data.", 
            "title": "Use cases"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/centralization-vs-decentralization/", 
            "text": "Centralization vs Decentralization\n\n\nUsually, in complex system, decentralization is not all-or-nothing (e.g. the email protocol is decentralized but there are private email providers).\n\n\nAspects of decentralization in cryptocurrencies\n\n\n\n\nWho maintains the ledger?\n\n\nWho has authority over transactions validity?\n\n\nWho creates new \ncoins\n?\n\n\nWho determines how the rules of the system change?\n\n\nHow does the currency acquire exchange value?\n\n\n\n\nNote that the cryptocurrencies protocols usually are decentralized but services built around them may be centralized (e.g. exchanges).\n\n\nAspects of decentralization in Bitcoin\n\n\n\n\nBitcoin is based on a p2p network:\n anybody is allowed to join the network. Also, the barrier to entry is really low.\n\n\nMining:\n anyone is allowed to mine Bitcoins, but in this case the power concentrates in few entities in the network.\n\n\nSoftware updates:\n Bitcoin core developers are trusted by the whole community and thus they have a centralized power over the network.", 
            "title": "Centralization and decentralization"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/centralization-vs-decentralization/#centralization-vs-decentralization", 
            "text": "Usually, in complex system, decentralization is not all-or-nothing (e.g. the email protocol is decentralized but there are private email providers).", 
            "title": "Centralization vs Decentralization"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/centralization-vs-decentralization/#aspects-of-decentralization-in-cryptocurrencies", 
            "text": "Who maintains the ledger?  Who has authority over transactions validity?  Who creates new  coins ?  Who determines how the rules of the system change?  How does the currency acquire exchange value?   Note that the cryptocurrencies protocols usually are decentralized but services built around them may be centralized (e.g. exchanges).", 
            "title": "Aspects of decentralization in cryptocurrencies"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/centralization-vs-decentralization/#aspects-of-decentralization-in-bitcoin", 
            "text": "Bitcoin is based on a p2p network:  anybody is allowed to join the network. Also, the barrier to entry is really low.  Mining:  anyone is allowed to mine Bitcoins, but in this case the power concentrates in few entities in the network.  Software updates:  Bitcoin core developers are trusted by the whole community and thus they have a centralized power over the network.", 
            "title": "Aspects of decentralization in Bitcoin"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/distributed-consensus/", 
            "text": "Distributed consensus\n\n\nA key challenge of distributed systems is achieving \ndistributed consensus\n, because it is required for reliability in the system.\nAs example, consider a distributed database: if sometimes consensus is not achieved then some databases will not be consistent with the others.\n\n\nDefinition\n\n\nLet's suppose there is a network with \nN\n nodes. Each node receives an input value. Consensus in the network happens if:\n\n\n\n\nThe consensus protocol terminates.\n\n\nAll \nN\n nodes decide on the same value.\n\n\nThe decided value must be one of the input values.\n\n\n\n\nConsensus in Bitcoin\n\n\nLet's examine what happens when Alice wants to pay Bob some bitcoins:\n\n\n\n\nAlice signs the transaction referencing Bob's public key. The transactions contain the hash pointing to previously received coins by Alice.\n\n\nAlice broadcasts the transaction to the whole network.\n\n\nIf Bob wants to be notified of the transaction, he might run a Bitcoin node. But his listening is not required for him to receive coins. The network will acknowledge (if valid) the transaction nonetheless.\n\n\n\n\nIt is really important that the network reaches consensus on the validity and ordering of transactions if we want the whole system to work.\n\n\nBut we cannot solve this problem with an algorithm that has the consensus properties described before, because:\n\n\n\n\nNodes may crash\n\n\nNodes may be malicious\n\n\nA p2p network is imperfect by nature (e.g. not all nodes are connected, there are faults, there is latency because the network has no notion of global time, etc.)\n\n\n\n\nAlso, the literature on distributed consensus is pessimistic, presenting several impossibility results (e.g. \nByzantine generals problem\n)\n\n\nStill, there exist algorithms for achieving distributed consensus that trade off some properties with others (e.g. \nPaxos\n).\n\n\nBut note that the hypotheses under which impossible results were proved are not applicable to the Bitcoin network. In fact, distributed consensus works better in practice than in theory for Bitcoin, because:\n\n\n\n\nThe idea of \nincentive\n is introduced\n\n\nConsensus happens over long periods of time (1h usually), not in fixed periods. As time goes on, the probability that an invalid transaction is considered valid decreases exponentially.\n\n\n\n\nSo Bitcoins solves the distributed consensus problem with a probabilistic approach.", 
            "title": "Distributed consensus"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/distributed-consensus/#distributed-consensus", 
            "text": "A key challenge of distributed systems is achieving  distributed consensus , because it is required for reliability in the system.\nAs example, consider a distributed database: if sometimes consensus is not achieved then some databases will not be consistent with the others.", 
            "title": "Distributed consensus"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/distributed-consensus/#definition", 
            "text": "Let's suppose there is a network with  N  nodes. Each node receives an input value. Consensus in the network happens if:   The consensus protocol terminates.  All  N  nodes decide on the same value.  The decided value must be one of the input values.", 
            "title": "Definition"
        }, 
        {
            "location": "/bitcoin-and-cryptocurrency-technologies/distributed-consensus/#consensus-in-bitcoin", 
            "text": "Let's examine what happens when Alice wants to pay Bob some bitcoins:   Alice signs the transaction referencing Bob's public key. The transactions contain the hash pointing to previously received coins by Alice.  Alice broadcasts the transaction to the whole network.  If Bob wants to be notified of the transaction, he might run a Bitcoin node. But his listening is not required for him to receive coins. The network will acknowledge (if valid) the transaction nonetheless.   It is really important that the network reaches consensus on the validity and ordering of transactions if we want the whole system to work.  But we cannot solve this problem with an algorithm that has the consensus properties described before, because:   Nodes may crash  Nodes may be malicious  A p2p network is imperfect by nature (e.g. not all nodes are connected, there are faults, there is latency because the network has no notion of global time, etc.)   Also, the literature on distributed consensus is pessimistic, presenting several impossibility results (e.g.  Byzantine generals problem )  Still, there exist algorithms for achieving distributed consensus that trade off some properties with others (e.g.  Paxos ).  But note that the hypotheses under which impossible results were proved are not applicable to the Bitcoin network. In fact, distributed consensus works better in practice than in theory for Bitcoin, because:   The idea of  incentive  is introduced  Consensus happens over long periods of time (1h usually), not in fixed periods. As time goes on, the probability that an invalid transaction is considered valid decreases exponentially.   So Bitcoins solves the distributed consensus problem with a probabilistic approach.", 
            "title": "Consensus in Bitcoin"
        }, 
        {
            "location": "/building-microservices/", 
            "text": "Building microservices\n\n\nNotes on the \nBuilding Microservices: Designing Fine-Grained Systems\n book by Sam Newman.\n\n\nAbout this book:\n\n\n\n\nDistributed systems have become more fine-grained in the past 10 years, shifting from code-heavy monolithic applications to smaller, self-contained microservices. But developing these systems brings its own set of headaches. With lots of examples and practical advice, this book takes a holistic view of the topics that system architects and administrators must consider when building, managing, and evolving microservice architectures.\n\n\nMicroservice technologies are moving quickly. Author Sam Newman provides you with a firm grounding in the concepts while diving into current solutions for modeling, integrating, testing, deploying, and monitoring your own autonomous services. You\u2019ll follow a fictional company throughout the book to learn how building a microservice architecture affects a single domain.", 
            "title": "Home"
        }, 
        {
            "location": "/building-microservices/#building-microservices", 
            "text": "Notes on the  Building Microservices: Designing Fine-Grained Systems  book by Sam Newman.  About this book:   Distributed systems have become more fine-grained in the past 10 years, shifting from code-heavy monolithic applications to smaller, self-contained microservices. But developing these systems brings its own set of headaches. With lots of examples and practical advice, this book takes a holistic view of the topics that system architects and administrators must consider when building, managing, and evolving microservice architectures.  Microservice technologies are moving quickly. Author Sam Newman provides you with a firm grounding in the concepts while diving into current solutions for modeling, integrating, testing, deploying, and monitoring your own autonomous services. You\u2019ll follow a fictional company throughout the book to learn how building a microservice architecture affects a single domain.", 
            "title": "Building microservices"
        }, 
        {
            "location": "/building-microservices/microservices/", 
            "text": "Microservices\n\n\nMicroservices are small and autonomous services.\n\n\nBenefits\n\n\n\n\nAllow adoption of new technologies with reduced risk\n\n\nResiliency of services, the system can be kept up on partial failures\n\n\nScaling can be aimed at specific services, thus providing cost savings due to efficiency\n\n\nEase of deployment, can deploy small parts with small deltas to deploy more frequently\n\n\nOrganizational alignment, can assign team of ideal size (not too big, not too small) to a microservice development\n\n\nComposability and reusability of services\n\n\nSmall services can be easily decommissioned and replaced when the need arises\n\n\n\n\nRelationship with SOA\n\n\nSOA has some issues because it's not a well-defined specification, so there are lots of ways to do SOA. Microservices are a specific way to do SOA. Some say they are SOA done right.\n\n\nSimilar decomposition techniques\n\n\nDo we need microservices? Can similar decomposition techniques offer the same benefits provided by microservices?\n\n\nShared libraries\n\n\nDrawbacks:\n\n\n\n\nNeed to run on the same platform as the service (losing technology heterogeneity)\n\n\nCannot scale services independently\n\n\nUnless using DLLs, services cannot load a new version of the library without stopping their execution (losing ease of deploy in isolation)\n\n\nLack of system resiliency\n\n\n\n\nShared libraries are best suited for common code reuse. But be careful: business code reuse can cause coupling in microservices.\n\n\nModules\n\n\nUsually languages do not have proper support for isolated life cycle management of modules, it's hard (if possible at all) for developers to add this functionality.\nThe drawbacks in these cases are nearly the same as the ones provided by shared libraries.\n\n\nConsider even languages that have proper support for ILM (such as Erlang). The system should be based only on that language (losing technology heterogeneity) and this is usually not the case for projects that integrate with legacy software.\n\n\nAlso, in practice, using modules will likely make developers produce coupled code between modules, thus losing independence.\n\n\nNo silver bullet\n\n\nMicroservices are no silver bullet because they add to your system the challenges of distributed systems. Also, you have to be confident with deploys, testing, monitoring and scaling in order to effectively gain the benefits of microservices.", 
            "title": "Microservices"
        }, 
        {
            "location": "/building-microservices/microservices/#microservices", 
            "text": "Microservices are small and autonomous services.", 
            "title": "Microservices"
        }, 
        {
            "location": "/building-microservices/microservices/#benefits", 
            "text": "Allow adoption of new technologies with reduced risk  Resiliency of services, the system can be kept up on partial failures  Scaling can be aimed at specific services, thus providing cost savings due to efficiency  Ease of deployment, can deploy small parts with small deltas to deploy more frequently  Organizational alignment, can assign team of ideal size (not too big, not too small) to a microservice development  Composability and reusability of services  Small services can be easily decommissioned and replaced when the need arises", 
            "title": "Benefits"
        }, 
        {
            "location": "/building-microservices/microservices/#relationship-with-soa", 
            "text": "SOA has some issues because it's not a well-defined specification, so there are lots of ways to do SOA. Microservices are a specific way to do SOA. Some say they are SOA done right.", 
            "title": "Relationship with SOA"
        }, 
        {
            "location": "/building-microservices/microservices/#similar-decomposition-techniques", 
            "text": "Do we need microservices? Can similar decomposition techniques offer the same benefits provided by microservices?", 
            "title": "Similar decomposition techniques"
        }, 
        {
            "location": "/building-microservices/microservices/#shared-libraries", 
            "text": "Drawbacks:   Need to run on the same platform as the service (losing technology heterogeneity)  Cannot scale services independently  Unless using DLLs, services cannot load a new version of the library without stopping their execution (losing ease of deploy in isolation)  Lack of system resiliency   Shared libraries are best suited for common code reuse. But be careful: business code reuse can cause coupling in microservices.", 
            "title": "Shared libraries"
        }, 
        {
            "location": "/building-microservices/microservices/#modules", 
            "text": "Usually languages do not have proper support for isolated life cycle management of modules, it's hard (if possible at all) for developers to add this functionality.\nThe drawbacks in these cases are nearly the same as the ones provided by shared libraries.  Consider even languages that have proper support for ILM (such as Erlang). The system should be based only on that language (losing technology heterogeneity) and this is usually not the case for projects that integrate with legacy software.  Also, in practice, using modules will likely make developers produce coupled code between modules, thus losing independence.", 
            "title": "Modules"
        }, 
        {
            "location": "/building-microservices/microservices/#no-silver-bullet", 
            "text": "Microservices are no silver bullet because they add to your system the challenges of distributed systems. Also, you have to be confident with deploys, testing, monitoring and scaling in order to effectively gain the benefits of microservices.", 
            "title": "No silver bullet"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/", 
            "text": "Evolutionary architects\n\n\nArchitects of microservices based systems need to face difficult choices:\n\n\n\n\nDegree of technology unification between microservices\n\n\nTeam policies (e.g. allow different teams to use different patterns?)\n\n\nHow to merge/split microservices?\n\n\n\n\nBut to provide effective guidance we must first understand the role of software architects in IT.\n\n\nIT is a young industry that borrowed the \narchitect\n term from actual architects and engineers but there is a substantial difference in these roles: software is not subject to physical constraints. Software is flexible and can be easily adapted and evolved to new requirements.\n\n\nArchitects need to:\n\n\n\n\nSet direction in broad strokes (i.e. set software zones), they must be involved in specific implementation details in limited cases\n\n\nEnsure that the system is suitable for the current requirements\n\n\nEnsure that the system can accommodate future requirements\n\n\nMake the system work for both users and developers\n\n\nUnderstand implementation complexity\n\n\n\n\nTip: an architect should spend some time working on user stories with developers to better understand the state/challenges of the system.\n\n\nA principled approach\n\n\nThere are lots of tradeoffs in decisions about microservices based systems. Defining a set of principles and practices can guide us through these choices.\n\n\nPrinciples\n are rules made by an architect to align the development activity to larger system goals. An example is the \n12 factor app\n, defined by Heroku to guide the development of scalable cloud SaaS applications.\n\n\nPractices\n are ways to make sure a principle is followed.\nPractices can differ when following same principles (e.g. different practices for .NET and Java systems following the same principles).\n\n\nPrinciples and practices adopted should depend on goals that we want to achieve, also taking into account \nstrategic goals\n (i.e. the long term goals of your organization).\n\n\nThis is what happens in the real world:\n\n\n\n\nThe required standard\n\n\nOne of the core balances to find is how much variability to allow in your system.\nToo much variability can cause issues such as onboarding problems and other expressed before.\nOne way to identify a standard is to identify the attributes of an ideal microservice.\n\n\nZoning\n\n\nOur zones are service boundaries or groups of services.\nAs architects, more important to know how services communicate between each other than how a single isolated service works.\n\n\nMany organizations are using microservices to make teams more autonomous, architects then rely on those teams to make local optimal decisions.\nStill, care is needed for choosing the technologies of single services: sparse technology does not facilitate experience growth and makes it harder for developers to switch teams.\n\n\nAlso, care of protocol for communication between microservices, because each ms will need to know how to operate with a certain protocol and this adds complexity.\n\n\nMonitoring\n\n\nIt's important to monitor the health of the whole system and gather health and log data in a single place in order to analyze it. Remember to use an agnostic log/health/data reporting protocol/format so your monitoring system does not change as services change.\n\n\nInterfaces\n\n\nKeep interfaces of services as simple as possible, supporting the minimum standards required.\nThis makes it easier to handle versioning and system complexity, because it will be easier to evolve the system.\n\n\nArchitectural safety\n\n\nServices need to resist to partial failures in the system. A partial failure should not affect the system as a whole.\n\n\nGovernance through code\n\n\nMaking sure that developers are implementing the defined standards can be a burden. Exemplars and service templates help a lot with this problem.\n\n\nExemplars\n should ideally be real-world services following your standards. Developers can safely look at exemplars to further develop the application.\n\n\nService templates\n are a set of technologies or even frameworks to be used in your services. These can guide the developer teams and make their work easier. But be careful: frameworks should not be enforced by an external team and they should be user-friendly. Another danger is that service templates can cause coupling between services.\n\n\nTechnical debt\n\n\nOften the technical vision cannot be fully followed through because of business requirements. This is a source of technical debt because a short-term benefit will be paid with a long-term cost.\nTeams can manage their technical debt or it can be managed by a centralized source.\n\n\nHandling exceptions to the rules\n\n\nSometimes you will need to build a part of your system while not following some rules of the standard you defined. If you find this happening too often, it can make sense to change you rule set.\n\n\nSummary\n\n\nThe following core responsibilities of architects emerged:\n\n\n\n\nVision\n\n\nEmpathy\n\n\nCollaboration\n\n\nAdaptability\n\n\nTeam autonomy\n\n\nGovernance\n\n\n\n\nArchitects need to constantly balance aspects of their systems to successfully do their job.", 
            "title": "Evolutionary architects"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#evolutionary-architects", 
            "text": "Architects of microservices based systems need to face difficult choices:   Degree of technology unification between microservices  Team policies (e.g. allow different teams to use different patterns?)  How to merge/split microservices?   But to provide effective guidance we must first understand the role of software architects in IT.  IT is a young industry that borrowed the  architect  term from actual architects and engineers but there is a substantial difference in these roles: software is not subject to physical constraints. Software is flexible and can be easily adapted and evolved to new requirements.  Architects need to:   Set direction in broad strokes (i.e. set software zones), they must be involved in specific implementation details in limited cases  Ensure that the system is suitable for the current requirements  Ensure that the system can accommodate future requirements  Make the system work for both users and developers  Understand implementation complexity   Tip: an architect should spend some time working on user stories with developers to better understand the state/challenges of the system.", 
            "title": "Evolutionary architects"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#a-principled-approach", 
            "text": "There are lots of tradeoffs in decisions about microservices based systems. Defining a set of principles and practices can guide us through these choices.  Principles  are rules made by an architect to align the development activity to larger system goals. An example is the  12 factor app , defined by Heroku to guide the development of scalable cloud SaaS applications.  Practices  are ways to make sure a principle is followed.\nPractices can differ when following same principles (e.g. different practices for .NET and Java systems following the same principles).  Principles and practices adopted should depend on goals that we want to achieve, also taking into account  strategic goals  (i.e. the long term goals of your organization).  This is what happens in the real world:", 
            "title": "A principled approach"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#the-required-standard", 
            "text": "One of the core balances to find is how much variability to allow in your system.\nToo much variability can cause issues such as onboarding problems and other expressed before.\nOne way to identify a standard is to identify the attributes of an ideal microservice.", 
            "title": "The required standard"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#zoning", 
            "text": "Our zones are service boundaries or groups of services.\nAs architects, more important to know how services communicate between each other than how a single isolated service works.  Many organizations are using microservices to make teams more autonomous, architects then rely on those teams to make local optimal decisions.\nStill, care is needed for choosing the technologies of single services: sparse technology does not facilitate experience growth and makes it harder for developers to switch teams.  Also, care of protocol for communication between microservices, because each ms will need to know how to operate with a certain protocol and this adds complexity.", 
            "title": "Zoning"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#monitoring", 
            "text": "It's important to monitor the health of the whole system and gather health and log data in a single place in order to analyze it. Remember to use an agnostic log/health/data reporting protocol/format so your monitoring system does not change as services change.", 
            "title": "Monitoring"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#interfaces", 
            "text": "Keep interfaces of services as simple as possible, supporting the minimum standards required.\nThis makes it easier to handle versioning and system complexity, because it will be easier to evolve the system.", 
            "title": "Interfaces"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#architectural-safety", 
            "text": "Services need to resist to partial failures in the system. A partial failure should not affect the system as a whole.", 
            "title": "Architectural safety"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#governance-through-code", 
            "text": "Making sure that developers are implementing the defined standards can be a burden. Exemplars and service templates help a lot with this problem.  Exemplars  should ideally be real-world services following your standards. Developers can safely look at exemplars to further develop the application.  Service templates  are a set of technologies or even frameworks to be used in your services. These can guide the developer teams and make their work easier. But be careful: frameworks should not be enforced by an external team and they should be user-friendly. Another danger is that service templates can cause coupling between services.", 
            "title": "Governance through code"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#technical-debt", 
            "text": "Often the technical vision cannot be fully followed through because of business requirements. This is a source of technical debt because a short-term benefit will be paid with a long-term cost.\nTeams can manage their technical debt or it can be managed by a centralized source.", 
            "title": "Technical debt"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#handling-exceptions-to-the-rules", 
            "text": "Sometimes you will need to build a part of your system while not following some rules of the standard you defined. If you find this happening too often, it can make sense to change you rule set.", 
            "title": "Handling exceptions to the rules"
        }, 
        {
            "location": "/building-microservices/evolutionary-architects/#summary", 
            "text": "The following core responsibilities of architects emerged:   Vision  Empathy  Collaboration  Adaptability  Team autonomy  Governance   Architects need to constantly balance aspects of their systems to successfully do their job.", 
            "title": "Summary"
        }, 
        {
            "location": "/building-microservices/how-to-model-services/", 
            "text": "How to model services\n\n\nWhat makes a good service? The whole point of microservices is the ability to deploy them independently, so \nloose coupling\n and \nhigh cohesion\n (same kind of logic not distributed across different microservices) are needed.\n\n\nLet's introduce the fictional domain of MusicCorp, an old company who wants to sell music tapes online.\n\n\nThe Bounded Context\n\n\nBounded context is a concept introduced in DDD (\nDomain Driven Design\n). A bounded context has private and public models relative to a domain context. Only public models are exposed to other contexts. Also, public models can be a different representation of private models (they can be \nmapped\n models).\n\n\nThis allows for both loose coupling, since there are no references to whole models, and high cohesion, since these contexts are modeled from actual domain contexts.\n\n\nFor example, in the MusicCorp online business, \nWarehouse\n and \nFinance\n are different bounded contexts.\n\n\n\n\nHere \nStock item\n is a public model shared by each context which has different representations in each context.\n\n\nThe benefits provided by bounded contexts makes them really good candidates to be microservices.\n\n\nBut be careful: restructuring bounded contexts has a high cost, so architects must not fall into the \npremature decomposition\n trap. If you have a solid understanding and vision of the whole system, your bounded contexts will be solid. Otherwise, you may need to reorganize them frequently.\n\n\nNesting\n\n\nIt's best to think first about coarse-grained bounded contexts. Then these usually can be further divided into subcontexts. Should you keep the nested contexts public or private to the parent context?\nUsually, if each subcontext has a respective team that handles that area in the organization, it's ideal to make each subcontext public (thus an effective context).", 
            "title": "How to model services"
        }, 
        {
            "location": "/building-microservices/how-to-model-services/#how-to-model-services", 
            "text": "What makes a good service? The whole point of microservices is the ability to deploy them independently, so  loose coupling  and  high cohesion  (same kind of logic not distributed across different microservices) are needed.  Let's introduce the fictional domain of MusicCorp, an old company who wants to sell music tapes online.", 
            "title": "How to model services"
        }, 
        {
            "location": "/building-microservices/how-to-model-services/#the-bounded-context", 
            "text": "Bounded context is a concept introduced in DDD ( Domain Driven Design ). A bounded context has private and public models relative to a domain context. Only public models are exposed to other contexts. Also, public models can be a different representation of private models (they can be  mapped  models).  This allows for both loose coupling, since there are no references to whole models, and high cohesion, since these contexts are modeled from actual domain contexts.  For example, in the MusicCorp online business,  Warehouse  and  Finance  are different bounded contexts.   Here  Stock item  is a public model shared by each context which has different representations in each context.  The benefits provided by bounded contexts makes them really good candidates to be microservices.  But be careful: restructuring bounded contexts has a high cost, so architects must not fall into the  premature decomposition  trap. If you have a solid understanding and vision of the whole system, your bounded contexts will be solid. Otherwise, you may need to reorganize them frequently.", 
            "title": "The Bounded Context"
        }, 
        {
            "location": "/building-microservices/how-to-model-services/#nesting", 
            "text": "It's best to think first about coarse-grained bounded contexts. Then these usually can be further divided into subcontexts. Should you keep the nested contexts public or private to the parent context?\nUsually, if each subcontext has a respective team that handles that area in the organization, it's ideal to make each subcontext public (thus an effective context).", 
            "title": "Nesting"
        }, 
        {
            "location": "/building-microservices/integration/", 
            "text": "Integration\n\n\nDesirable properties of communication between microservices:\n\n\n\n\nAvoid breaking changes as much as possible.\n\n\nTechnology agnostic APIs.\n\n\nMake it easy to consume APIs.\n\n\nHidden internal implementation details.\n\n\n\n\nShared database\n\n\nThe most common form of integration.\n\n\n\n\nHas the following issues:\n\n\n\n\nInternal representations are not private, causing high coupling.\n\n\nLogic to modify some kind of data is present in different services, causing loss of cohesion.\n\n\nEvery kind of data must be stored using the same DBMS technology.\n\n\n\n\nThese issues would eliminate the benefits of using microservices, so shared databases are to avoid.\n\n\nSynchronous vs Asynchronous\n\n\nSynchronous communication\n starts with a blocking call to the server that resolves once the operation completes. It's easy to debug but lacks capabilities to effectively handle long-running processes.\n\n\nAsynchronous communication\n does not wait for the server to respond. In theory, a client may even not need to know if the server completed the operation. It's not easy to debug but can effectively handle long-running processes.\n\n\nThese two different modes of communication can enable two different styles of collaboration:\n\n\n\n\nRequest/response:\n natural fit to synchronous communication, can handle asynchronous communication too using callbacks.\n\n\nEvent-based:\n natural fit to asynchronous communication. It's more flexible since a client just issues an event, allowing for more services to listen on that event later on, without modifying the client's code.\n\n\n\n\nOrchestration vs Choreography\n\n\nOrchestration\n means having an orchestrator service that instructs other services on what to do and organizes the whole flow. This provides a clear view of the whole flow but can cause coupling if the orchestrator becomes a \u201cgod\u201d microservice.\n\n\nChoreography\n means that services can issue or listen to events. This approach keeps services decoupled but can make it hard to understand the whole flow.\n\n\nRemote procedure calls\n\n\nRemote procedure call refers to the technique of making a local call and having it execute on a remote service somewhere.\n\n\nRPC fit well with the request/response collaboration style.\n\n\nThe selling point of RPC is ease of use: it's really practical to make a remote call look like a local call.\n\n\nHowever, RPC has issues too:\n\n\n\n\nUsually it causes technology coupling between client and server.\n\n\nLocal calls must not be confused with remote calls, because of latency and unreliability.\n\n\nBrittleness, because server signatures and interfaces need to match exactly the ones in the client.\n\n\n\n\nCompared to database integration, RPC is certainly an improvement when we think about options for request/response collaboration.\n\n\nREST\n\n\nREpresentational State Transfer (REST) is an architectural style inspired by the Web. The most important concept is the one of resource, which can be requested in different representations. This favours decoupling between internal and external representations.\n\n\nThere are many different styles of REST, compared in the \nRichardson Maturity Model\n.\n\n\nUsually REST is implemented over HTTP because HTTP provides parts of the REST specification, such as verbs. Also, there are lots of tools supporting REST with HTTP.\n\n\nHATEOAS\n\n\nAnother principle introduced in REST that can help us avoid the coupling between client and server is the concept of \nhypermedia as the engine of application state\n (often abbreviated as HATEOAS). One of the downsides is that the navigation of controls can be quite chatty, as the client needs to follow links to find the operation it wants to perform. Ultimately, this is a trade-off.\n\n\nSerialization format\n\n\nREST provides flexibility over the serialization format of the data. The most popular choices are JSON and XML.\nXML has built-in support for hypermedia while there are standards to provide hypermedia data with JSON.\n\n\nDownsides to REST Over HTTP\n\n\n\n\nNot easy to generate stubs for REST over HTTP services as it would be with RPC.\n\n\nSome web servers do not \nfully\n support all the HTTP verbs.\n\n\nPerformance is penalized because of hypermedia data and HTTP overhead.\n\n\nHTTP is not suited for frequently exchanging small volumes of data, WebSockets or protocol buffers are more suitable for this kind of communication.\n\n\n\n\nDespite these disadvantages, REST over HTTP is a sensible default choice for service-to-service interactions.\n\n\nImplementing Asynchronous Event-Based Collaboration\n\n\nTo implement asynchronous event-based collaboration we need to consider:\n\n\n\n\nA way for our microservices to emit events.\n\n\nA way for our consumers to find out those events have happened.\n\n\n\n\nTraditionally, \nmessage brokers\n like RabbitMQ can handle both problems, while also being able to scale and have resiliency.\n\n\nBut note that this kind of collaboration comes with a system complexity increase (e.g. if you're not careful, you could have \ncatastrophic failovers\n as intended by Martin Fowler).\n\n\nReactive extensions\n can help you a lot when handling lots of calls to downstream services. They are a popular choice in distributed systems.\n\n\nDRY in Microservices\n\n\nFollowing the DRY principle can cause coupling between microservices. As a general rule, DRY is to be followed only inside service boundaries. Across different services, code duplication is a smaller problem than coupling. An exception to this rule can be model-agnostic code such as logging, which can be safely shared between microservices.\n\n\nClient libraries\n\n\nClient libraries can cause coupling between services and clients. To limit this danger, it's best if different developer teams develop the server API and the client library: this way there should be no \"logic leaks\" from the server into the client.\nIt's also important to give clients control on when to upgrade their client libraries, to avoid coupling in deploys.\n\n\nAccess by Reference\n\n\nSometimes it may happen to pass around outdated information: we request a \nCustomer\n and then we use that customer in another request, but in the meanwhile it has changed.\nIn order to retrieve the current state, such requests must include an ID of the involved resources. But this approach has downsides too:\n\n\n\n\nIt may cause the \nCustomers\n service to be accessed too much.\n\n\nIt causes overhead in requests.\n\n\n\n\nThis is a tradeoff to consider. The point is: be aware of the freshness of data passed between microservices.\n\n\nService versioning\n\n\nThe following points can help you have a good service versioning in your system:\n\n\n\n\nDefer breaking changes as long as possible (e.g. by using the \nTolerant reader\n pattern).\n\n\nRobustness principle: \u201c\nBe conservative in what you do, be liberal in what you accept from others\n\u201d.\n\n\nCatch breaking changes early, tests help a lot here.\n\n\nUse semantic versioning.\n\n\nHave coexisting service versions to gradually adopt the new version in the system. Another option is to concurrently deploy microservices of different versions, but suppose you need to fix a bug in the service, then you would need to deploy 2 different services. Still, this is a good approach if you are doing blue/green deploys.\n\n\n\n\nUser interfaces\n\n\nEach type of user interface (e.g. browser, desktop, mobile) has its own constraints. So even though our core services are the same, we might need a way to adapt them for these constraints.\n\n\nLet\u2019s look at a few models of user interfaces to see how this might be\nachieved.\n\n\nAPI composition\n\n\nEach part of the UI communicates with a specific service via its API.\n\n\n\n\nDownsides:\n\n\n\n\nLittle ability to tailor the responses for different sorts of devices.\n\n\nIf another team is creating the UI, making even small changes requires change requests to multiple teams.\n\n\nThis communication could also be fairly chatty. Opening lots of calls directly to services can be quite intensive for mobile devices.\n\n\n\n\nUI Fragment composition\n\n\nRather than having our UI make API calls and map everything back to UI controls, we\ncould have our services provide parts of the UI directly.\n\n\n\n\nThe same team that makes changes to the services can also be in charge of making changes to those parts of the UI, allowing to get changes out faster.\n\n\nDownsides:\n\n\n\n\nWe need to ensure consistency of the user experience, CSS and HTML style guides can help.\n\n\nNot ideal for native interfaces, it would require falling back to the API composition model.\n\n\nThe more cross-cutting a form of interaction is, the less likely this model will fit, falling back to the API composition model.\n\n\n\n\nBackends for Frontends\n\n\nA common solution to the problem of chatty interfaces with backend services, or the need to vary content for different types of devices, is to have a server-side aggregation endpoint, or \nAPI gateway\n.\n\n\n\n\nThe problem that can occur is that normally we\u2019ll have one giant layer for all our services, losing ability to deploy clients independently.\n\n\nA model that solves this problem is \nBackends for frontends\n (\nBFFs\n), it restricts the use of backends for a specific client.\n\n\n\n\nThe danger with this approach is the same as with any aggregating layer: it can take on logic it shouldn\u2019t. These BFFs should only contain behavior specific to delivering a particular user experience.\n\n\nA Hybrid Approach\n\n\nSome systems use different models together (e.g. BFFs for mobile and UI fragment composition for web). The tricky part still remains avoiding putting too much logic into any intermediate layer. This causes coupling and low cohesion.\n\n\nIntegrating with Third-Party Software\n\n\nChallenges associated with integrating third-party software into your system:\n\n\n\n\nLack of control:\n probably many of the technical decisions have been made for you to simplify product usage. The tool selection process should take into account ease of use of third-party software.\n\n\nCustomization:\n many enterprise tools sell themselves on their ability to be heavily customized just for you. But the cost of customization can be more expensive than building something bespoke from scratch.\n\n\nIntegration spaghetti:\n ideally you want to standardize on a small number of types of integration. If one product forces you tu use proprietary protocols, it could mean troubles.\n\n\n\n\nBest practices:\n\n\n\n\nTreat third-party software as a service and place all the customization code in services you control, if possible.\n\n\nWhen moving away from integrated COTS or legacy software, adopt the \nStrangler Application Pattern\n: intercept calls to such software and route them either to the legacy services or to your new services. This allows for a gradual switch.\n\n\n\n\nSummary\n\n\nTo ensure our microservices remain as decoupled as possible from their other collaborators:\n\n\n\n\nAvoid database integration at all costs.\n\n\nUnderstand the trade-offs between REST and RPC, but strongly consider REST as a good starting point for request/response integration.\n\n\nPrefer choreography over orchestration.\n\n\nAvoid breaking changes and the need to version by understanding Postel\u2019s Law and using tolerant readers.\n\n\nThink of user interfaces as compositional layers.", 
            "title": "Integration"
        }, 
        {
            "location": "/building-microservices/integration/#integration", 
            "text": "Desirable properties of communication between microservices:   Avoid breaking changes as much as possible.  Technology agnostic APIs.  Make it easy to consume APIs.  Hidden internal implementation details.", 
            "title": "Integration"
        }, 
        {
            "location": "/building-microservices/integration/#shared-database", 
            "text": "The most common form of integration.   Has the following issues:   Internal representations are not private, causing high coupling.  Logic to modify some kind of data is present in different services, causing loss of cohesion.  Every kind of data must be stored using the same DBMS technology.   These issues would eliminate the benefits of using microservices, so shared databases are to avoid.", 
            "title": "Shared database"
        }, 
        {
            "location": "/building-microservices/integration/#synchronous-vs-asynchronous", 
            "text": "Synchronous communication  starts with a blocking call to the server that resolves once the operation completes. It's easy to debug but lacks capabilities to effectively handle long-running processes.  Asynchronous communication  does not wait for the server to respond. In theory, a client may even not need to know if the server completed the operation. It's not easy to debug but can effectively handle long-running processes.  These two different modes of communication can enable two different styles of collaboration:   Request/response:  natural fit to synchronous communication, can handle asynchronous communication too using callbacks.  Event-based:  natural fit to asynchronous communication. It's more flexible since a client just issues an event, allowing for more services to listen on that event later on, without modifying the client's code.", 
            "title": "Synchronous vs Asynchronous"
        }, 
        {
            "location": "/building-microservices/integration/#orchestration-vs-choreography", 
            "text": "Orchestration  means having an orchestrator service that instructs other services on what to do and organizes the whole flow. This provides a clear view of the whole flow but can cause coupling if the orchestrator becomes a \u201cgod\u201d microservice.  Choreography  means that services can issue or listen to events. This approach keeps services decoupled but can make it hard to understand the whole flow.", 
            "title": "Orchestration vs Choreography"
        }, 
        {
            "location": "/building-microservices/integration/#remote-procedure-calls", 
            "text": "Remote procedure call refers to the technique of making a local call and having it execute on a remote service somewhere.  RPC fit well with the request/response collaboration style.  The selling point of RPC is ease of use: it's really practical to make a remote call look like a local call.  However, RPC has issues too:   Usually it causes technology coupling between client and server.  Local calls must not be confused with remote calls, because of latency and unreliability.  Brittleness, because server signatures and interfaces need to match exactly the ones in the client.   Compared to database integration, RPC is certainly an improvement when we think about options for request/response collaboration.", 
            "title": "Remote procedure calls"
        }, 
        {
            "location": "/building-microservices/integration/#rest", 
            "text": "REpresentational State Transfer (REST) is an architectural style inspired by the Web. The most important concept is the one of resource, which can be requested in different representations. This favours decoupling between internal and external representations.  There are many different styles of REST, compared in the  Richardson Maturity Model .  Usually REST is implemented over HTTP because HTTP provides parts of the REST specification, such as verbs. Also, there are lots of tools supporting REST with HTTP.", 
            "title": "REST"
        }, 
        {
            "location": "/building-microservices/integration/#hateoas", 
            "text": "Another principle introduced in REST that can help us avoid the coupling between client and server is the concept of  hypermedia as the engine of application state  (often abbreviated as HATEOAS). One of the downsides is that the navigation of controls can be quite chatty, as the client needs to follow links to find the operation it wants to perform. Ultimately, this is a trade-off.", 
            "title": "HATEOAS"
        }, 
        {
            "location": "/building-microservices/integration/#serialization-format", 
            "text": "REST provides flexibility over the serialization format of the data. The most popular choices are JSON and XML.\nXML has built-in support for hypermedia while there are standards to provide hypermedia data with JSON.", 
            "title": "Serialization format"
        }, 
        {
            "location": "/building-microservices/integration/#downsides-to-rest-over-http", 
            "text": "Not easy to generate stubs for REST over HTTP services as it would be with RPC.  Some web servers do not  fully  support all the HTTP verbs.  Performance is penalized because of hypermedia data and HTTP overhead.  HTTP is not suited for frequently exchanging small volumes of data, WebSockets or protocol buffers are more suitable for this kind of communication.   Despite these disadvantages, REST over HTTP is a sensible default choice for service-to-service interactions.", 
            "title": "Downsides to REST Over HTTP"
        }, 
        {
            "location": "/building-microservices/integration/#implementing-asynchronous-event-based-collaboration", 
            "text": "To implement asynchronous event-based collaboration we need to consider:   A way for our microservices to emit events.  A way for our consumers to find out those events have happened.   Traditionally,  message brokers  like RabbitMQ can handle both problems, while also being able to scale and have resiliency.  But note that this kind of collaboration comes with a system complexity increase (e.g. if you're not careful, you could have  catastrophic failovers  as intended by Martin Fowler).  Reactive extensions  can help you a lot when handling lots of calls to downstream services. They are a popular choice in distributed systems.", 
            "title": "Implementing Asynchronous Event-Based Collaboration"
        }, 
        {
            "location": "/building-microservices/integration/#dry-in-microservices", 
            "text": "Following the DRY principle can cause coupling between microservices. As a general rule, DRY is to be followed only inside service boundaries. Across different services, code duplication is a smaller problem than coupling. An exception to this rule can be model-agnostic code such as logging, which can be safely shared between microservices.", 
            "title": "DRY in Microservices"
        }, 
        {
            "location": "/building-microservices/integration/#client-libraries", 
            "text": "Client libraries can cause coupling between services and clients. To limit this danger, it's best if different developer teams develop the server API and the client library: this way there should be no \"logic leaks\" from the server into the client.\nIt's also important to give clients control on when to upgrade their client libraries, to avoid coupling in deploys.", 
            "title": "Client libraries"
        }, 
        {
            "location": "/building-microservices/integration/#access-by-reference", 
            "text": "Sometimes it may happen to pass around outdated information: we request a  Customer  and then we use that customer in another request, but in the meanwhile it has changed.\nIn order to retrieve the current state, such requests must include an ID of the involved resources. But this approach has downsides too:   It may cause the  Customers  service to be accessed too much.  It causes overhead in requests.   This is a tradeoff to consider. The point is: be aware of the freshness of data passed between microservices.", 
            "title": "Access by Reference"
        }, 
        {
            "location": "/building-microservices/integration/#service-versioning", 
            "text": "The following points can help you have a good service versioning in your system:   Defer breaking changes as long as possible (e.g. by using the  Tolerant reader  pattern).  Robustness principle: \u201c Be conservative in what you do, be liberal in what you accept from others \u201d.  Catch breaking changes early, tests help a lot here.  Use semantic versioning.  Have coexisting service versions to gradually adopt the new version in the system. Another option is to concurrently deploy microservices of different versions, but suppose you need to fix a bug in the service, then you would need to deploy 2 different services. Still, this is a good approach if you are doing blue/green deploys.", 
            "title": "Service versioning"
        }, 
        {
            "location": "/building-microservices/integration/#user-interfaces", 
            "text": "Each type of user interface (e.g. browser, desktop, mobile) has its own constraints. So even though our core services are the same, we might need a way to adapt them for these constraints.  Let\u2019s look at a few models of user interfaces to see how this might be\nachieved.", 
            "title": "User interfaces"
        }, 
        {
            "location": "/building-microservices/integration/#api-composition", 
            "text": "Each part of the UI communicates with a specific service via its API.   Downsides:   Little ability to tailor the responses for different sorts of devices.  If another team is creating the UI, making even small changes requires change requests to multiple teams.  This communication could also be fairly chatty. Opening lots of calls directly to services can be quite intensive for mobile devices.", 
            "title": "API composition"
        }, 
        {
            "location": "/building-microservices/integration/#ui-fragment-composition", 
            "text": "Rather than having our UI make API calls and map everything back to UI controls, we\ncould have our services provide parts of the UI directly.   The same team that makes changes to the services can also be in charge of making changes to those parts of the UI, allowing to get changes out faster.  Downsides:   We need to ensure consistency of the user experience, CSS and HTML style guides can help.  Not ideal for native interfaces, it would require falling back to the API composition model.  The more cross-cutting a form of interaction is, the less likely this model will fit, falling back to the API composition model.", 
            "title": "UI Fragment composition"
        }, 
        {
            "location": "/building-microservices/integration/#backends-for-frontends", 
            "text": "A common solution to the problem of chatty interfaces with backend services, or the need to vary content for different types of devices, is to have a server-side aggregation endpoint, or  API gateway .   The problem that can occur is that normally we\u2019ll have one giant layer for all our services, losing ability to deploy clients independently.  A model that solves this problem is  Backends for frontends  ( BFFs ), it restricts the use of backends for a specific client.   The danger with this approach is the same as with any aggregating layer: it can take on logic it shouldn\u2019t. These BFFs should only contain behavior specific to delivering a particular user experience.", 
            "title": "Backends for Frontends"
        }, 
        {
            "location": "/building-microservices/integration/#a-hybrid-approach", 
            "text": "Some systems use different models together (e.g. BFFs for mobile and UI fragment composition for web). The tricky part still remains avoiding putting too much logic into any intermediate layer. This causes coupling and low cohesion.", 
            "title": "A Hybrid Approach"
        }, 
        {
            "location": "/building-microservices/integration/#integrating-with-third-party-software", 
            "text": "Challenges associated with integrating third-party software into your system:   Lack of control:  probably many of the technical decisions have been made for you to simplify product usage. The tool selection process should take into account ease of use of third-party software.  Customization:  many enterprise tools sell themselves on their ability to be heavily customized just for you. But the cost of customization can be more expensive than building something bespoke from scratch.  Integration spaghetti:  ideally you want to standardize on a small number of types of integration. If one product forces you tu use proprietary protocols, it could mean troubles.   Best practices:   Treat third-party software as a service and place all the customization code in services you control, if possible.  When moving away from integrated COTS or legacy software, adopt the  Strangler Application Pattern : intercept calls to such software and route them either to the legacy services or to your new services. This allows for a gradual switch.", 
            "title": "Integrating with Third-Party Software"
        }, 
        {
            "location": "/building-microservices/integration/#summary", 
            "text": "To ensure our microservices remain as decoupled as possible from their other collaborators:   Avoid database integration at all costs.  Understand the trade-offs between REST and RPC, but strongly consider REST as a good starting point for request/response integration.  Prefer choreography over orchestration.  Avoid breaking changes and the need to version by understanding Postel\u2019s Law and using tolerant readers.  Think of user interfaces as compositional layers.", 
            "title": "Summary"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/", 
            "text": "Splitting the monolith\n\n\nWhy would you want to split a monolith?\n\n\n\n\nThere are lots of changes coming to a part of the monolith, splitting that part into a service will make you roll out those changes faster.\n\n\nSeparate teams work on separate parts of the monolith.\n\n\nA part of the monolith requires high security measures not needed by the rest of the system.\n\n\nA part of the monolith can be improved by switching technology.\n\n\n\n\nHow do we go about decomposing monolithic applications without having to embark on a big-bang rewrite?\n\n\nSeams\n\n\nWe want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both.\n\n\nA seam is a portion of the code that can be treated in isolation and worked on without impacting the rest of the codebase. Bounded contexts are good seams.\n\n\nSo when splitting, the first step is to identify seams in our system and then gradually move the code of these seams into different packages. Tests are really useful to make sure you're not introducing bugs with this packaging. This process will also help identify seams that you did not think of: they will come out when you are left with some code that you don't know in which package to place.\n\n\nThe splitting should start from the seam that is least depended on.\n\n\nDatabases\n\n\nWe have to find seams in databases too, but this is a difficult task.\n\n\nAfter having packaged your application code by seams, you should do the same for the code accessing the database (usually the code in the so called \nrepository layer\n).\n\n\nForeign keys\n\n\nSome tables may have foreign keys linking them to other tables. A common solution for this problem is to remove the table relationship and make the service accessing that table call the API of the service handling the other table.\n\n\n\n\nShared static data\n\n\nLet's suppose we have different services accessing a table filled with static data.\n\n\n\n\nThere are several solutions:\n\n\n\n\nDuplicate tables in each db, but this can cause consistency issues.\n\n\nTreat static data as code/configuration files in each service. This can cause consistency issues too, but they would be far easier to solve.\n\n\nCreate a microservice to handle the static data. This is overkill in most situations, but it can be justified if the static data has high complexity.\n\n\n\n\nShared mutable data\n\n\nLet's suppose we have different services accessing a table filled with mutable data.\n\n\n\n\nUsually this means we need a \nCustomer\n microservice to handle that data. This service can then be called by \nWarehouse\n and \nFinance\n.\n\n\nShared tables\n\n\nLet's suppose we have different services accessing a table which aggregates different information in the same record (catalog entry and stock level).\n\n\n\n\nThe answer here is to split the table in two, creating a stock levels table for the \nWarehouse\n and a catalog entry table for the \nCatalog\n.\n\n\nStaging the break\n\n\nThe best way to commit the database changes would be to keep the services together and split the schemas. The db split will increase the number of db calls and make you lose transactional integrity. Having the same application will enable you to deal more easily with these problems. Then, when you are satisfied with the new db, you can commit the changes.\n\n\nTransactional Boundaries\n\n\nTransactions allow us to say that operations either all happen together, or none of them happen.\n\n\nTransactions are typically used in databases, but they can be supported but other systems such as message brokers.\n\n\nSplitting schemas will cause the loss of transactional integrity in our system. There are several solutions to this problem:\n\n\n\n\nA \ntry again later\n mechanism, but this alone is not sufficient since it assumes that eventually a failed request will be successful. This is a form of \neventual consistency\n: rather than using a transactional boundary to ensure that the system is in a consistent state when the transaction completes, instead we accept that the system will get itself into a consistent state at some point in the future.\n\n\nCompensating transactions\n can be used to undo the committed transactions preceding a failed operation. But what if a compensating transaction fails? We would need other mechanism such as automated jobs or human administration. Also, this mechanism becomes more difficult to manage as the number of operations increases in transactions.\n\n\nDistributed transactions\n are transactions done across different process or network boundaries. They are orchestrated by a \ntransaction manager\n. The most common algorithm handling short-lived distributed transactions is \ntwo-phase commit\n. With a two-phase commit, first comes the voting phase: each participant in the distributed transaction tells the transaction manager whether it thinks its local transaction can be completed. If the transaction manager gets a yes vote from everyone, then it tells them all to go ahead and perform their commits. A single no vote is enough for the transaction manager to send out a rollback to all parties. Distributed transactions make scaling systems much more difficult, since the transaction manager is a single point of failure and waiting for response while locking resources can cause outages. Also, there is no guarantee that the transactions are actually committed when the clients approve them.\n\n\n\n\nEach of these solutions adds complexity. Before implementing business operations happening in a transaction, ask yourself: can they happen in different, local transactions, and rely on the concept of eventual consistency? These systems are much easier to build and scale.\n\n\nIf you do encounter state that really needs to be kept consistent, try to avoid splitting it. If you really need to split it, try moving from a purely technical view of the process (e.g., a database transaction) and actually create a concrete concept to represent the transaction. This gives you a hook on which to run other operations like compensating transactions, and a way to monitor and manage these more complex concepts in your system.\n\n\nReporting\n\n\nWhen splitting data, we'll come across the problem of splitting reporting data too.\n\n\nThe Reporting Database\n\n\nIn monolithic systems, aggregating data for reporting is easy. Usually reporting is implemented like this:\n\n\n\n\nBenefits:\n\n\n\n\nAll data is one place so it's easy to query it.\n\n\n\n\nDownsides:\n\n\n\n\nThe db schema is a shared API between the monolith and the reporting service.\n\n\nCannot optimize schema structure for both use cases. Either the db is optimized for the monolith or the reporting.\n\n\nCannot use different technology that could be more efficient for reporting.\n\n\n\n\nThere are several alternatives to this approach when our data is distributed across different services.\n\n\nData Retrieval via Service Calls\n\n\nA very simple approach: call service APIs and aggregate the results for reporting.\n\n\nBenefits:\n\n\n\n\nEasy to implement and works well for small volumes of data (e.g. #orders placed in the last 15 minutes).\n\n\n\n\nDownsides:\n\n\n\n\nBreaks down when trying to do reporting with large volumes of data (e.g. customer behavior of last 24 months).\n\n\nReporting systems usually need to integrate with third-party tools over SQL-like interfaces, this approach would require extra work.\n\n\nThe API may not have been designed for reporting, leading to an inefficient reporting system and general slowdown. Caching can help, but reporting data is usually historic so there would be a lot of expensive cache misses. Adding reporting-specific APIs can help.\n\n\n\n\nData Pumps\n\n\nRather than have the reporting system pull the data, the data can instead be pushed to the reporting system. This \ndata pump\n needs to have intimate knowledge of both the internal database for the service, and also the reporting schema. The pump\u2019s job is to map one from the other.\n\n\n\n\nBenefits:\n\n\n\n\nCan handle large amounts of data without maintaining a reporting-specific API.\n\n\n\n\nDownsides:\n\n\n\n\nCauses coupling with the reporting db schema. The reporting service must be treated as a published API that is hard to change. There is also a potential mitigation: exposing only specific schemas that are mapped to an underlying monolithic schema, but this can cause performance issues depending on the db technology choice.\n\n\n\n\nEvent Data Pump\n\n\nWe can write a subscriber listening to microservices events that pushes data in the reporting db.\n\n\nBenefits:\n\n\n\n\nAvoids coupling between db schemas.\n\n\nCan see reported data as it happens, opposed to wait for a scheduled data transfer.\n\n\nIt is easier to only process new events (i.e. \ndeltas\n), while with a data pump we would need to write the code ourselves.\n\n\nThe event mapper can be managed by a different team, and it can evolve independently of the services.\n\n\n\n\nDownsides:\n\n\n\n\nAll information must be broadcast as event. It may not scale well with large volumes of data, for which a data pump is more efficient.\n\n\n\n\nBackup data pump\n\n\nUsing backup data as a source for reporting. This approach was taken by Netflix: backed up Cassandra tables would be stored in Amazon's S3 object store and accessed by Hadoop for reporting. This ended up as a tool named \nAegisthus\n.\n\n\nBenefits:\n\n\n\n\nCan handle enormous amounts of data.\n\n\nEfficient if there is already a backup system in place.\n\n\n\n\nDownsides:\n\n\n\n\nHas coupling with the reporting db schema.\n\n\n\n\nSummary\n\n\nWe decompose our system by finding seams along which service boundaries can emerge, and this can be an incremental approach. This way, costs of errors are mitigated and we can continue to evolve the system as we proceed.", 
            "title": "Splitting the monolith"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#splitting-the-monolith", 
            "text": "Why would you want to split a monolith?   There are lots of changes coming to a part of the monolith, splitting that part into a service will make you roll out those changes faster.  Separate teams work on separate parts of the monolith.  A part of the monolith requires high security measures not needed by the rest of the system.  A part of the monolith can be improved by switching technology.   How do we go about decomposing monolithic applications without having to embark on a big-bang rewrite?", 
            "title": "Splitting the monolith"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#seams", 
            "text": "We want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both.  A seam is a portion of the code that can be treated in isolation and worked on without impacting the rest of the codebase. Bounded contexts are good seams.  So when splitting, the first step is to identify seams in our system and then gradually move the code of these seams into different packages. Tests are really useful to make sure you're not introducing bugs with this packaging. This process will also help identify seams that you did not think of: they will come out when you are left with some code that you don't know in which package to place.  The splitting should start from the seam that is least depended on.", 
            "title": "Seams"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#databases", 
            "text": "We have to find seams in databases too, but this is a difficult task.  After having packaged your application code by seams, you should do the same for the code accessing the database (usually the code in the so called  repository layer ).", 
            "title": "Databases"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#foreign-keys", 
            "text": "Some tables may have foreign keys linking them to other tables. A common solution for this problem is to remove the table relationship and make the service accessing that table call the API of the service handling the other table.", 
            "title": "Foreign keys"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#shared-static-data", 
            "text": "Let's suppose we have different services accessing a table filled with static data.   There are several solutions:   Duplicate tables in each db, but this can cause consistency issues.  Treat static data as code/configuration files in each service. This can cause consistency issues too, but they would be far easier to solve.  Create a microservice to handle the static data. This is overkill in most situations, but it can be justified if the static data has high complexity.", 
            "title": "Shared static data"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#shared-mutable-data", 
            "text": "Let's suppose we have different services accessing a table filled with mutable data.   Usually this means we need a  Customer  microservice to handle that data. This service can then be called by  Warehouse  and  Finance .", 
            "title": "Shared mutable data"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#shared-tables", 
            "text": "Let's suppose we have different services accessing a table which aggregates different information in the same record (catalog entry and stock level).   The answer here is to split the table in two, creating a stock levels table for the  Warehouse  and a catalog entry table for the  Catalog .", 
            "title": "Shared tables"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#staging-the-break", 
            "text": "The best way to commit the database changes would be to keep the services together and split the schemas. The db split will increase the number of db calls and make you lose transactional integrity. Having the same application will enable you to deal more easily with these problems. Then, when you are satisfied with the new db, you can commit the changes.", 
            "title": "Staging the break"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#transactional-boundaries", 
            "text": "Transactions allow us to say that operations either all happen together, or none of them happen.  Transactions are typically used in databases, but they can be supported but other systems such as message brokers.  Splitting schemas will cause the loss of transactional integrity in our system. There are several solutions to this problem:   A  try again later  mechanism, but this alone is not sufficient since it assumes that eventually a failed request will be successful. This is a form of  eventual consistency : rather than using a transactional boundary to ensure that the system is in a consistent state when the transaction completes, instead we accept that the system will get itself into a consistent state at some point in the future.  Compensating transactions  can be used to undo the committed transactions preceding a failed operation. But what if a compensating transaction fails? We would need other mechanism such as automated jobs or human administration. Also, this mechanism becomes more difficult to manage as the number of operations increases in transactions.  Distributed transactions  are transactions done across different process or network boundaries. They are orchestrated by a  transaction manager . The most common algorithm handling short-lived distributed transactions is  two-phase commit . With a two-phase commit, first comes the voting phase: each participant in the distributed transaction tells the transaction manager whether it thinks its local transaction can be completed. If the transaction manager gets a yes vote from everyone, then it tells them all to go ahead and perform their commits. A single no vote is enough for the transaction manager to send out a rollback to all parties. Distributed transactions make scaling systems much more difficult, since the transaction manager is a single point of failure and waiting for response while locking resources can cause outages. Also, there is no guarantee that the transactions are actually committed when the clients approve them.   Each of these solutions adds complexity. Before implementing business operations happening in a transaction, ask yourself: can they happen in different, local transactions, and rely on the concept of eventual consistency? These systems are much easier to build and scale.  If you do encounter state that really needs to be kept consistent, try to avoid splitting it. If you really need to split it, try moving from a purely technical view of the process (e.g., a database transaction) and actually create a concrete concept to represent the transaction. This gives you a hook on which to run other operations like compensating transactions, and a way to monitor and manage these more complex concepts in your system.", 
            "title": "Transactional Boundaries"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#reporting", 
            "text": "When splitting data, we'll come across the problem of splitting reporting data too.", 
            "title": "Reporting"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#the-reporting-database", 
            "text": "In monolithic systems, aggregating data for reporting is easy. Usually reporting is implemented like this:   Benefits:   All data is one place so it's easy to query it.   Downsides:   The db schema is a shared API between the monolith and the reporting service.  Cannot optimize schema structure for both use cases. Either the db is optimized for the monolith or the reporting.  Cannot use different technology that could be more efficient for reporting.   There are several alternatives to this approach when our data is distributed across different services.", 
            "title": "The Reporting Database"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#data-retrieval-via-service-calls", 
            "text": "A very simple approach: call service APIs and aggregate the results for reporting.  Benefits:   Easy to implement and works well for small volumes of data (e.g. #orders placed in the last 15 minutes).   Downsides:   Breaks down when trying to do reporting with large volumes of data (e.g. customer behavior of last 24 months).  Reporting systems usually need to integrate with third-party tools over SQL-like interfaces, this approach would require extra work.  The API may not have been designed for reporting, leading to an inefficient reporting system and general slowdown. Caching can help, but reporting data is usually historic so there would be a lot of expensive cache misses. Adding reporting-specific APIs can help.", 
            "title": "Data Retrieval via Service Calls"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#data-pumps", 
            "text": "Rather than have the reporting system pull the data, the data can instead be pushed to the reporting system. This  data pump  needs to have intimate knowledge of both the internal database for the service, and also the reporting schema. The pump\u2019s job is to map one from the other.   Benefits:   Can handle large amounts of data without maintaining a reporting-specific API.   Downsides:   Causes coupling with the reporting db schema. The reporting service must be treated as a published API that is hard to change. There is also a potential mitigation: exposing only specific schemas that are mapped to an underlying monolithic schema, but this can cause performance issues depending on the db technology choice.", 
            "title": "Data Pumps"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#event-data-pump", 
            "text": "We can write a subscriber listening to microservices events that pushes data in the reporting db.  Benefits:   Avoids coupling between db schemas.  Can see reported data as it happens, opposed to wait for a scheduled data transfer.  It is easier to only process new events (i.e.  deltas ), while with a data pump we would need to write the code ourselves.  The event mapper can be managed by a different team, and it can evolve independently of the services.   Downsides:   All information must be broadcast as event. It may not scale well with large volumes of data, for which a data pump is more efficient.", 
            "title": "Event Data Pump"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#backup-data-pump", 
            "text": "Using backup data as a source for reporting. This approach was taken by Netflix: backed up Cassandra tables would be stored in Amazon's S3 object store and accessed by Hadoop for reporting. This ended up as a tool named  Aegisthus .  Benefits:   Can handle enormous amounts of data.  Efficient if there is already a backup system in place.   Downsides:   Has coupling with the reporting db schema.", 
            "title": "Backup data pump"
        }, 
        {
            "location": "/building-microservices/splitting-the-monolith/#summary", 
            "text": "We decompose our system by finding seams along which service boundaries can emerge, and this can be an incremental approach. This way, costs of errors are mitigated and we can continue to evolve the system as we proceed.", 
            "title": "Summary"
        }, 
        {
            "location": "/building-microservices/deployment/", 
            "text": "Deployment\n\n\nDeployment in microservices differs from monolithic systems. It's important to have a working approach following the continuous integration and delivery practices.\n\n\nMapping Continuous Integration to Microservices\n\n\nThe goal is to be able to deploy microservices independently.\nSo, how to map microservices to builds and code repositories? We have different options:\n\n\nSingle repository and single build\n\n\nUse a single repository to store all our code, and have a single build, triggered on every code integration, that produces every build artifact we need.\n\n\nBenefits:\n\n\n\n\nEasy to implement.\n\n\nEasy to commit changes.\n\n\n\n\nDownsides:\n\n\n\n\nA small change to a single service will trigger builds we do not need.\n\n\nWhat services do we need to deploy? Hard to determine which services changed by only looking at the pushed commit.\n\n\nIf a commit breaks the build, the build needs to be fixed before any other team can push code in the repository, locking those teams.\n\n\n\n\nSingle repository and multiple builds\n\n\nA variation of the previous approach is to have a single repository but setup multiple CI builds mapping to parts of the source code.\n\n\nBenefits:\n\n\n\n\nEasy to commit changes.\n\n\n\n\nDownsides:\n\n\n\n\nDevelopers can get into the habit of making changes to different services in the same commit.\n\n\n\n\nMultiple repositories and multiple builds\n\n\nEach microservice has its own repository and CI build.\n\n\nBenefits:\n\n\n\n\nOnly needed builds and tests are run when the build is triggered.\n\n\nA team can own the repository that it's working on.\n\n\n\n\nDownsides:\n\n\n\n\nMaking changes across microservices is more difficult, but this is still preferable to the single repository approach.\n\n\n\n\nBuild Pipelines and Continuous Delivery\n\n\nIn build processes usually there are a lot of fast, small-scoped tests, and a few large-scoped, slow tests. We will not get fast feedback when our fast tests fail if we're waiting for the other tests to finish. Also, if the fast tests fail, there is no need to run other tests.\n\n\nA solution to this problem is to have different stages in our build, i.e. a \nbuild pipeline\n.\nBuild pipelines allow to track the software as it goes through each build stage, giving a clear idea of its stability.\n\n\nIn \ncontinuous delivery\n (\nCD\n) we get constant feedback on the production readiness of each and every check-in, and treat each and every check-in as a release candidate. So clearly \nCD\n benefits from build pipelines.\nIn microservices with CI, we want one pipeline per service, in which a single artifact will move through our path to production.\n\n\nExceptions to Continuous Delivery\n\n\nIn the starting stage of a project, a single repository and single build approach may be more convenient since developers are not confident with the domain yet because the service boundaries are likely to change a lot. In this case, having a multi repository model will increase a lot the cost of these changes.\nThen, when the development team acquires experience in the domain, it can start moving out services in their own repositories and build pipelines.\n\n\nPlatform-Specific Artifacts\n\n\nSome artifacts are platform-specific (e.g. JAR files). This means that they need a specific configuration and a specific platform to be run in an environment.\nTools like \nPuppet\n and \nChef\n can help to automate this process.\n\n\nOperating System Artifacts\n\n\nAnother option for platform-specific artifacts is to use os-specific artifacts. This way, the OS can manage dependencies, installation and removal of your software.\n\n\nThe downside is in actually creating these packages, because the difficulty depends on the target OS (e.g. teams using Windows, not known for package management capabilities, may be unhappy with this approach).\n\n\nAnother downside is that if you need to deploy on different OS, there is an increase in complexity in your build and test process.\n\n\nCustom images\n\n\nThe problem with tools like Puppet and Chef is that \nthey take time to provision a machine\n. They need to install platforms (e.g. JVM) or perform expensive checks on the system to detect if a valid platform version is already installed.\n\n\nAnd if we're using an on-demand compute platform we might be constantly shutting down and spinning up new instances frequently, making the time cost of these tools really high.\n\n\nIf you need to install the same tools multiple times per day (e.g. because of CI) this becomes a real problem in terms of providing fast feedback. It can also lead to increased downtime when deploying in production if your systems do not allow zero-downtime deployment (\nblue/green deployment\n can help mitigate this issue).\n\n\nOne approach to \nreducing the provisioning time\n is to create a virtual machine image that bakes in some common dependencies we use. When we want to deploy our software, we spin up an instance of this custom image, and all we have to do is install the latest version of our service.\n\n\nWhen you launch new copies of this image you don't need to spend time installing your dependencies, as they are already there. This can result in significant time savings.\n\n\nThere are drawbacks too:\n\n\n\n\nBuild times are increased.\n\n\nResulting images can be very large, making it hard to move them across the network.\n\n\nThe image build process differs from platform to platform (e.g. VMWare images, Vagrant images).\n  Tools like \nPacker\n can help.\n\n\n\n\nAs we'll see later, container technology mitigates these drawbacks.\n\n\nImages as Artifacts\n\n\nWhy stop at including only dependencies in these images? We can also include our software in it.\nThis will make our software platform agnostic and it is a good way to start implementing the \nimmutable server\n deployment concept.\n\n\nImmutable Servers\n\n\nTo keep our servers immutable we also must be sure that no one is able to access them after they've been deployed (e.g. by disabling \nSSH\n in the image artifact).\nOtherwise, the configuration could be edited, causing a \nconfiguration drift\n.\nIf we want to have environments that are easy to reason about, every configuration change must pass through a build pipeline.\n\n\nEnvironments\n\n\nOur microservice artifact will move in different environments during the CD pipeline.\nUsually these are:\n\n\n\n\nSlow tests environment.\n\n\nUAT environment.\n\n\nPerformance/load test environment.\n\n\nProduction environment.\n\n\n\n\nAs you go on in the pipeline, you want the environments to look more like the production environment, allowing to catch production problems before they happen in production.\nBut consider that production environments are more expensive and slower to set up. So you should balance the ability to find production-like bugs with the ability to get fast feedback from builds.\n\n\nService configuration\n\n\nOur services need some configuration (e.g. db username and password). Ideally this should be a small amount of data. Also, it's best to minimize configuration that changes between environments, so that you minimize chances for environment-specific bugs.\nBut how to handle this kind of configuration?\n\n\n\n\nBundling the configuration in your build artifacts is to be avoided because it violates the principles of \nCD\n.\n  In this case it would be hard to avoid having sensitive data (e.g. passwords) in your source code.\n  Also, build times are increased since you now have more images.\n  Then you have to know at build time which environments exist, coupling the build process with the delivery process.\n\n\nCreate a single artifact and place configuration files in environments or use a dedicated system for providing configuration (a popular approach in microservices).\n\n\n\n\nService-to-Host Mapping\n\n\nIn this era of virtualization, the mapping between a single host running an operating system and the underlying physical infrastructure can vary a lot.\n\n\nLet's define \nhost\n to be the generic unit of isolation, i.e. an operating system onto which you can install and run your services.\n\n\nSo how many services per host should we have? There are different options.\n\n\nMultiple Services Per Host\n\n\nHaving multiple instances of your service per host.\n\n\n\n\nBenefits:\n\n\n\n\nSimpler work for the team that manages the infrastructure.\n\n\nUsing host virtualization can add overhead and thus increase costs.\n\n\nEasier for developers to deploy: a deploy with this setup works like a deploy to a dev machine.\n\n\n\n\nDownsides:\n\n\n\n\nMake monitoring more difficult (e.g. monitor the host CPU usage or each instance?).\n\n\nCauses side effects (e.g. when a service is under heavy load, it's likely some other service instances will slow down too).\n\n\nNeed to ensure that a service deployment does not affect other services on the same host.\n  Usually this is solved by deploying all service in one step, thus losing ability to deploy independently.\n\n\nAutonomy of teams is inhibited in case services of different teams are deployed to the same host.\n\n\nCannot deploy images and immutable servers.\n\n\nIt can be complicated to target scaling at a service in a host.\n\n\nIf a service handles sensitive data or has different needs (e.g. another network segment), you cannot deploy it with the others.\n\n\n\n\nApplication Containers\n\n\nUse an application container (e.g. IIS or Java servlet container) that provides utilities such as management, monitoring and scaling of services.\n\n\n\n\nBenefits:\n\n\n\n\nHas too for managing monitoring, scaling and other aspects.\n\n\nIf all services require the same runtime, this approach reduces overhead (e.g. for \nn\n Java services only a single JVM instance is needed).\n\n\n\n\nDownsides:\n\n\n\n\nTechnology choice and tools that automate services management are constrained. Losing automation here means having to do a lot of work in managing services.\n\n\nUsually slow spin-up times, slowing feedback for developers.\n\n\nAnalyzing resources use is hard, as you have multiple applications sharing a single process.\n\n\nApplication containers have their own resource consumption overhead.\n\n\n\n\nSingle Service per Host\n\n\nA host contains only a single service.\n\n\n\n\nBenefits:\n\n\n\n\nEasier to monitor resources usage.\n\n\nEasier to avoid the side effects of having multiple services in a single host.\n\n\nReduces complexity of your system.\n\n\n\n\nDownsides:\n\n\n\n\nMore hosts mean more servers to manage and costs might increase.\n\n\n\n\nYou can mitigate the complexity of managing more hosts by using a \nplatform as a service\n (PaaS). This way, the host management problem is simplified, but you lose control over your hosts.\n\n\nTip: some PaaS try to automate too much (e.g. automate scaling), making them less effective for your specific use case.\n\n\nAutomation\n\n\nAutomation is the solution to many of the problems we have raised so far.\n\n\nOne of the pushbacks for switching to single service per host is the perception that the amount of overhead for management will increase. If you do everything manually, it surely will, but automation will prevent this issue.\n\n\nAutomation also allow developers to be productive, especially if they have access to the same technologies used in production because it will help catch bugs early on.\n\n\nEmbracing a culture of automation is key if you want to keep the complexities of microservice architectures in check.\n\n\nFrom Physical to Virtual\n\n\nOne of the key tools available to us in managing a large number of hosts is finding ways\nof chunking up existing physical machines into smaller parts.\n\n\nTraditional Virtualization\n\n\nHaving lots of hosts can be really expensive if you need a physical server per host. By virtualizing you can split a physical machine in separate parts but of course this comes with an overhead.\n\n\nFor example, in \nType 2\n virtualization, the \nhypervisor\n sets aside resources for each virtual machine it manages, but these resources could be used for something else instead of being idle and reserved.\n\n\n\n\nVagrant\n\n\nA deployment platform usually employed for development and testing. It allows to define instructions about how to setup and configure VMs. This makes it easier for you to create production-like environments on your local machine.\n\n\nOne of the downsides is that if we have one service to one VM, you may not be able to bring up\nyour entire system on your local machine.\n\n\nLinux containers\n\n\nLinux containers, instead of using an hypervisor, create a separate process space in which other processes live.\n\n\n\n\nEach container is effectively a subtree of the overall system process tree. These containers can have physical resources allocated to them, something the kernel handles for us.\n\n\nBenefits:\n\n\n\n\nNo need for an hypervisor.\n\n\nMuch faster to provision than traditional VMs.\n\n\nFiner-grained control over resources assignation.\n\n\nSince they are lighter than VMs, we can have more containers running on the same host.\n\n\n\n\nDownsides:\n\n\n\n\nThe host OS has to share the same kernel with the base OS.\n\n\nNot as isolated from other processes as VMs, not suitable for running code you don't trust.\n\n\nHow to expose containers to the outer world? A specific network configuration is needed, something that is usually provided by hypervisors.\n\n\n\n\nDocker\n\n\nDocker is a platform built on top of lightweight containers. Docker manages the container provisioning, handles some of the networking problems and provides its own registry that allows you to store and version Docker applications.\n\n\nDocker can also alleviate some of the downsides of running lots of services locally for dev and test purposes, in a more efficient way than Vagrant.\n\n\nSeveral technologies are build around the Docker concepts, such as \nCoreOS\n, a stripped-down Linux OS that provides only the essential services to allow Docker to run.\n\n\nDocker itself doesn\u2019t solve all problems for us. Think of it as a simple PaaS that works on a single machine. If you want tools to help you manage services across multiple Docker instances across multiple machines, you\u2019ll need to look at software such as \nKubernetes\n or CoreOS.\n\n\nA Deployment Interface\n\n\nWhatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital to easily deploy microservices to development, test, production and other environments.\n\n\nA good way to trigger deployments is via CLI tools, because it can be triggered by other scripts, used in CI and called manually.\n\n\nWe need some information for a deploy:\n\n\n\n\nWhat microservice we want to deploy.\n\n\nWhat version of said microservice we want to deploy.\n\n\nWhat environment we want our microservice deployed into.\n\n\n\n\nFor this to work, we need to define in some way what our environments look like. YAML could be a good way of expressing our environments definitions.\n\n\nSummary\n\n\nMain points collected in this chapter:\n\n\n\n\nMaintain the ability to deploy microservices independently.\n\n\nSeparate source code and CI builds for each microservices.\n\n\nUse a single-service per host/container model. Evaluate the tooling aiming for high levels of automation.\n\n\nUnderstand how deployment choices affects developers. Creating tools that help deploying to different environments helps a lot.", 
            "title": "Deployment"
        }, 
        {
            "location": "/building-microservices/deployment/#deployment", 
            "text": "Deployment in microservices differs from monolithic systems. It's important to have a working approach following the continuous integration and delivery practices.", 
            "title": "Deployment"
        }, 
        {
            "location": "/building-microservices/deployment/#mapping-continuous-integration-to-microservices", 
            "text": "The goal is to be able to deploy microservices independently.\nSo, how to map microservices to builds and code repositories? We have different options:", 
            "title": "Mapping Continuous Integration to Microservices"
        }, 
        {
            "location": "/building-microservices/deployment/#single-repository-and-single-build", 
            "text": "Use a single repository to store all our code, and have a single build, triggered on every code integration, that produces every build artifact we need.  Benefits:   Easy to implement.  Easy to commit changes.   Downsides:   A small change to a single service will trigger builds we do not need.  What services do we need to deploy? Hard to determine which services changed by only looking at the pushed commit.  If a commit breaks the build, the build needs to be fixed before any other team can push code in the repository, locking those teams.", 
            "title": "Single repository and single build"
        }, 
        {
            "location": "/building-microservices/deployment/#single-repository-and-multiple-builds", 
            "text": "A variation of the previous approach is to have a single repository but setup multiple CI builds mapping to parts of the source code.  Benefits:   Easy to commit changes.   Downsides:   Developers can get into the habit of making changes to different services in the same commit.", 
            "title": "Single repository and multiple builds"
        }, 
        {
            "location": "/building-microservices/deployment/#multiple-repositories-and-multiple-builds", 
            "text": "Each microservice has its own repository and CI build.  Benefits:   Only needed builds and tests are run when the build is triggered.  A team can own the repository that it's working on.   Downsides:   Making changes across microservices is more difficult, but this is still preferable to the single repository approach.", 
            "title": "Multiple repositories and multiple builds"
        }, 
        {
            "location": "/building-microservices/deployment/#build-pipelines-and-continuous-delivery", 
            "text": "In build processes usually there are a lot of fast, small-scoped tests, and a few large-scoped, slow tests. We will not get fast feedback when our fast tests fail if we're waiting for the other tests to finish. Also, if the fast tests fail, there is no need to run other tests.  A solution to this problem is to have different stages in our build, i.e. a  build pipeline .\nBuild pipelines allow to track the software as it goes through each build stage, giving a clear idea of its stability.  In  continuous delivery  ( CD ) we get constant feedback on the production readiness of each and every check-in, and treat each and every check-in as a release candidate. So clearly  CD  benefits from build pipelines.\nIn microservices with CI, we want one pipeline per service, in which a single artifact will move through our path to production.", 
            "title": "Build Pipelines and Continuous Delivery"
        }, 
        {
            "location": "/building-microservices/deployment/#exceptions-to-continuous-delivery", 
            "text": "In the starting stage of a project, a single repository and single build approach may be more convenient since developers are not confident with the domain yet because the service boundaries are likely to change a lot. In this case, having a multi repository model will increase a lot the cost of these changes.\nThen, when the development team acquires experience in the domain, it can start moving out services in their own repositories and build pipelines.", 
            "title": "Exceptions to Continuous Delivery"
        }, 
        {
            "location": "/building-microservices/deployment/#platform-specific-artifacts", 
            "text": "Some artifacts are platform-specific (e.g. JAR files). This means that they need a specific configuration and a specific platform to be run in an environment.\nTools like  Puppet  and  Chef  can help to automate this process.", 
            "title": "Platform-Specific Artifacts"
        }, 
        {
            "location": "/building-microservices/deployment/#operating-system-artifacts", 
            "text": "Another option for platform-specific artifacts is to use os-specific artifacts. This way, the OS can manage dependencies, installation and removal of your software.  The downside is in actually creating these packages, because the difficulty depends on the target OS (e.g. teams using Windows, not known for package management capabilities, may be unhappy with this approach).  Another downside is that if you need to deploy on different OS, there is an increase in complexity in your build and test process.", 
            "title": "Operating System Artifacts"
        }, 
        {
            "location": "/building-microservices/deployment/#custom-images", 
            "text": "The problem with tools like Puppet and Chef is that  they take time to provision a machine . They need to install platforms (e.g. JVM) or perform expensive checks on the system to detect if a valid platform version is already installed.  And if we're using an on-demand compute platform we might be constantly shutting down and spinning up new instances frequently, making the time cost of these tools really high.  If you need to install the same tools multiple times per day (e.g. because of CI) this becomes a real problem in terms of providing fast feedback. It can also lead to increased downtime when deploying in production if your systems do not allow zero-downtime deployment ( blue/green deployment  can help mitigate this issue).  One approach to  reducing the provisioning time  is to create a virtual machine image that bakes in some common dependencies we use. When we want to deploy our software, we spin up an instance of this custom image, and all we have to do is install the latest version of our service.  When you launch new copies of this image you don't need to spend time installing your dependencies, as they are already there. This can result in significant time savings.  There are drawbacks too:   Build times are increased.  Resulting images can be very large, making it hard to move them across the network.  The image build process differs from platform to platform (e.g. VMWare images, Vagrant images).\n  Tools like  Packer  can help.   As we'll see later, container technology mitigates these drawbacks.", 
            "title": "Custom images"
        }, 
        {
            "location": "/building-microservices/deployment/#images-as-artifacts", 
            "text": "Why stop at including only dependencies in these images? We can also include our software in it.\nThis will make our software platform agnostic and it is a good way to start implementing the  immutable server  deployment concept.", 
            "title": "Images as Artifacts"
        }, 
        {
            "location": "/building-microservices/deployment/#immutable-servers", 
            "text": "To keep our servers immutable we also must be sure that no one is able to access them after they've been deployed (e.g. by disabling  SSH  in the image artifact).\nOtherwise, the configuration could be edited, causing a  configuration drift .\nIf we want to have environments that are easy to reason about, every configuration change must pass through a build pipeline.", 
            "title": "Immutable Servers"
        }, 
        {
            "location": "/building-microservices/deployment/#environments", 
            "text": "Our microservice artifact will move in different environments during the CD pipeline.\nUsually these are:   Slow tests environment.  UAT environment.  Performance/load test environment.  Production environment.   As you go on in the pipeline, you want the environments to look more like the production environment, allowing to catch production problems before they happen in production.\nBut consider that production environments are more expensive and slower to set up. So you should balance the ability to find production-like bugs with the ability to get fast feedback from builds.", 
            "title": "Environments"
        }, 
        {
            "location": "/building-microservices/deployment/#service-configuration", 
            "text": "Our services need some configuration (e.g. db username and password). Ideally this should be a small amount of data. Also, it's best to minimize configuration that changes between environments, so that you minimize chances for environment-specific bugs.\nBut how to handle this kind of configuration?   Bundling the configuration in your build artifacts is to be avoided because it violates the principles of  CD .\n  In this case it would be hard to avoid having sensitive data (e.g. passwords) in your source code.\n  Also, build times are increased since you now have more images.\n  Then you have to know at build time which environments exist, coupling the build process with the delivery process.  Create a single artifact and place configuration files in environments or use a dedicated system for providing configuration (a popular approach in microservices).", 
            "title": "Service configuration"
        }, 
        {
            "location": "/building-microservices/deployment/#service-to-host-mapping", 
            "text": "In this era of virtualization, the mapping between a single host running an operating system and the underlying physical infrastructure can vary a lot.  Let's define  host  to be the generic unit of isolation, i.e. an operating system onto which you can install and run your services.  So how many services per host should we have? There are different options.", 
            "title": "Service-to-Host Mapping"
        }, 
        {
            "location": "/building-microservices/deployment/#multiple-services-per-host", 
            "text": "Having multiple instances of your service per host.   Benefits:   Simpler work for the team that manages the infrastructure.  Using host virtualization can add overhead and thus increase costs.  Easier for developers to deploy: a deploy with this setup works like a deploy to a dev machine.   Downsides:   Make monitoring more difficult (e.g. monitor the host CPU usage or each instance?).  Causes side effects (e.g. when a service is under heavy load, it's likely some other service instances will slow down too).  Need to ensure that a service deployment does not affect other services on the same host.\n  Usually this is solved by deploying all service in one step, thus losing ability to deploy independently.  Autonomy of teams is inhibited in case services of different teams are deployed to the same host.  Cannot deploy images and immutable servers.  It can be complicated to target scaling at a service in a host.  If a service handles sensitive data or has different needs (e.g. another network segment), you cannot deploy it with the others.", 
            "title": "Multiple Services Per Host"
        }, 
        {
            "location": "/building-microservices/deployment/#application-containers", 
            "text": "Use an application container (e.g. IIS or Java servlet container) that provides utilities such as management, monitoring and scaling of services.   Benefits:   Has too for managing monitoring, scaling and other aspects.  If all services require the same runtime, this approach reduces overhead (e.g. for  n  Java services only a single JVM instance is needed).   Downsides:   Technology choice and tools that automate services management are constrained. Losing automation here means having to do a lot of work in managing services.  Usually slow spin-up times, slowing feedback for developers.  Analyzing resources use is hard, as you have multiple applications sharing a single process.  Application containers have their own resource consumption overhead.", 
            "title": "Application Containers"
        }, 
        {
            "location": "/building-microservices/deployment/#single-service-per-host", 
            "text": "A host contains only a single service.   Benefits:   Easier to monitor resources usage.  Easier to avoid the side effects of having multiple services in a single host.  Reduces complexity of your system.   Downsides:   More hosts mean more servers to manage and costs might increase.   You can mitigate the complexity of managing more hosts by using a  platform as a service  (PaaS). This way, the host management problem is simplified, but you lose control over your hosts.  Tip: some PaaS try to automate too much (e.g. automate scaling), making them less effective for your specific use case.", 
            "title": "Single Service per Host"
        }, 
        {
            "location": "/building-microservices/deployment/#automation", 
            "text": "Automation is the solution to many of the problems we have raised so far.  One of the pushbacks for switching to single service per host is the perception that the amount of overhead for management will increase. If you do everything manually, it surely will, but automation will prevent this issue.  Automation also allow developers to be productive, especially if they have access to the same technologies used in production because it will help catch bugs early on.  Embracing a culture of automation is key if you want to keep the complexities of microservice architectures in check.", 
            "title": "Automation"
        }, 
        {
            "location": "/building-microservices/deployment/#from-physical-to-virtual", 
            "text": "One of the key tools available to us in managing a large number of hosts is finding ways\nof chunking up existing physical machines into smaller parts.", 
            "title": "From Physical to Virtual"
        }, 
        {
            "location": "/building-microservices/deployment/#traditional-virtualization", 
            "text": "Having lots of hosts can be really expensive if you need a physical server per host. By virtualizing you can split a physical machine in separate parts but of course this comes with an overhead.  For example, in  Type 2  virtualization, the  hypervisor  sets aside resources for each virtual machine it manages, but these resources could be used for something else instead of being idle and reserved.", 
            "title": "Traditional Virtualization"
        }, 
        {
            "location": "/building-microservices/deployment/#vagrant", 
            "text": "A deployment platform usually employed for development and testing. It allows to define instructions about how to setup and configure VMs. This makes it easier for you to create production-like environments on your local machine.  One of the downsides is that if we have one service to one VM, you may not be able to bring up\nyour entire system on your local machine.", 
            "title": "Vagrant"
        }, 
        {
            "location": "/building-microservices/deployment/#linux-containers", 
            "text": "Linux containers, instead of using an hypervisor, create a separate process space in which other processes live.   Each container is effectively a subtree of the overall system process tree. These containers can have physical resources allocated to them, something the kernel handles for us.  Benefits:   No need for an hypervisor.  Much faster to provision than traditional VMs.  Finer-grained control over resources assignation.  Since they are lighter than VMs, we can have more containers running on the same host.   Downsides:   The host OS has to share the same kernel with the base OS.  Not as isolated from other processes as VMs, not suitable for running code you don't trust.  How to expose containers to the outer world? A specific network configuration is needed, something that is usually provided by hypervisors.", 
            "title": "Linux containers"
        }, 
        {
            "location": "/building-microservices/deployment/#docker", 
            "text": "Docker is a platform built on top of lightweight containers. Docker manages the container provisioning, handles some of the networking problems and provides its own registry that allows you to store and version Docker applications.  Docker can also alleviate some of the downsides of running lots of services locally for dev and test purposes, in a more efficient way than Vagrant.  Several technologies are build around the Docker concepts, such as  CoreOS , a stripped-down Linux OS that provides only the essential services to allow Docker to run.  Docker itself doesn\u2019t solve all problems for us. Think of it as a simple PaaS that works on a single machine. If you want tools to help you manage services across multiple Docker instances across multiple machines, you\u2019ll need to look at software such as  Kubernetes  or CoreOS.", 
            "title": "Docker"
        }, 
        {
            "location": "/building-microservices/deployment/#a-deployment-interface", 
            "text": "Whatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital to easily deploy microservices to development, test, production and other environments.  A good way to trigger deployments is via CLI tools, because it can be triggered by other scripts, used in CI and called manually.  We need some information for a deploy:   What microservice we want to deploy.  What version of said microservice we want to deploy.  What environment we want our microservice deployed into.   For this to work, we need to define in some way what our environments look like. YAML could be a good way of expressing our environments definitions.", 
            "title": "A Deployment Interface"
        }, 
        {
            "location": "/building-microservices/deployment/#summary", 
            "text": "Main points collected in this chapter:   Maintain the ability to deploy microservices independently.  Separate source code and CI builds for each microservices.  Use a single-service per host/container model. Evaluate the tooling aiming for high levels of automation.  Understand how deployment choices affects developers. Creating tools that help deploying to different environments helps a lot.", 
            "title": "Summary"
        }, 
        {
            "location": "/building-microservices/testing/", 
            "text": "Testing\n\n\nDistributed systems add complexity in automated tests too.\n\n\nTypes of Tests\n\n\nTests can be categorized by the following diagram:\n\n\n\n\nIn microservices, the amount of manual tests should be kept at a minimum in order to reduce test times. Also, since there are no significant difference in manual testing, we will examine how automated testing changes from monolithic systems to microservices systems.\n\n\nTest scope\n\n\nThe Test Pyramid is a model proposed by Mike Cohn to associate the ideal amount of tests to each test scope.\n\n\n\n\nTerms like \nservice\n and \nunit\n in this context are ambiguous and we will refer to the \nUI\n layer as \nend-to-end\n tests.\n\n\nTo better explain what each layer represents, we introduce the following communication scenario:\n\n\n\n\nUnit Tests\n\n\nThey typically test a single function or method call in isolation (i.e. without starting services or using external resources such as network connectivity).\n\n\nDone right, they can be very fast. You could run a lot of them in less than a minute.\n\n\nThese are \ntechnology-facing\n tests that will help us catch the most bugs and guide us through code restructuring thanks to their fast feedback and reliability.\n\n\nService Tests\n\n\nThey are designed to bypass the user interface and test services  directly.\n\n\nIn monolithic systems, a group of classes that provide a certain service to users can be tested together.\n\n\nIn microservices, we need to isolate the service we want to test so that we are able to quickly find the root cause of a bug. To achieve this isolation, we need to stub out other services interacting with the one under test.\n\n\nWIP", 
            "title": "Testing"
        }, 
        {
            "location": "/building-microservices/testing/#testing", 
            "text": "Distributed systems add complexity in automated tests too.", 
            "title": "Testing"
        }, 
        {
            "location": "/building-microservices/testing/#types-of-tests", 
            "text": "Tests can be categorized by the following diagram:   In microservices, the amount of manual tests should be kept at a minimum in order to reduce test times. Also, since there are no significant difference in manual testing, we will examine how automated testing changes from monolithic systems to microservices systems.", 
            "title": "Types of Tests"
        }, 
        {
            "location": "/building-microservices/testing/#test-scope", 
            "text": "The Test Pyramid is a model proposed by Mike Cohn to associate the ideal amount of tests to each test scope.   Terms like  service  and  unit  in this context are ambiguous and we will refer to the  UI  layer as  end-to-end  tests.  To better explain what each layer represents, we introduce the following communication scenario:", 
            "title": "Test scope"
        }, 
        {
            "location": "/building-microservices/testing/#unit-tests", 
            "text": "They typically test a single function or method call in isolation (i.e. without starting services or using external resources such as network connectivity).  Done right, they can be very fast. You could run a lot of them in less than a minute.  These are  technology-facing  tests that will help us catch the most bugs and guide us through code restructuring thanks to their fast feedback and reliability.", 
            "title": "Unit Tests"
        }, 
        {
            "location": "/building-microservices/testing/#service-tests", 
            "text": "They are designed to bypass the user interface and test services  directly.  In monolithic systems, a group of classes that provide a certain service to users can be tested together.  In microservices, we need to isolate the service we want to test so that we are able to quickly find the root cause of a bug. To achieve this isolation, we need to stub out other services interacting with the one under test.", 
            "title": "Service Tests"
        }, 
        {
            "location": "/building-microservices/testing/#wip", 
            "text": "", 
            "title": "WIP"
        }, 
        {
            "location": "/specimen/", 
            "text": "Specimen\n\n\nBody copy\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Cras arcu libero,\nmollis sed massa vel, \nornare viverra ex\n. Mauris a ullamcorper lacus. Nullam\nurna elit, malesuada eget finibus ut, ullamcorper ac tortor. Vestibulum sodales\npulvinar nisl, pharetra aliquet est. Quisque volutpat erat ac nisi accumsan\ntempor.\n\n\nSed suscipit\n, orci non pretium pretium, quam mi gravida metus, vel\nvenenatis justo est condimentum diam. Maecenas non ornare justo. Nam a ipsum\neros. \nNulla aliquam\n orci sit amet nisl posuere malesuada. Proin aliquet\nnulla velit, quis ultricies orci feugiat et. \nUt tincidunt sollicitudin\n\ntincidunt. Aenean ullamcorper sit amet nulla at interdum.\n\n\nHeadings\n\n\nThe 3\nrd\n level\n\n\nThe 4\nth\n level\n\n\nThe 5\nth\n level\n\n\nThe 6\nth\n level\n\n\nHeadings \nwith secondary text\n\n\nThe 3\nrd\n level \nwith secondary text\n\n\nThe 4\nth\n level \nwith secondary text\n\n\nThe 5\nth\n level \nwith secondary text\n\n\nThe 6\nth\n level \nwith secondary text\n\n\nBlockquotes\n\n\n\n\nMorbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum.\n  Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc\n  mauris, a ultricies libero efficitur sed. \nClass aptent\n taciti sociosqu ad\n  litora torquent per conubia nostra, per inceptos himenaeos. Sed molestie\n  imperdiet consectetur.\n\n\n\n\nBlockquote nesting\n\n\n\n\nSed aliquet\n, neque at rutrum mollis, neque nisi tincidunt nibh, vitae\n  faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem\n  \nlibero fermentum\n urna, ut efficitur elit ligula et nunc.\n\n\n\n\nMauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla.\n    Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio.\n    Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum.\n    eu odio.\n\n\n\n\nSuspendisse rutrum facilisis risus\n, eu posuere neque commodo a.\n      Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo\n      bibendum, sodales mauris ut, tincidunt massa.\n\n\n\n\n\n\n\n\nOther content blocks\n\n\n\n\nVestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu\n  lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl,\n  sit amet laoreet nibh.\n  \nvar\n \n_extends\n \n=\n \nfunction\n(\ntarget\n)\n \n{\n\n  \nfor\n \n(\nvar\n \ni\n \n=\n \n1\n;\n \ni\n \n \narguments\n.\nlength\n;\n \ni\n++\n)\n \n{\n\n    \nvar\n \nsource\n \n=\n \narguments\n[\ni\n];\n\n    \nfor\n \n(\nvar\n \nkey\n \nin\n \nsource\n)\n \n{\n\n      \ntarget\n[\nkey\n]\n \n=\n \nsource\n[\nkey\n];\n\n    \n}\n\n  \n}\n\n\n  \nreturn\n \ntarget\n;\n\n\n};\n\n\n\n\n\n\nPraesent at \nreturn\n \ntarget\n, sodales nibh vel, tempor felis. Fusce\n      vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices.\n      Donec consectetur mauris non neque imperdiet, eget volutpat libero.\n\n\n\n\n\n\nLists\n\n\nUnordered lists\n\n\n\n\n\n\nSed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus\n  non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci,\n  at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero.\n\n\n\n\nDuis mollis est eget nibh volutpat, fermentum aliquet dui mollis.\n\n\nNam vulputate tincidunt fringilla.\n\n\nNullam dignissim ultrices urna non auctor.\n\n\n\n\n\n\n\n\nAliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut\n  eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam\n  ac, aliquet sed mauris.\n\n\n\n\n\n\nNulla et rhoncus turpis. Mauris ultricies elementum leo. Duis efficitur\n  accumsan nibh eu mattis. Vivamus tempus velit eros, porttitor placerat nibh\n  lacinia sed. Aenean in finibus diam.\n\n\n\n\n\n\nOrdered lists\n\n\n\n\n\n\nInteger vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis\n  elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla\n  consectetur feugiat sodales.\n\n\n\n\n\n\nCum sociis natoque penatibus et magnis dis parturient montes, nascetur\n  ridiculus mus. Aliquam ornare feugiat quam et egestas. Nunc id erat et quam\n  pellentesque lacinia eu vel odio.\n\n\n\n\n\n\nVivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet\n  quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a\n  ultricies libero efficitur sed.\n\n\n\n\nMauris dictum mi lacus\n\n\nUt sit amet placerat ante\n\n\nSuspendisse ac eros arcu\n\n\n\n\n\n\n\n\nMorbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet\n  rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Sed\n  aliquet, neque at rutrum mollis, neque nisi tincidunt nibh.\n\n\n\n\n\n\nPellentesque eget \nvar\n \n_extends\n ornare tellus, ut gravida mi.\n\nvar\n \n_extends\n \n=\n \nfunction\n(\ntarget\n)\n \n{\n\n\n  \nfor\n \n(\nvar\n \ni\n \n=\n \n1\n;\n \ni\n \n \narguments\n.\nlength\n;\n \ni\n++\n)\n \n{\n\n    \nvar\n \nsource\n \n=\n \narguments\n[\ni\n];\n\n    \nfor\n \n(\nvar\n \nkey\n \nin\n \nsource\n)\n \n{\n\n      \ntarget\n[\nkey\n]\n \n=\n \nsource\n[\nkey\n];\n\n    \n}\n\n  \n}\n\n  \nreturn\n \ntarget\n;\n\n\n};\n\n\n\n\n\n\n\n\n\n\n\n\nVivamus id mi enim. Integer id turpis sapien. Ut condimentum lobortis\n  sagittis. Aliquam purus tellus, faucibus eget urna at, iaculis venenatis\n  nulla. Vivamus a pharetra leo.\n\n\n\n\n\n\nDefinition lists\n\n\n\n\nLorem ipsum dolor sit amet\n\n\n\n\nSed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus\ntellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor\nlobortis orci, at elementum urna sodales vitae. In in vehicula nulla.\n\n\nDuis mollis est eget nibh volutpat, fermentum aliquet dui mollis.\nNam vulputate tincidunt fringilla.\nNullam dignissim ultrices urna non auctor.\n\n\n\n\nCras arcu libero\n\n\n\n\nAliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin\nut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at\naliquam ac, aliquet sed mauris.\n\n\n\n\n\n\nCode blocks\n\n\nInline\n\n\nMorbi eget \ndapibus felis\n. Vivamus \nvenenatis porttitor\n tortor sit amet\nrutrum. Class aptent taciti sociosqu ad litora torquent per conubia nostra,\nper inceptos himenaeos. \nPellentesque aliquet quam enim\n, eu volutpat urna\nrutrum a.\n\n\nNam vehicula nunc \nreturn\n \ntarget\n mauris, a ultricies libero efficitur\nsed. Sed molestie imperdiet consectetur. Vivamus a pharetra leo. Pellentesque\neget ornare tellus, ut gravida mi. Fusce vel lacinia lacus.\n\n\nListing\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nvar\n \n_extends\n \n=\n \nfunction\n(\ntarget\n)\n \n{\n\n  \nfor\n \n(\nvar\n \ni\n \n=\n \n1\n;\n \ni\n \n \narguments\n.\nlength\n;\n \ni\n++\n)\n \n{\n\n    \nvar\n \nsource\n \n=\n \narguments\n[\ni\n];\n\n    \nfor\n \n(\nvar\n \nkey\n \nin\n \nsource\n)\n \n{\n\n      \ntarget\n[\nkey\n]\n \n=\n \nsource\n[\nkey\n];\n\n    \n}\n\n  \n}\n\n\n  \nreturn\n \ntarget\n;\n\n\n};\n\n\n\n\n\n\n\nHorizontal rules\n\n\nAenean in finibus diam. Duis mollis est eget nibh volutpat, fermentum aliquet\ndui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna\nnon auctor.\n\n\n\n\nInteger vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis\nelementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla\nconsectetur feugiat sodales.\n\n\nData tables\n\n\n\n\n\n\n\n\nSollicitudo / Pellentesi\n\n\nconsectetur\n\n\nadipiscing\n\n\nelit\n\n\narcu\n\n\nsed\n\n\n\n\n\n\n\n\n\n\nVivamus a pharetra\n\n\nyes\n\n\nyes\n\n\nyes\n\n\nyes\n\n\nyes\n\n\n\n\n\n\nOrnare viverra ex\n\n\nyes\n\n\nyes\n\n\nyes\n\n\nyes\n\n\nyes\n\n\n\n\n\n\nMauris a ullamcorper\n\n\nyes\n\n\nyes\n\n\npartial\n\n\nyes\n\n\nyes\n\n\n\n\n\n\nNullam urna elit\n\n\nyes\n\n\nyes\n\n\nyes\n\n\nyes\n\n\nyes\n\n\n\n\n\n\nMalesuada eget finibus\n\n\nyes\n\n\nyes\n\n\nyes\n\n\nyes\n\n\nyes\n\n\n\n\n\n\nUllamcorper\n\n\nyes\n\n\nyes\n\n\nyes\n\n\nyes\n\n\nyes\n\n\n\n\n\n\nVestibulum sodales\n\n\nyes\n\n\n-\n\n\nyes\n\n\n-\n\n\nyes\n\n\n\n\n\n\nPulvinar nisl\n\n\nyes\n\n\nyes\n\n\nyes\n\n\n-\n\n\n-\n\n\n\n\n\n\nPharetra aliquet est\n\n\nyes\n\n\nyes\n\n\nyes\n\n\nyes\n\n\nyes\n\n\n\n\n\n\nSed suscipit\n\n\nyes\n\n\nyes\n\n\nyes\n\n\nyes\n\n\nyes\n\n\n\n\n\n\nOrci non pretium\n\n\nyes\n\n\npartial\n\n\n-\n\n\n-\n\n\n-\n\n\n\n\n\n\n\n\nSed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus\nnon sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci,\nat elementum urna sodales vitae. In in vehicula nulla, quis ornare libero.\n\n\n\n\n\n\n\n\nLeft\n\n\nCenter\n\n\nRight\n\n\n\n\n\n\n\n\n\n\nLorem\n\n\ndolor\n\n\namet\n\n\n\n\n\n\nipsum\n\n\nsit\n\n\n\n\n\n\n\n\n\n\nVestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu\nlectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl,\nsit amet laoreet nibh.\n\n\n\n  \n\n    \n\n    \n\n  \n\n  \n\n    \n\n      \nTable\n\n      \nwith colgroups (Pandoc)\n\n    \n\n  \n\n  \n\n    \n\n      \nLorem\n\n      \nipsum dolor sit amet.\n\n    \n\n    \n\n      \nSed sagittis\n\n      \neleifend rutrum. Donec vitae suscipit est.", 
            "title": "Specimen"
        }, 
        {
            "location": "/specimen/#specimen", 
            "text": "", 
            "title": "Specimen"
        }, 
        {
            "location": "/specimen/#body-copy", 
            "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras arcu libero,\nmollis sed massa vel,  ornare viverra ex . Mauris a ullamcorper lacus. Nullam\nurna elit, malesuada eget finibus ut, ullamcorper ac tortor. Vestibulum sodales\npulvinar nisl, pharetra aliquet est. Quisque volutpat erat ac nisi accumsan\ntempor.  Sed suscipit , orci non pretium pretium, quam mi gravida metus, vel\nvenenatis justo est condimentum diam. Maecenas non ornare justo. Nam a ipsum\neros.  Nulla aliquam  orci sit amet nisl posuere malesuada. Proin aliquet\nnulla velit, quis ultricies orci feugiat et.  Ut tincidunt sollicitudin \ntincidunt. Aenean ullamcorper sit amet nulla at interdum.", 
            "title": "Body copy"
        }, 
        {
            "location": "/specimen/#headings", 
            "text": "", 
            "title": "Headings"
        }, 
        {
            "location": "/specimen/#the-3rd-level", 
            "text": "", 
            "title": "The 3rd level"
        }, 
        {
            "location": "/specimen/#the-4th-level", 
            "text": "", 
            "title": "The 4th level"
        }, 
        {
            "location": "/specimen/#the-5th-level", 
            "text": "", 
            "title": "The 5th level"
        }, 
        {
            "location": "/specimen/#the-6th-level", 
            "text": "", 
            "title": "The 6th level"
        }, 
        {
            "location": "/specimen/#headings-with-secondary-text", 
            "text": "", 
            "title": "Headings with secondary text"
        }, 
        {
            "location": "/specimen/#the-3rd-level-with-secondary-text", 
            "text": "", 
            "title": "The 3rd level with secondary text"
        }, 
        {
            "location": "/specimen/#the-4th-level-with-secondary-text", 
            "text": "", 
            "title": "The 4th level with secondary text"
        }, 
        {
            "location": "/specimen/#the-5th-level-with-secondary-text", 
            "text": "", 
            "title": "The 5th level with secondary text"
        }, 
        {
            "location": "/specimen/#the-6th-level-with-secondary-text", 
            "text": "", 
            "title": "The 6th level with secondary text"
        }, 
        {
            "location": "/specimen/#blockquotes", 
            "text": "Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum.\n  Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc\n  mauris, a ultricies libero efficitur sed.  Class aptent  taciti sociosqu ad\n  litora torquent per conubia nostra, per inceptos himenaeos. Sed molestie\n  imperdiet consectetur.", 
            "title": "Blockquotes"
        }, 
        {
            "location": "/specimen/#blockquote-nesting", 
            "text": "Sed aliquet , neque at rutrum mollis, neque nisi tincidunt nibh, vitae\n  faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem\n   libero fermentum  urna, ut efficitur elit ligula et nunc.   Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla.\n    Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio.\n    Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum.\n    eu odio.   Suspendisse rutrum facilisis risus , eu posuere neque commodo a.\n      Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo\n      bibendum, sodales mauris ut, tincidunt massa.", 
            "title": "Blockquote nesting"
        }, 
        {
            "location": "/specimen/#other-content-blocks", 
            "text": "Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu\n  lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl,\n  sit amet laoreet nibh.\n   var   _extends   =   function ( target )   { \n   for   ( var   i   =   1 ;   i     arguments . length ;   i ++ )   { \n     var   source   =   arguments [ i ]; \n     for   ( var   key   in   source )   { \n       target [ key ]   =   source [ key ]; \n     } \n   }     return   target ;  };    Praesent at  return   target , sodales nibh vel, tempor felis. Fusce\n      vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices.\n      Donec consectetur mauris non neque imperdiet, eget volutpat libero.", 
            "title": "Other content blocks"
        }, 
        {
            "location": "/specimen/#lists", 
            "text": "", 
            "title": "Lists"
        }, 
        {
            "location": "/specimen/#unordered-lists", 
            "text": "Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus\n  non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci,\n  at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero.   Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis.  Nam vulputate tincidunt fringilla.  Nullam dignissim ultrices urna non auctor.     Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut\n  eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam\n  ac, aliquet sed mauris.    Nulla et rhoncus turpis. Mauris ultricies elementum leo. Duis efficitur\n  accumsan nibh eu mattis. Vivamus tempus velit eros, porttitor placerat nibh\n  lacinia sed. Aenean in finibus diam.", 
            "title": "Unordered lists"
        }, 
        {
            "location": "/specimen/#ordered-lists", 
            "text": "Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis\n  elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla\n  consectetur feugiat sodales.    Cum sociis natoque penatibus et magnis dis parturient montes, nascetur\n  ridiculus mus. Aliquam ornare feugiat quam et egestas. Nunc id erat et quam\n  pellentesque lacinia eu vel odio.    Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet\n  quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a\n  ultricies libero efficitur sed.   Mauris dictum mi lacus  Ut sit amet placerat ante  Suspendisse ac eros arcu     Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet\n  rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Sed\n  aliquet, neque at rutrum mollis, neque nisi tincidunt nibh.    Pellentesque eget  var   _extends  ornare tellus, ut gravida mi. var   _extends   =   function ( target )   {     for   ( var   i   =   1 ;   i     arguments . length ;   i ++ )   { \n     var   source   =   arguments [ i ]; \n     for   ( var   key   in   source )   { \n       target [ key ]   =   source [ key ]; \n     } \n   } \n   return   target ;  };       Vivamus id mi enim. Integer id turpis sapien. Ut condimentum lobortis\n  sagittis. Aliquam purus tellus, faucibus eget urna at, iaculis venenatis\n  nulla. Vivamus a pharetra leo.", 
            "title": "Ordered lists"
        }, 
        {
            "location": "/specimen/#definition-lists", 
            "text": "Lorem ipsum dolor sit amet   Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus\ntellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor\nlobortis orci, at elementum urna sodales vitae. In in vehicula nulla.  Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis.\nNam vulputate tincidunt fringilla.\nNullam dignissim ultrices urna non auctor.   Cras arcu libero   Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin\nut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at\naliquam ac, aliquet sed mauris.", 
            "title": "Definition lists"
        }, 
        {
            "location": "/specimen/#code-blocks", 
            "text": "", 
            "title": "Code blocks"
        }, 
        {
            "location": "/specimen/#inline", 
            "text": "Morbi eget  dapibus felis . Vivamus  venenatis porttitor  tortor sit amet\nrutrum. Class aptent taciti sociosqu ad litora torquent per conubia nostra,\nper inceptos himenaeos.  Pellentesque aliquet quam enim , eu volutpat urna\nrutrum a.  Nam vehicula nunc  return   target  mauris, a ultricies libero efficitur\nsed. Sed molestie imperdiet consectetur. Vivamus a pharetra leo. Pellentesque\neget ornare tellus, ut gravida mi. Fusce vel lacinia lacus.", 
            "title": "Inline"
        }, 
        {
            "location": "/specimen/#listing", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 var   _extends   =   function ( target )   { \n   for   ( var   i   =   1 ;   i     arguments . length ;   i ++ )   { \n     var   source   =   arguments [ i ]; \n     for   ( var   key   in   source )   { \n       target [ key ]   =   source [ key ]; \n     } \n   }     return   target ;  };", 
            "title": "Listing"
        }, 
        {
            "location": "/specimen/#horizontal-rules", 
            "text": "Aenean in finibus diam. Duis mollis est eget nibh volutpat, fermentum aliquet\ndui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna\nnon auctor.   Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis\nelementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla\nconsectetur feugiat sodales.", 
            "title": "Horizontal rules"
        }, 
        {
            "location": "/specimen/#data-tables", 
            "text": "Sollicitudo / Pellentesi  consectetur  adipiscing  elit  arcu  sed      Vivamus a pharetra  yes  yes  yes  yes  yes    Ornare viverra ex  yes  yes  yes  yes  yes    Mauris a ullamcorper  yes  yes  partial  yes  yes    Nullam urna elit  yes  yes  yes  yes  yes    Malesuada eget finibus  yes  yes  yes  yes  yes    Ullamcorper  yes  yes  yes  yes  yes    Vestibulum sodales  yes  -  yes  -  yes    Pulvinar nisl  yes  yes  yes  -  -    Pharetra aliquet est  yes  yes  yes  yes  yes    Sed suscipit  yes  yes  yes  yes  yes    Orci non pretium  yes  partial  -  -  -     Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus\nnon sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci,\nat elementum urna sodales vitae. In in vehicula nulla, quis ornare libero.     Left  Center  Right      Lorem  dolor  amet    ipsum  sit      Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu\nlectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl,\nsit amet laoreet nibh.  \n   \n     \n     \n   \n   \n     \n       Table \n       with colgroups (Pandoc) \n     \n   \n   \n     \n       Lorem \n       ipsum dolor sit amet. \n     \n     \n       Sed sagittis \n       eleifend rutrum. Donec vitae suscipit est.", 
            "title": "Data tables"
        }
    ]
}